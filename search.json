[
  {
    "objectID": "05-19 Factor Analysis/pca.html",
    "href": "05-19 Factor Analysis/pca.html",
    "title": "Implementation of Principle Component Analyis",
    "section": "",
    "text": "One of the most common dimension reduction techniques"
  },
  {
    "objectID": "05-19 Factor Analysis/pca.html#in-scikit-learn-you-use-this-package",
    "href": "05-19 Factor Analysis/pca.html#in-scikit-learn-you-use-this-package",
    "title": "Implementation of Principle Component Analyis",
    "section": "In Scikit-Learn you use this package",
    "text": "In Scikit-Learn you use this package\n\nfrom sklearn.preprocessing import StandardScaler\n\niris = load_iris()\nX = iris['data']\ny = iris['target']\n\nX_std = StandardScaler().fit_transform(X)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2).fit(X_std)\n\nprint('Components:\\n', pca.components_)\nprint('Explained variance ratio:\\n', pca.explained_variance_ratio_)\n\ncum_explained_variance = np.cumsum(pca.explained_variance_ratio_)\nprint('Cumulative explained variance:\\n', cum_explained_variance)\n\nX_pca = pca.transform(X_std) # Apply dimensionality reduction to X.\nprint('Transformed data shape:', X_pca.shape)\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c = y)\nplt.xlabel('PC1'); plt.xticks([])\nplt.ylabel('PC2'); plt.yticks([])\nplt.title('2 components, captures {}% of total variation'.format(cum_explained_variance[1].round(4)*100))\nplt.show()\n\nComponents:\n [[ 0.52106591 -0.26934744  0.5804131   0.56485654]\n [ 0.37741762  0.92329566  0.02449161  0.06694199]]\nExplained variance ratio:\n [0.72962445 0.22850762]\nCumulative explained variance:\n [0.72962445 0.95813207]\nTransformed data shape: (150, 2)"
  },
  {
    "objectID": "05-19 Factor Analysis/pca.html#conclusion",
    "href": "05-19 Factor Analysis/pca.html#conclusion",
    "title": "Implementation of Principle Component Analyis",
    "section": "Conclusion",
    "text": "Conclusion\n\nPCA is based covariance a measurement of correlation of two product\nPCA is linear based algebra.\nDimension reduction is in fact finding top “k” highest eigen value and their corresponding “eigen-vector”, they corresponding sets of co-efficiency (or scores) that each dimension contributes to (the latent factor).\nThe resulting “k” dimension results in most explained variance been kept.\n\n\nReference: Implementation of Principle Component\n\nAlireza Bagheri Python Implementation of PCA\nSergen Cansiz exmpalin co-variance with eigenvalues\nGrant Sanderson’s Visually Expalin Linear Algebra\nDuke University’s Guide and Other Cool Staff\nStack Exchange: Intuition of Eigenvalue with Covariance Matrix\n\n\n\nOther Dimension Reduction Techniques\n\nMote Carlo Generator\nLinear Discreminant Analysis & Comparsion here"
  },
  {
    "objectID": "03-15-Time-Series/statlab - Normality of Data.html",
    "href": "03-15-Time-Series/statlab - Normality of Data.html",
    "title": "Data Normality",
    "section": "",
    "text": "This notebook explore: - The R square against normal data - Norm data against lag - qq plot against normal data\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nplot_params = {'color': '0.75',\n 'style': '.-',\n 'markeredgecolor': '0.25',\n 'markerfacecolor': '0.25',\n 'legend': False}\n\nplt.style.use('seaborn-whitegrid')\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\n%config InlineBackend.figure_format = 'retina'\n\n/var/folders/r5/1cdq52mn21zdnqzl0fvp44zw0000gn/T/ipykernel_23399/1354677373.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n\n\n\n\n\n# explore linear correlation\ndef foo(x):\n    y = x * 3 + 1\n    return y\n\nx = np.arange(1, 4)\ny = np.apply_along_axis(foo, axis = 0, arr = x)\n\nif ((x - x.mean())**2).sum()/x.shape[0] == x.var():\n    print('var is not sqrt-ed variance')\nelse: \n    print('var is standard deviation')\n\ndef corrianda(X, Y):\n    c = ((X - X.mean())*(Y - Y.mean())).sum()\n    return(c)\ndef ssd(X):\n    c = X.var() * X.shape[0]\n    return(c)\ndef covar(X, Y):\n    c = corrianda(x, y) / np.sqrt(ssd(x) * ssd(y))\n    return(c)\n\nvar is not sqrt-ed variance\n\n\n\nx = np.arange(1, 10000)\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a perfect line has coefficency of {covar(x, y)}')\n\na perfect line has coefficency of 1.0\n\n\n\nx = np.random.normal(50, 10, 10000)\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a standard normal line has coefficency of {covar(x, y)}')\n\na standard normal line has coefficency of 1.0\n\n\n\nx = np.arange(1, 10000)**2\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a skewed data(power) has coefficency of {covar(x, y)}')\n\na skewed data(power) has coefficency of 1.0000000000000002\n\n\n\nx = np.log(np.arange(1, 10000))\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a skewed data(log) has coefficency of {covar(x, y)}')\n\na skewed data(log) has coefficency of 0.9999999999999999\n\n\n\n\n\n\ndf = pd.read_csv(\n    'data/output/Calenrier_output.csv',\n    parse_dates = ['date']\n)\ndf = df.set_index('date').to_period('D')\n\n\ndf.filter(regex='s\\(\\d+,\\d+\\)').iloc[0:15,:].plot(style = 'o')\ndf.filter(regex='sin|cos').iloc[0:15,:].plot(style = 'o:')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe s something seems to be modeling week\n\n\n\n\nx = pd.Series(np.random.normal(50, 10, 200))\n\n\nfig, (_) = plt.subplots(2, 3, figsize = (15,10) )\nlags = np.append(np.arange(5), 50)\nfor i in np.arange(len(lags)):\n    sns.scatterplot(\n        data = pd.DataFrame({\"x\":x, \"lag\" :x.shift(i)}),\n        x = 'x',\n        y = 'lag',\n        ax = _[i%2, i//2]\n    )\n    _[i%2, i//2].set_title(f'Lag {lags[i]} steps')\n\n\n\n\n\n\n\n\nWhat I realise about this a normal distributed plot (taking out in random orders) will not have auto-correlatio no matter what. This is because after taking out a random value, the distribution is still normal.\nAlternatively If I order x and let it leg this results might be very different:\n\nfig, (_) = plt.subplots(2, 3, figsize = (15,10) )\nlags = np.append(np.arange(5), 50)\nfor i in np.arange(len(lags)):\n    sns.scatterplot(\n        data = pd.DataFrame({\"x\":x.sort_values(), \"lag\" :x.sort_values().shift(i)}),\n        x = 'x',\n        y = 'lag',\n        ax = _[i%2, i//2]\n    )\n    _[i%2, i//2].set_title(f'Lag {lags[i]} steps')\n\n\n\n\n\n\n\n\nThink of this problem in very extrem ways. At very extrem. The order of a normally distributed variables are extremely random. There will be no pattern to lag at all.\nAt the other end, our variable are extremely ordered. There will be near perfect correcation between lag and time. (This correlation will be near one)\nFor a gross sample that is normal. The slop should only be between 0 to 1. Trimed tail should not affect result here.\nWhat does it means when any of your is extremely random\nI guess what by looking at distribution of something you really know what is going on underneath.\n\n\nUse qqplot\n\nimport statsmodels.api as sm\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (14, 7))\nsm.qqplot(x.sort_values().head(100), ax=ax1);\nax1.set_title('trail tails results in upward bending')\n# trail tails results in upward bending\nsm.qqplot(x.sort_values().tail(100), ax=ax2);\nax2.set_title('trim heads restuls in downward bending')\n# trim heads restuls in downward bending\n\nText(0.5, 1.0, 'trim heads restuls in downward bending')\n\n\n\n\n\n\n\n\n\nNormal data can be abnormal by missing some part.\n\n\n\n\nx = np.random.normal(50, 4, 10000)\nsx = pd.Series(x).sort_values()\ndfx = pd.DataFrame(sx, columns=['x'])\ndfx['rank'] = dfx.rank()\n\ndfx_capture = dfx.query('x &lt; x.quantile(0.9)')\ndfx_escaped= dfx.query('x &gt;= x.quantile(0.9)')\ng = sns.scatterplot(data = dfx_capture, x = 'x', y = 'rank', edgecolor = None, alpha = 0.05)\ng = sns.scatterplot(data = dfx_escaped, x = 'x', y = 'rank', edgecolor= None, color = 'red', alpha = 0.01)\ng.set_title('value x to rank plot is a symatric sigmoid curve')\n\nText(0.5, 1.0, 'value x to rank plot is a symatric sigmoid curve')\n\n\n\n\n\n\n\n\n\nPloting distribution as sigmoid. Should in theory against tail trim. This is useful. In biology experiments, some capture device is not particular good at capturing top 10% escape expert animals. As a result, sample will give us"
  },
  {
    "objectID": "03-15-Time-Series/statlab - Normality of Data.html#intro",
    "href": "03-15-Time-Series/statlab - Normality of Data.html#intro",
    "title": "Data Normality",
    "section": "",
    "text": "This notebook explore: - The R square against normal data - Norm data against lag - qq plot against normal data\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nplot_params = {'color': '0.75',\n 'style': '.-',\n 'markeredgecolor': '0.25',\n 'markerfacecolor': '0.25',\n 'legend': False}\n\nplt.style.use('seaborn-whitegrid')\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\n%config InlineBackend.figure_format = 'retina'\n\n/var/folders/r5/1cdq52mn21zdnqzl0fvp44zw0000gn/T/ipykernel_23399/1354677373.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n\n\n\n\n\n# explore linear correlation\ndef foo(x):\n    y = x * 3 + 1\n    return y\n\nx = np.arange(1, 4)\ny = np.apply_along_axis(foo, axis = 0, arr = x)\n\nif ((x - x.mean())**2).sum()/x.shape[0] == x.var():\n    print('var is not sqrt-ed variance')\nelse: \n    print('var is standard deviation')\n\ndef corrianda(X, Y):\n    c = ((X - X.mean())*(Y - Y.mean())).sum()\n    return(c)\ndef ssd(X):\n    c = X.var() * X.shape[0]\n    return(c)\ndef covar(X, Y):\n    c = corrianda(x, y) / np.sqrt(ssd(x) * ssd(y))\n    return(c)\n\nvar is not sqrt-ed variance\n\n\n\nx = np.arange(1, 10000)\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a perfect line has coefficency of {covar(x, y)}')\n\na perfect line has coefficency of 1.0\n\n\n\nx = np.random.normal(50, 10, 10000)\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a standard normal line has coefficency of {covar(x, y)}')\n\na standard normal line has coefficency of 1.0\n\n\n\nx = np.arange(1, 10000)**2\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a skewed data(power) has coefficency of {covar(x, y)}')\n\na skewed data(power) has coefficency of 1.0000000000000002\n\n\n\nx = np.log(np.arange(1, 10000))\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a skewed data(log) has coefficency of {covar(x, y)}')\n\na skewed data(log) has coefficency of 0.9999999999999999\n\n\n\n\n\n\ndf = pd.read_csv(\n    'data/output/Calenrier_output.csv',\n    parse_dates = ['date']\n)\ndf = df.set_index('date').to_period('D')\n\n\ndf.filter(regex='s\\(\\d+,\\d+\\)').iloc[0:15,:].plot(style = 'o')\ndf.filter(regex='sin|cos').iloc[0:15,:].plot(style = 'o:')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe s something seems to be modeling week\n\n\n\n\nx = pd.Series(np.random.normal(50, 10, 200))\n\n\nfig, (_) = plt.subplots(2, 3, figsize = (15,10) )\nlags = np.append(np.arange(5), 50)\nfor i in np.arange(len(lags)):\n    sns.scatterplot(\n        data = pd.DataFrame({\"x\":x, \"lag\" :x.shift(i)}),\n        x = 'x',\n        y = 'lag',\n        ax = _[i%2, i//2]\n    )\n    _[i%2, i//2].set_title(f'Lag {lags[i]} steps')\n\n\n\n\n\n\n\n\nWhat I realise about this a normal distributed plot (taking out in random orders) will not have auto-correlatio no matter what. This is because after taking out a random value, the distribution is still normal.\nAlternatively If I order x and let it leg this results might be very different:\n\nfig, (_) = plt.subplots(2, 3, figsize = (15,10) )\nlags = np.append(np.arange(5), 50)\nfor i in np.arange(len(lags)):\n    sns.scatterplot(\n        data = pd.DataFrame({\"x\":x.sort_values(), \"lag\" :x.sort_values().shift(i)}),\n        x = 'x',\n        y = 'lag',\n        ax = _[i%2, i//2]\n    )\n    _[i%2, i//2].set_title(f'Lag {lags[i]} steps')\n\n\n\n\n\n\n\n\nThink of this problem in very extrem ways. At very extrem. The order of a normally distributed variables are extremely random. There will be no pattern to lag at all.\nAt the other end, our variable are extremely ordered. There will be near perfect correcation between lag and time. (This correlation will be near one)\nFor a gross sample that is normal. The slop should only be between 0 to 1. Trimed tail should not affect result here.\nWhat does it means when any of your is extremely random\nI guess what by looking at distribution of something you really know what is going on underneath.\n\n\nUse qqplot\n\nimport statsmodels.api as sm\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (14, 7))\nsm.qqplot(x.sort_values().head(100), ax=ax1);\nax1.set_title('trail tails results in upward bending')\n# trail tails results in upward bending\nsm.qqplot(x.sort_values().tail(100), ax=ax2);\nax2.set_title('trim heads restuls in downward bending')\n# trim heads restuls in downward bending\n\nText(0.5, 1.0, 'trim heads restuls in downward bending')\n\n\n\n\n\n\n\n\n\nNormal data can be abnormal by missing some part.\n\n\n\n\nx = np.random.normal(50, 4, 10000)\nsx = pd.Series(x).sort_values()\ndfx = pd.DataFrame(sx, columns=['x'])\ndfx['rank'] = dfx.rank()\n\ndfx_capture = dfx.query('x &lt; x.quantile(0.9)')\ndfx_escaped= dfx.query('x &gt;= x.quantile(0.9)')\ng = sns.scatterplot(data = dfx_capture, x = 'x', y = 'rank', edgecolor = None, alpha = 0.05)\ng = sns.scatterplot(data = dfx_escaped, x = 'x', y = 'rank', edgecolor= None, color = 'red', alpha = 0.01)\ng.set_title('value x to rank plot is a symatric sigmoid curve')\n\nText(0.5, 1.0, 'value x to rank plot is a symatric sigmoid curve')\n\n\n\n\n\n\n\n\n\nPloting distribution as sigmoid. Should in theory against tail trim. This is useful. In biology experiments, some capture device is not particular good at capturing top 10% escape expert animals. As a result, sample will give us"
  },
  {
    "objectID": "03-15-Time-Series/Learn -Time Series.html",
    "href": "03-15-Time-Series/Learn -Time Series.html",
    "title": "Engineering Features for Time Series",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#pip install -U scikit-learn scipy\nfrom sklearn.linear_model import LinearRegression\n\n\nData\nkaggle timeserires tutorial\nstore sale data\n\n\nPython Enviroment Version\nI’ve created conda enviroment py10 for this running the python 3.10.4. Always use python --version to check if you are on py10. This should have package pyeath installed. Uninstalled conda. use Python 3.11.\n\n\nSetting Figures\n\nplot_params = {'color': '0.75',\n 'style': '.-',\n 'markeredgecolor': '0.25',\n 'markerfacecolor': '0.25',\n 'legend': False}\n\nplt.style.use('seaborn-whitegrid')\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\n%config InlineBackend.figure_format = 'retina'\n\n\n#pip install kaggle\n#!kaggle kernels output ryanholbrook/linear-regression-with-time-series -p data\ndf = pd.read_csv('data/store-sales-time-series-forecasting/train.csv',\n                 index_col='date',\n                 parse_dates=['date']\n                 )\ndf.info()\ntype(df.index)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 3000888 entries, 2013-01-01 to 2017-08-15\nData columns (total 5 columns):\n #   Column       Dtype  \n---  ------       -----  \n 0   id           int64  \n 1   store_nbr    int64  \n 2   family       object \n 3   sales        float64\n 4   onpromotion  int64  \ndtypes: float64(1), int64(3), object(1)\nmemory usage: 137.4+ MB\n\n\npandas.core.indexes.datetimes.DatetimeIndex\n\n\n\n\nTime Step Feature\nSimply index time as x. Day 1, Day 2 ect\n\n# Engineer a time step feature\nimport numpy as np\ngross_sale = df.groupby('date')[['sales']].sum()\ngross_sale['Time']=np.arange(len(gross_sale.index))\ngross_sale.head(3)\n\n\n\n\n\n\n\n\n\nsales\nTime\n\n\ndate\n\n\n\n\n\n\n2013-01-01\n2511.618999\n0\n\n\n2013-01-02\n496092.417944\n1\n\n\n2013-01-03\n361461.231124\n2\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n# scatter plot\nax.plot('Time', 'sales', data=gross_sale, color='0.75')\n# regression plot\nax = sns.regplot(x='Time', y='sales', data=gross_sale, ci=None, scatter_kws=dict(color='0.25'))\nax.set_title('Time Plot of Gross Sales');\n\n\n\n\n\n\n\n\nAdvantage of a timestep feature instead of just time is it scales verywell. So you don’t have to worry too much about does it matter we measure in date units or in month units.\n\n\nLag Features\nThis one is about use previous value to predict future recent value\n\ngross_sale['Lag_1'] = gross_sale['sales'].shift(1)\nfig, ax = plt.subplots()\nax = sns.regplot(x='Lag_1', y='sales', data=gross_sale, ci=None, scatter_kws=dict(color='0.25'))\nax.set_aspect('equal')\nax.set_title('Lag Plot of Hardcover Sales')\n\nText(0.5, 1.0, 'Lag Plot of Hardcover Sales')\n\n\n\n\n\n\n\n\n\nserial dependence high sales on one day usually means hig sales on the next day\n\nX = gross_sale.loc[:, ['Time']]\ny = gross_sale.loc[:, 'sales']\nprint('X looks like this: '), print(X.head(3)), print('...'), print(f\"X is {type(X)}\")\nprint('y looks like this: '), print(y.head(3)), print('...'), print(f\"y is {type(y)}\")\n# HINT: do you know now why when fit X they prefer use the higher X\n\nX looks like this: \n            Time\ndate            \n2013-01-01     0\n2013-01-02     1\n2013-01-03     2\n...\nX is &lt;class 'pandas.core.frame.DataFrame'&gt;\ny looks like this: \ndate\n2013-01-01      2511.618999\n2013-01-02    496092.417944\n2013-01-03    361461.231124\nName: sales, dtype: float64\n...\ny is &lt;class 'pandas.core.series.Series'&gt;\n\n\n(None, None, None, None)\n\n\n\n# fit a model be like:\nX = gross_sale.loc[:, ['Time']]\ny = gross_sale.loc[:, 'sales']\nmodel = LinearRegression()\nmodel.fit(X, y)\npred_y = pd.Series(model.predict(X), index = X.index)\n\n\n\nPlus: Multi-assign and Plot several plot\n\n# python hack about multi-assign\na, b = [1,2]\na, (b, c) = [1, (2, 3)]\n\n\n\nTrend\n\nData\n\ndf = pd.read_csv('data/store-sales-time-series-forecasting/train.csv',\n                        index_col = 'date',\n                        parse_dates = ['date']\n                        ).to_period('D')\naverage_sales = df.groupby('date')['sales'].mean()\n\n\naverage_sales.head()\n\ndate\n2013-01-01      1.409438\n2013-01-02    278.390807\n2013-01-03    202.840197\n2013-01-04    198.911154\n2013-01-05    267.873244\nFreq: D, Name: sales, dtype: float64\n\n\n\n\nMoving Average\nMoving average is the idea of observe overage value within a window of time frame. But instead of those windows being mutually exclusive, those windows roll on.\n\nlen(average_sales.loc['2013'])\n\n364\n\n\nIn the gross_sale data, one enclose cycle is one years. So window should be set as 360. minimum periods is typically half this window (not sure why)\n\nmoving_average = average_sales.rolling(\n    window = 364,\n    center = True,\n    min_periods=183\n).mean()\nax = average_sales.plot(style = '.', color = '0.5')\nmoving_average.plot(\n    ax = ax,\n    linewidth = 3,\n    title = 'Ploting Moving Average'\n)\n\n\n\n\n\n\n\n\n\nmoving_average.sample(3), gross_sale.sample(3)\n\n(date\n 2015-12-20    437.490098\n 2014-04-11    288.010483\n 2015-12-17    436.879765\n Freq: D, Name: sales, dtype: float64,\n                    sales  Time          Lag_1\n date                                         \n 2016-05-25  6.375120e+05  1237  606377.205216\n 2015-03-24  4.073697e+05   810  462664.237004\n 2016-04-02  1.150825e+06  1184  872467.320075)\n\n\n\n\nDeterministic Model\nwhich is a fancy term for linear regression with time series. This is also a linear model. To make this model work requries you to converge time index to_period index.\nThis process is similar to linear model. Instead fitting x, y, z three independ varaibles, you are fitting three x of different ‘orders’.\n\\[\ny = a + b\\,x + c\\,x^2 + d\\,x^3 + ... + n\\,x^{n}\n\\]\n(Somewhat reminds me of Taylor’s series. Maybe that’s what order mans) What’s interesting about this function is odd quatric functions can vary ups and downs, so to fit into any shape you like.\n\n#pip install statsmodels\nfrom statsmodels.tsa.deterministic import DeterministicProcess\n\ny = average_sales.copy()\ndp = DeterministicProcess(\n    index=y.index,\n    constant = True, # dummy features\n    order = 3,       # time dummy trend, 1 is linear, 2 is quadratic, 3 cubic\n    drop = True\n)\nX = dp.in_sample()\nX.tail()\n\n\n\n\n\n\n\n\n\nconst\ntrend\ntrend_squared\ntrend_cubed\n\n\ndate\n\n\n\n\n\n\n\n\n2017-08-11\n1.0\n1680.0\n2822400.0\n4.741632e+09\n\n\n2017-08-12\n1.0\n1681.0\n2825761.0\n4.750104e+09\n\n\n2017-08-13\n1.0\n1682.0\n2829124.0\n4.758587e+09\n\n\n2017-08-14\n1.0\n1683.0\n2832489.0\n4.767079e+09\n\n\n2017-08-15\n1.0\n1684.0\n2835856.0\n4.775582e+09\n\n\n\n\n\n\n\n\n\nX_fore = dp.out_of_sample(steps = 90)\nX_fore.head()\n\n\n\n\n\n\n\n\n\nconst\ntrend\ntrend_squared\ntrend_cubed\n\n\n\n\n2017-08-16\n1.0\n1685.0\n2839225.0\n4.784094e+09\n\n\n2017-08-17\n1.0\n1686.0\n2842596.0\n4.792617e+09\n\n\n2017-08-18\n1.0\n1687.0\n2845969.0\n4.801150e+09\n\n\n2017-08-19\n1.0\n1688.0\n2849344.0\n4.809693e+09\n\n\n2017-08-20\n1.0\n1689.0\n2852721.0\n4.818246e+09\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\ny = average_sales\nmodel = LinearRegression(fit_intercept = False)\nmodel.fit(X, y)\ny_pred = pd.Series(model.predict(X), index = X.index)\ny_fore = pd.Series(model.predict(X_fore), index = X_fore.index)\n\n\nax = average_sales.plot(\n    style = '.', color = '0.5', title = 'average sales'\n)\nax = y_pred.plot(ax= ax, linewidth=3, label='Trend') # underscore is for temporary variable\nax = y_fore.plot(ax = ax, linewidth=3, label=\"Trend Forcast\", color = 'C3')\n\n\n\n\n\n\n\n\n\n\nRisks of Highorder Ploynomials\nDue to the property of function &gt; An order 11 polynomial will include terms like t ** 11. Terms like these tend to diverge rapidly outside of the training period making forecasts very unreliable.\n\ndp = DeterministicProcess(\n    index=y.index,\n    order = 11,       # time dummy trend, 1 is linear, 2 is quadratic, 3 cubic\n)\nX = dp.in_sample()\n\nmodel = LinearRegression(fit_intercept = True)\nmodel.fit(X, y)\n\nX_fore = dp.out_of_sample(steps=90)\ny_pred = pd.Series(model.predict(X), index=X.index)\ny_fore = pd.Series(model.predict(X_fore), index = X_fore.index)\n\nax = y.plot(style = '.', alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\")\nax = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\", color='C0')\nax = y_fore.plot(ax=ax, linewidth=3, label=\"Trend Forecast\", color='C3')\nax.legend();\n\n\n\n\n\n\n\n\n\n\nFit Trend with Spines\nMultivariate Adaptive Regression Splines (MARS) &gt; Splines are a nice alternative to polynomials when you want to fit a trend. The Multivariate Adaptive Regression Splines (MARS) algorithm in the pyearth library is powerful and easy to use. There are a lot of hyperparameters you may want to investigate.\nThis use Earth() model in pyearth package by Stephen Milborrow, the originsl is R version. API here You will need to install via conda. Use the one under scikit-learn.\nconda install pip install sklearn-contrib-py-earth\n\n#from pyearth import Earth\ntry: \n    y = average_sales.copy()\n    dp = DeterministicProcess(index=y.index, order=1)\n    X = dp.in_sample()\n\n    # Fit a MARS model with `Earth`\n\n    model = Earth()\n    model.fit(X, y)\n\n    y_pred = pd.Series(model.predict(X), index=X.index)\n\n    ax = y.plot(#**plot_params, \n                title=\"Average Sales\", ylabel=\"items sold\")\n    ax = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\")\nexcept: \n    print('This code will no execute until pyearth is installed')\n\nThis code will no execute until pyearth is installed\n\n\n\n\n\nSeasonality\nYou already know sine and cosine functions are used to model these. Terms are called Fourtier features.\n\nFourier Features\n1 pair of fourier features are: \\[\nk =  2 \\pi \\frac{t}{f}\n\\\\\nf(j) = \\beta_1 \\sin(j * k) + \\beta_2 \\cos(j * k)\n\\]\n\\(n\\) order(s) of fourier features: \\[\nF(n) = \\sum_{j=1}^{n} f(j)\n\\]\n\n\\(k\\) is time scaled to frequency\n\\(i\\) is order of features\n\\(\\beta_1\\) and \\(\\beta_2\\) is what you throw into linear regression\n\nThe advantage of a fourier pair is so that two parameters are at the same scale.\n\n# create fourier feature for linear regression to figure out\ndef fourier_features(index, freq, order):\n    time = np.arange(len(index), dtype=np.float32)\n    k = 2 * np.pi * (1 / freq) * time\n    features = {}\n    for i in range(1, order + 1):\n        features.update({\n            f\"sin_{freq}_{i}\": np.sin(i * k),\n            f\"cos_{freq}_{i}\": np.cos(i * k),\n        })\n    return pd.DataFrame(features, index=index)\n# this transform it into something you are easy to fits into linear regression\n\n\n\nPeriodogram\nGiven frequency y = $ {2}$ \\(\\beta_1\\), \\(\\beta_2\\) is the coefficients of sine and cosine.\nA useful trigonometric identity is is: \\[\nA \\cos(2 \\pi \\omega t + \\phi) = \\beta_1 \\cos(2 \\pi \\omega t) + \\beta_2 \\sin(2 \\pi \\omega t) \\\\\n\\beta_1 = A \\cos(\\phi) \\\\\n\\beta_2 = - A \\sin(\\phi) \\\\\n2A^2 = \\beta_1^2 + \\beta_2^2\n\\] The whole time series is represented as: \\[\nx_t = \\sum_{j = 1}^{n/2}\n      [\n        \\beta_1(\\frac{j}{n}) cos(2 \\pi \\omega_j t)+\n        \\beta_2(\\frac{j}{n}) sin(2 \\pi \\omega_j t)\n      ]\n\\] In periodogram given \\(\\frac{j} {n}\\) frequency: \\[\nP(\\frac{j} {n}) = \\beta_1^2 (\\frac{j} {n}) + \\beta_2^2(\\frac{j}{n})\n\\]\n\nA relatively large value of P(j/n) indicates relatively more importance for the frequency j/n (or near j/n) in explaining the oscillation in the observed series. P(j/n) is proportional to the squared correlation between the observed series and a cosine wave with frequency j/n. The dominant frequencies might be used to fit cosine (or sine) waves to the data, or might be used simply to describe the important periodicities in the series.\n\nsource: PennState Eberly College of Science\n\nEstimate \\(\\beta_1\\) and \\(\\beta_2\\) is two of n parameters\nThey are not neccessary estimated by regression but this math device called Fast Fourier Transformation (FFT)\n\n\n\n(Fast) Fourier Transformation\n\n\n\nimage\n\n\nI think of it this way. Think you can fit a n sum of fontier pairs together. Your frequency is then coposed of n possible pair of fourier pairs. The higher order, the higher the freqency (lower the wave length). Some fourier will be more dominant than the other. When a fourier pair is not dominant, their sum of \\(\\beta\\) square may as well be 0. This means it cancels them out. So if you slice the equation by fourier pairs. Each pair will represent strenght of that wave function. With 0 indicate very low effect. Higher value indicate more dominate effect.\ndig further: what is fourier transform\n\nCustom Functions\n\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\ndef seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax\n\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram( # this code do not generate graph it creates two vectors\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\n\n\nExample\n\naverage_sales_2017 = average_sales.squeeze().loc['2017']\nX = average_sales_2017.to_frame()\nX['week'] = X.index.week\nX['day'] = X.index.dayofweek\nseasonal_plot(X, \n              'sales',\n              period = 'week',\n              freq = 'day')\nplot_periodogram(average_sales_2017)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTE: This set of features happens to have four spikes and the four spkes happens to be consecutive\nNote x is completely different scale to what’s introduce in PenState’s text book (where frequency is represented as a fraction). Here frequency is represented as the inverse whole period with low frequency left and high frequency right.\nThis graph tells you strong weekly seasonality. Because you want to reduce chances of over fitting. You want to try reduce the fourier sets. You can model these ways:\n\n12 month frequency (30/365) with 4 fourier pairs\n1 anual frequency (364/365) with 26 fourier paris (there will be a lot of frequency here)\n\nSet frequency as how you want period to return. As order increase it captures the intrinsic orders of the frequency.\nSo in the one the first fourier feature is observed monthly hense the set frequency to month. There are about four additionaly waves (they all happens to be twice as frequent as the other one). Hense we set four fourier pairs.\n\n\n\n\nCompute Fourier Feature (statmodels)\n\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfourier = CalendarFourier(freq=\"M\", order=4) # here parameter derived from periodogram\ndp = DeterministicProcess(\n    index=average_sales_2017.index,\n    constant=True,               # dummy feature for bias (y-intercept)\n    order=1,                     # trend (order 1 means linear)\n    seasonal=True,               # weekly seasonality (indicators)\n    additional_terms=[fourier],  # annual seasonality (fourier)\n    drop=True,                   # drop terms to avoid collinearity\n)\nX = dp.in_sample()  \n# note fourier needs to be made as a list for it to be literatable\ny = average_sales_2017\nmodel = LinearRegression(fit_intercept=False)\n_ = model.fit(X, y)\ny_pred = pd.Series(model.predict(X), index=y.index)\nX_fore = dp.out_of_sample(steps=90)\ny_fore = pd.Series(model.predict(X_fore), index=X_fore.index)\nax = y.plot(color='0.25', style='.-', alpha = 0.25,title=\"Store Average Sales\")\nax = y_pred.plot(ax=ax, label=\"Seasonal\")\nax = y_fore.plot(ax=ax, label=\"Seasonal Forecast\", color='C3')\n_ = ax.legend();\n\n\n\n\n\n\n\n\n\n#X.to_csv('data/output/Calenrier_output.csv')\n\n\n\nDetrend or deseasonalising\nVerify that we are not modeling random variance\n\ny_deseason = y - y_pred\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(10, 7))\nax1 = plot_periodogram(y, ax=ax1)\nax1.set_title(\"Product Sales Frequency Components\")\nax2 = plot_periodogram(y_deseason, ax=ax2)\nax2.set_title(\"Deseasonalized\")\nax3 = plot_periodogram(y, ax=ax3)\n#ax.axes.set_facecolor('blue')\nax3 = plot_periodogram(y_deseason, ax=ax3)\nax3.axes.lines[0].set_color('blue')\nax3.set_title(\"Product Sales Frequency Components\"); # take this out you would be creating a new plot\n\n\n\n\n\n\n\n\nThis plot shows that our model ahs surverred very well in explaining seasonality variance.\n\nHoliday (Special Events)\nYou can fit spacial events by creating dummy variables (here it is convinent because we are only useing one years to train)\n\nholidays_events = pd.read_csv(\n    'data/store-sales-time-series-forecasting/holidays_events.csv',\n    index_col = 'date',\n    dtype={\n        'type': 'category',\n        'locale': 'category',\n        'locale_name': 'category',\n        'description': 'category',\n        'transferred': 'bool',\n    },\n    parse_dates = ['date'],\n    infer_datetime_format=True\n    ).to_period('D')\n#type(holidays_events.index)\nholidays = (\n    holidays_events\n    .query(\"locale in ['National', 'Regional']\")\n    .loc['2017':'2017-08-15', ['description']]\n    .assign(description=lambda x: x.description.cat.remove_unused_categories())\n)\ndisplay(holidays)\n\n\n\n\n\n\n\n\n\ndescription\n\n\ndate\n\n\n\n\n\n2017-01-01\nPrimer dia del ano\n\n\n2017-01-02\nTraslado Primer dia del ano\n\n\n2017-02-27\nCarnaval\n\n\n2017-02-28\nCarnaval\n\n\n2017-04-01\nProvincializacion de Cotopaxi\n\n\n2017-04-14\nViernes Santo\n\n\n2017-05-01\nDia del Trabajo\n\n\n2017-05-13\nDia de la Madre-1\n\n\n2017-05-14\nDia de la Madre\n\n\n2017-05-24\nBatalla de Pichincha\n\n\n2017-05-26\nTraslado Batalla de Pichincha\n\n\n2017-06-25\nProvincializacion de Imbabura\n\n\n2017-08-10\nPrimer Grito de Independencia\n\n\n2017-08-11\nTraslado Primer Grito de Independencia\n\n\n\n\n\n\n\n\n\nax = y_deseason.plot(**plot_params)\nplt.plot_date(holidays.index, y_deseason[holidays.index], color='C3')\nax.set_title('National and Regional Holidays');\n\n\n\n\n\n\n\n\n\nX_holidays = pd.get_dummies(\n    holidays,\n    columns = ['description']\n)\nX2 = X.join(X_holidays, on='date').fillna(0.0)\nmodel = LinearRegression().fit(X2, y)\ny_pred = pd.Series(\n    model.predict(X2),\n    index = X2.index,\n    name = 'Fitted',\n)\n\n\nX_holidays.sample(3)\n\n\n\n\n\n\n\n\n\ndescription_Batalla de Pichincha\ndescription_Carnaval\ndescription_Dia de la Madre\ndescription_Dia de la Madre-1\ndescription_Dia del Trabajo\ndescription_Primer Grito de Independencia\ndescription_Primer dia del ano\ndescription_Provincializacion de Cotopaxi\ndescription_Provincializacion de Imbabura\ndescription_Traslado Batalla de Pichincha\ndescription_Traslado Primer Grito de Independencia\ndescription_Traslado Primer dia del ano\ndescription_Viernes Santo\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-01\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n2017-08-10\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2017-08-11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\n\n\nax = y.plot(**plot_params, alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\")\nax = y_pred.plot(ax=ax, label=\"Seasonal\")\nax.legend();\n\n\n\n\n\n\n\n\n\nax = y.plot(color='0.25', style='.-', alpha = 0.25,title=\"Store Average Sales\")\nax = y_pred.plot(ax=ax, label=\"Seasonal\")\nax = y_fore.plot(ax=ax, label=\"Seasonal Forecast\", color='C3')\nax = plt.plot_date(holidays.index, y[holidays.index], color = 'C3');\n\n\n\n\n\n\n\n\n\n\n\nTime Series as Features\n\nPartial Autocorrelcation\n\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\nplot_pacf(average_sales_2017);\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/graphics/tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\nCycle\n\n\nHybrid Models\nEssentially all above combined:\nseries = trend + seasons + cycles + error\nRegression algorithmn: * transform target: * for example decision tree. * group target value in training and make prediction of feature by averaging values in a group * transform features: * for example polynormial function. Use mathmatical function. * featues as input combines and transform\nFeature Transformer Extrapolate Target Value (think of this as a point inbetween two discrete value amongst a function) beyound bondary of training set. Same cannot be said for Decision Tree. Random Forest and Gradient boosted decision tree takes the last step.\n#kaggle recommend: 1. linear regression for extrapolate trend 2. transform target to remove trend 3. apply XGBoost to detrended residual\n\nlinbear regression + XGBosst\n\nfrom pathlib import Path\nfrom warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom xgboost import XGBRegressor\n\n\nsimplefilter(\"ignore\")\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n)\n\ndata_dir = Path(\"data/ts-course/\")\nindustries = [\"BuildingMaterials\", \"FoodAndBeverage\"]\nretail = pd.read_csv(\n    data_dir / \"us-retail-sales.csv\",\n    usecols=['Month'] + industries,\n    parse_dates=['Month'],\n    index_col='Month',\n).to_period('D').reindex(columns=industries)\nretail = pd.concat({'Sales': retail}, names=[None, 'Industries'], axis=1) # this is a hayer of hierarachical index\n\nretail.head()\n\n\n\n\n\n\n\n\n\nSales\n\n\nIndustries\nBuildingMaterials\nFoodAndBeverage\n\n\nMonth\n\n\n\n\n\n\n1992-01-01\n8964\n29589\n\n\n1992-02-01\n9023\n28570\n\n\n1992-03-01\n10608\n29682\n\n\n1992-04-01\n11630\n30228\n\n\n1992-05-01\n12327\n31677\n\n\n\n\n\n\n\n\nTips: now retail is hierarchically indexed you can only have to access a top layer column before you can access a bottom layer.\n\ny = retail.copy()\ndp = DeterministicProcess(\n    index=y.index,\n    constant=True,\n    order=2,\n    drop=True\n)\nX = dp.in_sample() # features for training data\n\nidx_train, idx_test = train_test_split(\n    y.index, test_size=12 * 4, shuffle=False,\n)\nX_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\ny_train, y_test = y.loc[idx_train], y.loc[idx_test]\n\n# Fit trend model\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_fit = pd.DataFrame(\n    model.predict(X_train),\n    index=y_train.index, # index are index\n    columns=y_train.columns, # columns are column labels\n)\ny_pred = pd.DataFrame(\n    model.predict(X_test),\n    index=y_test.index,\n    columns=y_test.columns,\n)\n\n\n# Plot\naxs = y_train.plot(color='0.25', subplots=True, sharex=True)\naxs = y_test.plot(color='0.25', subplots=True, sharex=True, ax=axs)\naxs = y_fit.plot(color='C0', subplots=True, sharex=True, ax=axs)\naxs = y_pred.plot(color='C3', subplots=True, sharex=True, ax=axs)\nfor ax in axs: ax.legend([])\n_ = plt.suptitle(\"Trends\")\n\n\n\n\n\n\n\n\nData Transformation before add XGBOOST\n\nWhile the linear regression algorithm is capable of multi-output regression, the XGBoost algorithm is not. To predict multiple series at once with XGBoost, we’ll instead convert these series from wide format, with one time series per column, to long format, with series indexed by categories along rows.\n\n\nX = retail.stack()  # pivot dataset wide to long\ndisplay(X.head())\ny = X.pop('Sales')\n\n\n\n\n\n\n\n\n\n\nSales\n\n\nMonth\nIndustries\n\n\n\n\n\n1992-01-01\nBuildingMaterials\n8964\n\n\nFoodAndBeverage\n29589\n\n\n1992-02-01\nBuildingMaterials\n9023\n\n\nFoodAndBeverage\n28570\n\n\n1992-03-01\nBuildingMaterials\n10608\n\n\n\n\n\n\n\n\n\nprint(\"stack transform default retaill index \\n from {var1} \\n to {var2} \\\n      ( y index)\".format(\nvar1 =type(retail.head().index),\nvar2 =type(y.head().index)\n))\n\nstack transform default retaill index \n from &lt;class 'pandas.core.indexes.period.PeriodIndex'&gt; \n to &lt;class 'pandas.core.indexes.multi.MultiIndex'&gt;       ( y index)\n\n\n\n# Turn row labels into categorical feature columns with a label encoding\nX = X.reset_index('Industries')\nprint(\"X.index is now {var1}\".format(var1=type(X.index)))\n# Label encoding for 'Industries' feature\nfor colname in X.select_dtypes([\"object\", \"category\"]):\n    X[colname], _ = X[colname].factorize()\n\n# Label encoding for annual seasonality\nX[\"Month\"] = X.index.month  # values are 1, 2, ..., 12\n\n# Create splits\nX_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\ny_train, y_test = y.loc[idx_train], y.loc[idx_test]\n\nX.index is now &lt;class 'pandas.core.indexes.period.PeriodIndex'&gt;\n\n\n\n# Pivot wide to long (stack) and convert DataFrame to Series (squeeze)\ny_fit = y_fit.stack().squeeze()    # trend from training set\ny_pred = y_pred.stack().squeeze()  # trend from test set\n\nprint(\"y is now {y_index}\".format(\n    y_index = type(y_fit.index)\n))\n\n# Create residuals (the collection of detrended series) from the training set\ny_resid = y_train - y_fit\n\n# Train XGBoost on the residuals\nxgb = XGBRegressor()\nxgb.fit(X_train, y_resid)\n\n# Add the predicted residuals onto the predicted trends\ny_fit_boosted = xgb.predict(X_train) + y_fit\ny_pred_boosted = xgb.predict(X_test) + y_pred\n\ny is now &lt;class 'pandas.core.indexes.multi.MultiIndex'&gt;\n\n\n\naxs = y_train.unstack(['Industries']).plot(\n    color='0.25', figsize=(11, 5), subplots=True, sharex=True,\n    title=['BuildingMaterials', 'FoodAndBeverage'],\n)\naxs = y_test.unstack(['Industries']).plot(\n    color='0.25', subplots=True, sharex=True, ax=axs,\n)\naxs = y_fit_boosted.unstack(['Industries']).plot(\n    color='C0', subplots=True, sharex=True, ax=axs,\n)\naxs = y_pred_boosted.unstack(['Industries']).plot(\n    color='C3', subplots=True, sharex=True, ax=axs,\n)\nfor ax in axs: ax.legend([])\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\nfrom pathlib import Path\nfrom warnings import simplefilter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\n\ndef plot_multistep(y, every=1, ax=None, palette_kwargs=None):\n    palette_kwargs_ = dict(palette='husl', n_colors=16, desat=None)\n    if palette_kwargs is not None:\n        palette_kwargs_.update(palette_kwargs)\n    palette = sns.color_palette(**palette_kwargs_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.set_prop_cycle(plt.cycler('color', palette))\n    for date, preds in y[::every].iterrows():\n        preds.index = pd.period_range(start=date, periods=len(preds))\n        preds.plot(ax=ax)\n    return ax\n\nflu_trends = pd.read_csv(\"data/ts-course/flu-trends.csv\")\nflu_trends.set_index(\n    pd.PeriodIndex(flu_trends.Week, freq = 'W'),\n    inplace = True\n)\nflu_trends.drop(\"Week\", axis=1, inplace=True)\n\n\ndef make_lags(ts, lags, lead_time=1):\n    return pd.concat(\n        {\n            f'y_lag_{i}': ts.shift(i)\n            for i in range(lead_time, lags + lead_time)\n        },\n        axis=1)\n\ny = flu_trends.FluVisits.copy() #setting this up any change to flu_trend will be copied\nX = make_lags(y, lags=4).fillna(0.0)\n\n\ndef make_multistep_target(ts, steps):\n    return pd.concat(\n        {f'y_step_{i + 1}': ts.shift(-i)\n         for i in range(steps)},\n        axis=1)\ny = make_multistep_target(y, steps=8).dropna()\ny, X = y.align(X, join= \"inner\", axis = 0) # align make index of x and y homogenious\n\nto read more about align here\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns)\ny_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\n\n\ntrain_rmse = mean_squared_error(y_train, y_fit, squared=False)\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))\n\npalette = dict(palette='husl', n_colors=64)\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6))\nax1 = flu_trends.FluVisits[y_fit.index].plot(**plot_params, ax=ax1)\nax1 = plot_multistep(y_fit, ax=ax1, palette_kwargs=palette)\n_ = ax1.legend(['FluVisits (train)', 'Forecast'])\nax2 = flu_trends.FluVisits[y_pred.index].plot(**plot_params, ax=ax2)\nax2 = plot_multistep(y_pred, ax=ax2, palette_kwargs=palette)\n_ = ax2.legend(['FluVisits (test)', 'Forecast'])\n\nTrain RMSE: 389.12\nTest RMSE: 582.33\n\n\n\n\n\n\n\n\n\n\nDirect Strategy - Use XGBoost regreesor\nHere is short cut recursively produce mutli output regression model. &gt; XGBoost can’t produce multiple outputs for regression tasks. But by applying the Direct reduction strategy, we can still use it to produce multi-step forecasts. This is as easy as wrapping it with scikit-learn’s MultiOutputRegressor.\n\nfrom sklearn.multioutput import MultiOutputRegressor\n\nmodel = MultiOutputRegressor(XGBRegressor()) #cheat code\nmodel.fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns)\ny_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\n\n\n#X_train.sample(3)\ny_train.sample(3)\n\n\n\n\n\n\n\n\n\ny_step_1\ny_step_2\ny_step_3\ny_step_4\ny_step_5\ny_step_6\ny_step_7\ny_step_8\n\n\nWeek\n\n\n\n\n\n\n\n\n\n\n\n\n2013-07-08/2013-07-14\n11\n13.0\n14.0\n10.0\n13.0\n13.0\n13.0\n22.0\n\n\n2014-05-19/2014-05-25\n80\n61.0\n47.0\n33.0\n27.0\n19.0\n22.0\n19.0\n\n\n2012-12-24/2012-12-30\n2961\n2303.0\n2291.0\n2258.0\n2012.0\n1510.0\n1134.0\n930.0\n\n\n\n\n\n\n\n\n\ntrain_rmse = mean_squared_error(y_train, y_fit, squared=False)\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))\n\npalette = dict(palette='husl', n_colors=64)\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6))\nax1 = flu_trends.FluVisits[y_fit.index].plot(**plot_params, ax=ax1)\nax1 = plot_multistep(y_fit, ax=ax1, palette_kwargs=palette)\n_ = ax1.legend(['FluVisits (train)', 'Forecast'])\nax2 = flu_trends.FluVisits[y_pred.index].plot(**plot_params, ax=ax2)\nax2 = plot_multistep(y_pred, ax=ax2, palette_kwargs=palette)\n_ = ax2.legend(['FluVisits (test)', 'Forecast'])\n\nTrain RMSE: 1.22\nTest RMSE: 526.45\n\n\n\n\n\n\n\n\n\nRegressorChain() versus MultiOutputRegressor"
  },
  {
    "objectID": "02-09 Kaggle Competition/kaggle-2024-credit-risks/fl-02-08.html",
    "href": "02-09 Kaggle Competition/kaggle-2024-credit-risks/fl-02-08.html",
    "title": "What data look like?",
    "section": "",
    "text": "import pandas as pd\nimport os\nfrom directory_tree import display_tree\nfrom sklearn.metrics import roc_auc_score\nfrom pathlib import Path\nimport duckdb\n\nDATA_FOLDER=\"home-credit-credit-risk-model-stability\"\nassert Path(DATA_FOLDER).exists()\ndisplay_tree(DATA_FOLDER)\n\nhome-credit-credit-risk-model-stability/\n├── csv_files/\n│   ├── test/\n│   │   ├── test_applprev_1_0.csv\n│   │   ├── test_applprev_1_1.csv\n│   │   ├── test_applprev_1_2.csv\n│   │   ├── test_applprev_2.csv\n│   │   ├── test_base.csv\n│   │   ├── test_credit_bureau_a_1_0.csv\n│   │   ├── test_credit_bureau_a_1_1.csv\n│   │   ├── test_credit_bureau_a_1_2.csv\n│   │   ├── test_credit_bureau_a_1_3.csv\n│   │   ├── test_credit_bureau_a_1_4.csv\n│   │   ├── test_credit_bureau_a_2_0.csv\n│   │   ├── test_credit_bureau_a_2_1.csv\n│   │   ├── test_credit_bureau_a_2_10.csv\n│   │   ├── test_credit_bureau_a_2_11.csv\n│   │   ├── test_credit_bureau_a_2_2.csv\n│   │   ├── test_credit_bureau_a_2_3.csv\n│   │   ├── test_credit_bureau_a_2_4.csv\n│   │   ├── test_credit_bureau_a_2_5.csv\n│   │   ├── test_credit_bureau_a_2_6.csv\n│   │   ├── test_credit_bureau_a_2_7.csv\n│   │   ├── test_credit_bureau_a_2_8.csv\n│   │   ├── test_credit_bureau_a_2_9.csv\n│   │   ├── test_credit_bureau_b_1.csv\n│   │   ├── test_credit_bureau_b_2.csv\n│   │   ├── test_debitcard_1.csv\n│   │   ├── test_deposit_1.csv\n│   │   ├── test_other_1.csv\n│   │   ├── test_person_1.csv\n│   │   ├── test_person_2.csv\n│   │   ├── test_static_0_0.csv\n│   │   ├── test_static_0_1.csv\n│   │   ├── test_static_0_2.csv\n│   │   ├── test_static_cb_0.csv\n│   │   ├── test_tax_registry_a_1.csv\n│   │   ├── test_tax_registry_b_1.csv\n│   │   └── test_tax_registry_c_1.csv\n│   └── train/\n│       ├── train_applprev_1_0.csv\n│       ├── train_applprev_1_1.csv\n│       ├── train_applprev_2.csv\n│       ├── train_base.csv\n│       ├── train_credit_bureau_a_1_0.csv\n│       ├── train_credit_bureau_a_1_1.csv\n│       ├── train_credit_bureau_a_1_2.csv\n│       ├── train_credit_bureau_a_1_3.csv\n│       ├── train_credit_bureau_a_2_0.csv\n│       ├── train_credit_bureau_a_2_1.csv\n│       ├── train_credit_bureau_a_2_10.csv\n│       ├── train_credit_bureau_a_2_2.csv\n│       ├── train_credit_bureau_a_2_3.csv\n│       ├── train_credit_bureau_a_2_4.csv\n│       ├── train_credit_bureau_a_2_5.csv\n│       ├── train_credit_bureau_a_2_6.csv\n│       ├── train_credit_bureau_a_2_7.csv\n│       ├── train_credit_bureau_a_2_8.csv\n│       ├── train_credit_bureau_a_2_9.csv\n│       ├── train_credit_bureau_b_1.csv\n│       ├── train_credit_bureau_b_2.csv\n│       ├── train_debitcard_1.csv\n│       ├── train_deposit_1.csv\n│       ├── train_other_1.csv\n│       ├── train_person_1.csv\n│       ├── train_person_2.csv\n│       ├── train_static_0_0.csv\n│       ├── train_static_0_1.csv\n│       ├── train_static_cb_0.csv\n│       ├── train_tax_registry_a_1.csv\n│       ├── train_tax_registry_b_1.csv\n│       └── train_tax_registry_c_1.csv\n├── feature_definitions.csv\n├── parquet_files/\n│   ├── test/\n│   │   ├── test_applprev_1_0.parquet\n│   │   ├── test_applprev_1_1.parquet\n│   │   ├── test_applprev_1_2.parquet\n│   │   ├── test_applprev_2.parquet\n│   │   ├── test_base.parquet\n│   │   ├── test_credit_bureau_a_1_0.parquet\n│   │   ├── test_credit_bureau_a_1_1.parquet\n│   │   ├── test_credit_bureau_a_1_2.parquet\n│   │   ├── test_credit_bureau_a_1_3.parquet\n│   │   ├── test_credit_bureau_a_1_4.parquet\n│   │   ├── test_credit_bureau_a_2_0.parquet\n│   │   ├── test_credit_bureau_a_2_1.parquet\n│   │   ├── test_credit_bureau_a_2_10.parquet\n│   │   ├── test_credit_bureau_a_2_11.parquet\n│   │   ├── test_credit_bureau_a_2_2.parquet\n│   │   ├── test_credit_bureau_a_2_3.parquet\n│   │   ├── test_credit_bureau_a_2_4.parquet\n│   │   ├── test_credit_bureau_a_2_5.parquet\n│   │   ├── test_credit_bureau_a_2_6.parquet\n│   │   ├── test_credit_bureau_a_2_7.parquet\n│   │   ├── test_credit_bureau_a_2_8.parquet\n│   │   ├── test_credit_bureau_a_2_9.parquet\n│   │   ├── test_credit_bureau_b_1.parquet\n│   │   ├── test_credit_bureau_b_2.parquet\n│   │   ├── test_debitcard_1.parquet\n│   │   ├── test_deposit_1.parquet\n│   │   ├── test_other_1.parquet\n│   │   ├── test_person_1.parquet\n│   │   ├── test_person_2.parquet\n│   │   ├── test_static_0_0.parquet\n│   │   ├── test_static_0_1.parquet\n│   │   ├── test_static_0_2.parquet\n│   │   ├── test_static_cb_0.parquet\n│   │   ├── test_tax_registry_a_1.parquet\n│   │   ├── test_tax_registry_b_1.parquet\n│   │   └── test_tax_registry_c_1.parquet\n│   └── train/\n│       ├── train_applprev_1_0.parquet\n│       ├── train_applprev_1_1.parquet\n│       ├── train_applprev_2.parquet\n│       ├── train_base.parquet\n│       ├── train_credit_bureau_a_1_0.parquet\n│       ├── train_credit_bureau_a_1_1.parquet\n│       ├── train_credit_bureau_a_1_2.parquet\n│       ├── train_credit_bureau_a_1_3.parquet\n│       ├── train_credit_bureau_a_2_0.parquet\n│       ├── train_credit_bureau_a_2_1.parquet\n│       ├── train_credit_bureau_a_2_10.parquet\n│       ├── train_credit_bureau_a_2_2.parquet\n│       ├── train_credit_bureau_a_2_3.parquet\n│       ├── train_credit_bureau_a_2_4.parquet\n│       ├── train_credit_bureau_a_2_5.parquet\n│       ├── train_credit_bureau_a_2_6.parquet\n│       ├── train_credit_bureau_a_2_7.parquet\n│       ├── train_credit_bureau_a_2_8.parquet\n│       ├── train_credit_bureau_a_2_9.parquet\n│       ├── train_credit_bureau_b_1.parquet\n│       ├── train_credit_bureau_b_2.parquet\n│       ├── train_debitcard_1.parquet\n│       ├── train_deposit_1.parquet\n│       ├── train_other_1.parquet\n│       ├── train_person_1.parquet\n│       ├── train_person_2.parquet\n│       ├── train_static_0_0.parquet\n│       ├── train_static_0_1.parquet\n│       ├── train_static_cb_0.parquet\n│       ├── train_tax_registry_a_1.parquet\n│       ├── train_tax_registry_b_1.parquet\n│       └── train_tax_registry_c_1.parquet\n└── sample_submission.csv\n\n\n\nDATA_DIR='home-credit-credit-risk-model-stability/parquet_files/train'\np = Path(DATA_DIR)\n\n\nimport ibis\nimport ibis\nimport ibis.selectors as s\n\n# Set up ibis\nibis.options.interactive = True\n\n\n\nassert (p / 'train_base.parquet').exists\nprint('Your is training target is column `target`')\nibis.read_parquet((p / 'train_base.parquet')).head(3)\n\nYour is training target is column `target`\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┓\n┃ case_id ┃ date_decision ┃ MONTH  ┃ WEEK_NUM ┃ target ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━┩\n│ int64   │ string        │ int64  │ int64    │ int64  │\n├─────────┼───────────────┼────────┼──────────┼────────┤\n│       0 │ 2019-01-03    │ 201901 │        0 │      0 │\n│       1 │ 2019-01-03    │ 201901 │        0 │      0 │\n│       2 │ 2019-01-04    │ 201901 │        0 │      0 │\n└─────────┴───────────────┴────────┴──────────┴────────┘\n\n\n\nFrom reading this notebook ‘https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook’\n\nLast letter of column name is ‘P’ or ‘A’ is float64\nStrings are categorical variables\n\nObserver data below:\n\nthere are cardinality amont the tables.\n\n\n# load data into directory\ndata_files=list(p.glob('*_1.parquet'))#load_all data\nds = {f.name:ibis.read_parquet(f) for f in data_files}\n# preview of data\nfor i, j in ds.items():\n    print(i)\n    print(display(j.head(3)))\n\ntrain_other_1.parquet\nNone\ntrain_credit_bureau_a_1_1.parquet\nNone\ntrain_static_0_1.parquet\nNone\ntrain_tax_registry_c_1.parquet\nNone\ntrain_person_1.parquet\nNone\ntrain_credit_bureau_b_1.parquet\nNone\ntrain_tax_registry_b_1.parquet\nNone\ntrain_debitcard_1.parquet\nNone\ntrain_applprev_1_1.parquet\nNone\ntrain_tax_registry_a_1.parquet\nNone\ntrain_deposit_1.parquet\nNone\ntrain_credit_bureau_a_2_1.parquet\nNone\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ case_id ┃ amtdebitincoming_4809443A ┃ amtdebitoutgoing_4809440A ┃ amtdepositbalance_4809441A ┃ amtdepositincoming_4809444A ┃ amtdepositoutgoing_4809442A ┃ num_group1 ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ int64   │ float64                   │ float64                   │ float64                    │ float64                     │ float64                     │ int64      │\n├─────────┼───────────────────────────┼───────────────────────────┼────────────────────────────┼─────────────────────────────┼─────────────────────────────┼────────────┤\n│   43801 │                12466.6010 │                12291.2000 │                      914.2 │                         0.0 │                   304.80002 │          0 │\n│   43991 │                 3333.4001 │                 3273.4001 │                        0.0 │                         0.0 │                     0.00000 │          0 │\n│   44001 │                10000.0000 │                10000.0000 │                        0.0 │                         0.0 │                     0.00000 │          0 │\n└─────────┴───────────────────────────┴───────────────────────────┴────────────────────────────┴─────────────────────────────┴─────────────────────────────┴────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ annualeffectiverate_199L ┃ annualeffectiverate_63L ┃ classificationofcontr_13M ┃ classificationofcontr_400M ┃ contractst_545M ┃ contractst_964M ┃ contractsum_5085717L ┃ credlmt_230A ┃ credlmt_935A ┃ dateofcredend_289D ┃ dateofcredend_353D ┃ dateofcredstart_181D ┃ dateofcredstart_739D ┃ dateofrealrepmt_138D ┃ debtoutstand_525A ┃ debtoverdue_47A ┃ description_351M ┃ dpdmax_139P ┃ dpdmax_757P ┃ dpdmaxdatemonth_442T ┃ dpdmaxdatemonth_89T ┃ dpdmaxdateyear_596T ┃ dpdmaxdateyear_896T ┃ financialinstitution_382M ┃ financialinstitution_591M ┃ instlamount_768A ┃ instlamount_852A ┃ interestrate_508L ┃ lastupdate_1112D ┃ lastupdate_388D ┃ monthlyinstlamount_332A ┃ monthlyinstlamount_674A ┃ nominalrate_281L ┃ nominalrate_498L ┃ num_group1 ┃ numberofcontrsvalue_258L ┃ numberofcontrsvalue_358L ┃ numberofinstls_229L ┃ numberofinstls_320L ┃ numberofoutstandinstls_520L ┃ numberofoutstandinstls_59L ┃ numberofoverdueinstlmax_1039L ┃ numberofoverdueinstlmax_1151L ┃ numberofoverdueinstlmaxdat_148D ┃ numberofoverdueinstlmaxdat_641D ┃ numberofoverdueinstls_725L ┃ numberofoverdueinstls_834L ┃ outstandingamount_354A ┃ outstandingamount_362A ┃ overdueamount_31A ┃ overdueamount_659A ┃ overdueamountmax2_14A ┃ overdueamountmax2_398A ┃ overdueamountmax2date_1002D ┃ overdueamountmax2date_1142D ┃ overdueamountmax_155A ┃ overdueamountmax_35A ┃ overdueamountmaxdatemonth_284T ┃ overdueamountmaxdatemonth_365T ┃ overdueamountmaxdateyear_2T ┃ overdueamountmaxdateyear_994T ┃ periodicityofpmts_1102L ┃ periodicityofpmts_837L ┃ prolongationcount_1120L ┃ prolongationcount_599L ┃ purposeofcred_426M ┃ purposeofcred_874M ┃ refreshdate_3813885D ┃ residualamount_488A ┃ residualamount_856A ┃ subjectrole_182M ┃ subjectrole_93M ┃ totalamount_6A ┃ totalamount_996A ┃ totaldebtoverduevalue_178A ┃ totaldebtoverduevalue_718A ┃ totaloutstanddebtvalue_39A ┃ totaloutstanddebtvalue_668A ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64                  │ float64                 │ string                    │ string                     │ string          │ string          │ float64              │ float64      │ float64      │ string             │ string             │ string               │ string               │ string               │ float64           │ float64         │ string           │ float64     │ float64     │ float64              │ float64             │ float64             │ float64             │ string                    │ string                    │ float64          │ float64          │ float64           │ string           │ string          │ float64                 │ float64                 │ float64          │ float64          │ int64      │ float64                  │ float64                  │ float64             │ float64             │ float64                     │ float64                    │ float64                       │ float64                       │ string                          │ string                          │ float64                    │ float64                    │ float64                │ float64                │ float64           │ float64            │ float64               │ float64                │ string                      │ string                      │ float64               │ float64              │ float64                        │ float64                        │ float64                     │ float64                       │ float64                 │ float64                │ float64                 │ float64                │ string             │ string             │ string               │ float64             │ float64             │ string           │ string          │ float64        │ float64          │ float64                    │ float64                    │ float64                    │ float64                     │\n├─────────┼──────────────────────────┼─────────────────────────┼───────────────────────────┼────────────────────────────┼─────────────────┼─────────────────┼──────────────────────┼──────────────┼──────────────┼────────────────────┼────────────────────┼──────────────────────┼──────────────────────┼──────────────────────┼───────────────────┼─────────────────┼──────────────────┼─────────────┼─────────────┼──────────────────────┼─────────────────────┼─────────────────────┼─────────────────────┼───────────────────────────┼───────────────────────────┼──────────────────┼──────────────────┼───────────────────┼──────────────────┼─────────────────┼─────────────────────────┼─────────────────────────┼──────────────────┼──────────────────┼────────────┼──────────────────────────┼──────────────────────────┼─────────────────────┼─────────────────────┼─────────────────────────────┼────────────────────────────┼───────────────────────────────┼───────────────────────────────┼─────────────────────────────────┼─────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────┼────────────────────────┼───────────────────┼────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────────────────┼──────────────────────┼────────────────────────────────┼────────────────────────────────┼─────────────────────────────┼───────────────────────────────┼─────────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┼────────────────────┼────────────────────┼──────────────────────┼─────────────────────┼─────────────────────┼──────────────────┼─────────────────┼────────────────┼──────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼─────────────────────────────┤\n│   19694 │                     NULL │                    NULL │ ea6782cc                  │ a55475b1                   │ 7241344e        │ a55475b1        │                 NULL │         NULL │          0.0 │ 2020-09-29         │ NULL               │ NULL                 │ 2014-09-29           │ NULL                 │          43315.26 │             0.0 │ a55475b1         │        20.0 │        NULL │                 NULL │                12.0 │              2017.0 │                NULL │ a55475b1                  │ P204_66_73                │              0.0 │             NULL │              NULL │ 2019-06-27       │ NULL            │                  0.0000 │                    NULL │             NULL │             NULL │          0 │                      2.0 │                      8.0 │                NULL │                NULL │                        NULL │                       NULL │                          22.0 │                          NULL │ NULL                            │ 2017-11-21                      │                        0.0 │                       NULL │                   NULL │                   NULL │              NULL │                0.0 │              2967.666 │                   NULL │ NULL                        │ 2018-03-31                  │              2967.666 │                 NULL │                           NULL │                            4.0 │                      2018.0 │                          NULL │                    NULL │                   NULL │                    NULL │                   NULL │ 60c73645           │ a55475b1           │ NULL                 │                NULL │                 0.0 │ ab3c25cf         │ ab3c25cf        │           NULL │             NULL │                        0.0 │                        0.0 │                   43315.26 │                         0.0 │\n│   19694 │                     NULL │                    NULL │ ea6782cc                  │ a55475b1                   │ 7241344e        │ a55475b1        │                 NULL │         NULL │         NULL │ 2020-04-14         │ NULL               │ NULL                 │ 2016-04-14           │ NULL                 │              NULL │            NULL │ a55475b1         │        19.0 │        NULL │                 NULL │                 3.0 │              2018.0 │                NULL │ a55475b1                  │ P150_136_157              │             NULL │             NULL │              NULL │ 2019-07-03       │ NULL            │               5155.5425 │                    NULL │             NULL │             NULL │          1 │                     NULL │                     NULL │                NULL │                47.0 │                        NULL │                        9.0 │                          21.0 │                          NULL │ NULL                            │ 2018-03-08                      │                        0.0 │                       NULL │                   NULL │               43315.26 │              NULL │                0.0 │              5155.344 │                   NULL │ NULL                        │ 2017-01-19                  │              5061.540 │                 NULL │                           NULL │                           10.0 │                      2017.0 │                          NULL │                    NULL │                   30.0 │                    NULL │                   NULL │ 96a8fdfe           │ a55475b1           │ NULL                 │                NULL │                NULL │ a55475b1         │ a55475b1        │           NULL │         170000.0 │                       NULL │                       NULL │                       NULL │                        NULL │\n│   19694 │                     NULL │                    NULL │ a55475b1                  │ a55475b1                   │ a55475b1        │ a55475b1        │                 NULL │         NULL │         NULL │ NULL               │ NULL               │ NULL                 │ NULL                 │ NULL                 │              NULL │            NULL │ a55475b1         │        NULL │        NULL │                 NULL │                NULL │                NULL │                NULL │ a55475b1                  │ a55475b1                  │             NULL │             NULL │              NULL │ NULL             │ NULL            │                    NULL │                    NULL │             NULL │             NULL │          2 │                     NULL │                     NULL │                NULL │                NULL │                        NULL │                       NULL │                          NULL │                          NULL │ NULL                            │ NULL                            │                       NULL │                       NULL │                   NULL │                   NULL │              NULL │               NULL │                  NULL │                   NULL │ NULL                        │ NULL                        │                  NULL │                 NULL │                           NULL │                           NULL │                        NULL │                          NULL │                    NULL │                   NULL │                    NULL │                   NULL │ a55475b1           │ a55475b1           │ 2019-07-10           │                NULL │                NULL │ a55475b1         │ a55475b1        │           NULL │             NULL │                       NULL │                       NULL │                       NULL │                        NULL │\n└─────────┴──────────────────────────┴─────────────────────────┴───────────────────────────┴────────────────────────────┴─────────────────┴─────────────────┴──────────────────────┴──────────────┴──────────────┴────────────────────┴────────────────────┴──────────────────────┴──────────────────────┴──────────────────────┴───────────────────┴─────────────────┴──────────────────┴─────────────┴─────────────┴──────────────────────┴─────────────────────┴─────────────────────┴─────────────────────┴───────────────────────────┴───────────────────────────┴──────────────────┴──────────────────┴───────────────────┴──────────────────┴─────────────────┴─────────────────────────┴─────────────────────────┴──────────────────┴──────────────────┴────────────┴──────────────────────────┴──────────────────────────┴─────────────────────┴─────────────────────┴─────────────────────────────┴────────────────────────────┴───────────────────────────────┴───────────────────────────────┴─────────────────────────────────┴─────────────────────────────────┴────────────────────────────┴────────────────────────────┴────────────────────────┴────────────────────────┴───────────────────┴────────────────────┴───────────────────────┴────────────────────────┴─────────────────────────────┴─────────────────────────────┴───────────────────────┴──────────────────────┴────────────────────────────────┴────────────────────────────────┴─────────────────────────────┴───────────────────────────────┴─────────────────────────┴────────────────────────┴─────────────────────────┴────────────────────────┴────────────────────┴────────────────────┴──────────────────────┴─────────────────────┴─────────────────────┴──────────────────┴─────────────────┴────────────────┴──────────────────┴────────────────────────────┴────────────────────────────┴────────────────────────────┴─────────────────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ actualdpdtolerance_344P ┃ amtinstpaidbefduel24m_4187115A ┃ annuity_780A ┃ annuitynextmonth_57A ┃ applicationcnt_361L ┃ applications30d_658L ┃ applicationscnt_1086L ┃ applicationscnt_464L ┃ applicationscnt_629L ┃ applicationscnt_867L ┃ avgdbddpdlast24m_3658932P ┃ avgdbddpdlast3m_4187120P ┃ avgdbdtollast24m_4525197P ┃ avgdpdtolclosure24_3658938P ┃ avginstallast24m_3658937A ┃ avglnamtstart24m_4525187A ┃ avgmaxdpdlast9m_3716943P ┃ avgoutstandbalancel6m_4187114A ┃ avgpmtlast12m_4525200A ┃ bankacctype_710L ┃ cardtype_51L ┃ clientscnt12m_3712952L ┃ clientscnt3m_3712950L ┃ clientscnt6m_3712949L ┃ clientscnt_100L ┃ clientscnt_1022L ┃ clientscnt_1071L ┃ clientscnt_1130L ┃ clientscnt_136L ┃ clientscnt_157L ┃ clientscnt_257L ┃ clientscnt_304L ┃ clientscnt_360L ┃ clientscnt_493L ┃ clientscnt_533L ┃ clientscnt_887L ┃ clientscnt_946L ┃ cntincpaycont9m_3716944L ┃ cntpmts24_3658933L ┃ commnoinclast6m_3546845L ┃ credamount_770A ┃ credtype_322L ┃ currdebt_22A ┃ currdebtcredtyperange_828A ┃ datefirstoffer_1144D ┃ datelastinstal40dpd_247D ┃ datelastunpaid_3546854D ┃ daysoverduetolerancedd_3976961L ┃ deferredmnthsnum_166L ┃ disbursedcredamount_1113A ┃ disbursementtype_67L ┃ downpmt_116A ┃ dtlastpmtallstes_4499206D ┃ eir_270L ┃ equalitydataagreement_891L ┃ equalityempfrom_62L ┃ firstclxcampaign_1125D ┃ firstdatedue_489D ┃ homephncnt_628L ┃ inittransactionamount_650A ┃ inittransactioncode_186L ┃ interestrate_311L ┃ interestrategrace_34L ┃ isbidproduct_1095L ┃ isbidproductrequest_292L ┃ isdebitcard_729L ┃ lastactivateddate_801D ┃ lastapplicationdate_877D ┃ lastapprcommoditycat_1041M ┃ lastapprcommoditytypec_5251766M ┃ lastapprcredamount_781A ┃ lastapprdate_640D ┃ lastcancelreason_561M ┃ lastdelinqdate_224D ┃ lastdependentsnum_448L ┃ lastotherinc_902A ┃ lastotherlnsexpense_631A ┃ lastrejectcommoditycat_161M ┃ lastrejectcommodtypec_5251769M ┃ lastrejectcredamount_222A ┃ lastrejectdate_50D ┃ lastrejectreason_759M ┃ lastrejectreasonclient_4145040M ┃ lastrepayingdate_696D ┃ lastst_736L ┃ maininc_215A ┃ mastercontrelectronic_519L ┃ mastercontrexist_109L ┃ maxannuity_159A ┃ maxannuity_4075009A ┃ maxdbddpdlast1m_3658939P ┃ maxdbddpdtollast12m_3658940P ┃ maxdbddpdtollast6m_4187119P ┃ maxdebt4_972A ┃ maxdpdfrom6mto36m_3546853P ┃ maxdpdinstldate_3546855D ┃ maxdpdinstlnum_3546846P ┃ maxdpdlast12m_727P ┃ maxdpdlast24m_143P ┃ maxdpdlast3m_392P ┃ maxdpdlast6m_474P ┃ maxdpdlast9m_1059P ┃ maxdpdtolerance_374P ┃ maxinstallast24m_3658928A ┃ maxlnamtstart6m_4525199A ┃ maxoutstandbalancel12m_4187113A ┃ maxpmtlast3m_4525190A ┃ mindbddpdlast24m_3658935P ┃ mindbdtollast24m_4525191P ┃ mobilephncnt_593L ┃ monthsannuity_845L ┃ numactivecreds_622L ┃ numactivecredschannel_414L ┃ numactiverelcontr_750L ┃ numcontrs3months_479L ┃ numincomingpmts_3546848L ┃ numinstlallpaidearly3d_817L ┃ numinstls_657L ┃ numinstlsallpaid_934L ┃ numinstlswithdpd10_728L ┃ numinstlswithdpd5_4187116L ┃ numinstlswithoutdpd_562L ┃ numinstmatpaidtearly2d_4499204L ┃ numinstpaid_4499208L ┃ numinstpaidearly3d_3546850L ┃ numinstpaidearly3dest_4493216L ┃ numinstpaidearly5d_1087L ┃ numinstpaidearly5dest_4493211L ┃ numinstpaidearly5dobd_4499205L ┃ numinstpaidearly_338L ┃ numinstpaidearlyest_4493214L ┃ numinstpaidlastcontr_4325080L ┃ numinstpaidlate1d_3546852L ┃ numinstregularpaid_973L ┃ numinstregularpaidest_4493210L ┃ numinsttopaygr_769L ┃ numinsttopaygrest_4493213L ┃ numinstunpaidmax_3546851L ┃ numinstunpaidmaxest_4493212L ┃ numnotactivated_1143L ┃ numpmtchanneldd_318L ┃ numrejects9m_859L ┃ opencred_647L ┃ paytype1st_925L ┃ paytype_783L ┃ payvacationpostpone_4187118D ┃ pctinstlsallpaidearl3d_427L ┃ pctinstlsallpaidlat10d_839L ┃ pctinstlsallpaidlate1d_3546856L ┃ pctinstlsallpaidlate4d_3546849L ┃ pctinstlsallpaidlate6d_3546844L ┃ pmtnum_254L ┃ posfpd10lastmonth_333P ┃ posfpd30lastmonth_3976960P ┃ posfstqpd30lastmonth_3976962P ┃ previouscontdistrict_112M ┃ price_1097A ┃ sellerplacecnt_915L ┃ sellerplacescnt_216L ┃ sumoutstandtotal_3546847A ┃ sumoutstandtotalest_4493215A ┃ totaldebt_9A ┃ totalsettled_863A ┃ totinstallast1m_4525188A ┃ twobodfilling_608L ┃ typesuite_864L ┃ validfrom_1069D ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64                 │ float64                        │ float64      │ float64              │ float64             │ float64              │ float64               │ float64              │ float64              │ float64              │ float64                   │ float64                  │ float64                   │ float64                     │ float64                   │ float64                   │ float64                  │ float64                        │ float64                │ string           │ string       │ float64                │ float64               │ float64               │ float64         │ float64          │ float64          │ float64          │ float64         │ float64         │ float64         │ float64         │ float64         │ float64         │ float64         │ float64         │ float64         │ float64                  │ float64            │ float64                  │ float64         │ string        │ float64      │ float64                    │ string               │ string                   │ string                  │ float64                         │ float64               │ float64                   │ string               │ float64      │ string                    │ float64  │ boolean                    │ boolean             │ string                 │ string            │ float64         │ float64                    │ string                   │ float64           │ float64               │ boolean            │ boolean                  │ boolean          │ string                 │ string                   │ string                     │ string                          │ float64                 │ string            │ string                │ string              │ float64                │ float64           │ float64                  │ string                      │ string                         │ float64                   │ string             │ string                │ string                          │ string                │ string      │ float64      │ float64                    │ float64               │ float64         │ float64             │ float64                  │ float64                      │ float64                     │ float64       │ float64                    │ string                   │ float64                 │ float64            │ float64            │ float64           │ float64           │ float64            │ float64              │ float64                   │ float64                  │ float64                         │ float64               │ float64                   │ float64                   │ float64           │ float64            │ float64             │ float64                    │ float64                │ float64               │ float64                  │ float64                     │ float64        │ float64               │ float64                 │ float64                    │ float64                  │ float64                         │ float64              │ float64                     │ float64                        │ float64                  │ float64                        │ float64                        │ float64               │ float64                      │ float64                       │ float64                    │ float64                 │ float64                        │ float64             │ float64                    │ float64                   │ float64                      │ float64               │ float64              │ float64           │ boolean       │ string          │ string       │ string                       │ float64                     │ float64                     │ float64                         │ float64                         │ float64                         │ float64     │ float64                │ float64                    │ float64                       │ string                    │ float64     │ float64             │ float64              │ float64                   │ float64                      │ float64      │ float64           │ float64                  │ string             │ string         │ string          │\n├─────────┼─────────────────────────┼────────────────────────────────┼──────────────┼──────────────────────┼─────────────────────┼──────────────────────┼───────────────────────┼──────────────────────┼──────────────────────┼──────────────────────┼───────────────────────────┼──────────────────────────┼───────────────────────────┼─────────────────────────────┼───────────────────────────┼───────────────────────────┼──────────────────────────┼────────────────────────────────┼────────────────────────┼──────────────────┼──────────────┼────────────────────────┼───────────────────────┼───────────────────────┼─────────────────┼──────────────────┼──────────────────┼──────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼──────────────────────────┼────────────────────┼──────────────────────────┼─────────────────┼───────────────┼──────────────┼────────────────────────────┼──────────────────────┼──────────────────────────┼─────────────────────────┼─────────────────────────────────┼───────────────────────┼───────────────────────────┼──────────────────────┼──────────────┼───────────────────────────┼──────────┼────────────────────────────┼─────────────────────┼────────────────────────┼───────────────────┼─────────────────┼────────────────────────────┼──────────────────────────┼───────────────────┼───────────────────────┼────────────────────┼──────────────────────────┼──────────────────┼────────────────────────┼──────────────────────────┼────────────────────────────┼─────────────────────────────────┼─────────────────────────┼───────────────────┼───────────────────────┼─────────────────────┼────────────────────────┼───────────────────┼──────────────────────────┼─────────────────────────────┼────────────────────────────────┼───────────────────────────┼────────────────────┼───────────────────────┼─────────────────────────────────┼───────────────────────┼─────────────┼──────────────┼────────────────────────────┼───────────────────────┼─────────────────┼─────────────────────┼──────────────────────────┼──────────────────────────────┼─────────────────────────────┼───────────────┼────────────────────────────┼──────────────────────────┼─────────────────────────┼────────────────────┼────────────────────┼───────────────────┼───────────────────┼────────────────────┼──────────────────────┼───────────────────────────┼──────────────────────────┼─────────────────────────────────┼───────────────────────┼───────────────────────────┼───────────────────────────┼───────────────────┼────────────────────┼─────────────────────┼────────────────────────────┼────────────────────────┼───────────────────────┼──────────────────────────┼─────────────────────────────┼────────────────┼───────────────────────┼─────────────────────────┼────────────────────────────┼──────────────────────────┼─────────────────────────────────┼──────────────────────┼─────────────────────────────┼────────────────────────────────┼──────────────────────────┼────────────────────────────────┼────────────────────────────────┼───────────────────────┼──────────────────────────────┼───────────────────────────────┼────────────────────────────┼─────────────────────────┼────────────────────────────────┼─────────────────────┼────────────────────────────┼───────────────────────────┼──────────────────────────────┼───────────────────────┼──────────────────────┼───────────────────┼───────────────┼─────────────────┼──────────────┼──────────────────────────────┼─────────────────────────────┼─────────────────────────────┼─────────────────────────────────┼─────────────────────────────────┼─────────────────────────────────┼─────────────┼────────────────────────┼────────────────────────────┼───────────────────────────────┼───────────────────────────┼─────────────┼─────────────────────┼──────────────────────┼───────────────────────────┼──────────────────────────────┼──────────────┼───────────────────┼──────────────────────────┼────────────────────┼────────────────┼─────────────────┤\n│   40626 │                    NULL │                           NULL │    1976.2001 │                  0.0 │                 0.0 │                  0.0 │                   0.0 │                  1.0 │                  0.0 │                  0.0 │                      NULL │                     NULL │                      NULL │                        NULL │                      NULL │                      NULL │                     NULL │                           NULL │                   NULL │ NULL             │ NULL         │                    0.0 │                   0.0 │                   0.0 │             0.0 │              0.0 │              0.0 │              0.0 │            NULL │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │                     NULL │               NULL │                      0.0 │         36000.0 │ CAL           │          0.0 │                        0.0 │ NULL                 │ NULL                     │ NULL                    │                            NULL │                   0.0 │                   36000.0 │ GBA                  │          0.0 │ NULL                      │     0.28 │ NULL                       │ NULL                │ NULL                   │ NULL              │             0.0 │                       NULL │ CASH                     │              0.28 │                  NULL │ False              │ NULL                     │ NULL             │ NULL                   │ NULL                     │ a55475b1                   │ a55475b1                        │                    NULL │ NULL              │ a55475b1              │ NULL                │                   NULL │              NULL │                     NULL │ a55475b1                    │ a55475b1                       │                      NULL │ NULL               │ a55475b1              │ a55475b1                        │ NULL                  │ NULL        │         NULL │                        0.0 │                   0.0 │             0.0 │                NULL │                     NULL │                         NULL │                        NULL │           0.0 │                        0.0 │ NULL                     │                    NULL │                0.0 │                0.0 │               0.0 │               0.0 │                0.0 │                  0.0 │                      NULL │                     NULL │                            NULL │                  NULL │                      NULL │                      NULL │               1.0 │               NULL │                 0.0 │                        0.0 │                    0.0 │                   0.0 │                     NULL │                        NULL │            0.0 │                  NULL │                    NULL │                       NULL │                     NULL │                            NULL │                 NULL │                        NULL │                           NULL │                     NULL │                           NULL │                           NULL │                  NULL │                         NULL │                          NULL │                       NULL │                    NULL │                           NULL │                NULL │                       NULL │                      NULL │                         NULL │                   0.0 │                  0.0 │               0.0 │ NULL          │ OTHER           │ OTHER        │ NULL                         │                        NULL │                        NULL │                            NULL │                            NULL │                            NULL │        24.0 │                    0.0 │                        0.0 │                          NULL │ a55475b1                  │        NULL │                 0.0 │                  0.0 │                      NULL │                         NULL │          0.0 │               0.0 │                     NULL │ FO                 │ AL             │ NULL            │\n│   40704 │                    NULL │                           NULL │    3731.2000 │                  0.0 │                 0.0 │                  0.0 │                   0.0 │                  0.0 │                  0.0 │                  0.0 │                      NULL │                     NULL │                      NULL │                        NULL │                      NULL │                      NULL │                     NULL │                           NULL │                   NULL │ NULL             │ NULL         │                    0.0 │                   0.0 │                   0.0 │             0.0 │              0.0 │              0.0 │              0.0 │            NULL │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │                     NULL │               NULL │                      0.0 │         30000.0 │ CAL           │          0.0 │                        0.0 │ NULL                 │ NULL                     │ NULL                    │                            NULL │                   0.0 │                   30000.0 │ GBA                  │          0.0 │ NULL                      │     0.45 │ NULL                       │ NULL                │ NULL                   │ NULL              │             1.0 │                       NULL │ CASH                     │              0.45 │                  NULL │ False              │ NULL                     │ NULL             │ NULL                   │ 2018-11-20               │ a55475b1                   │ a55475b1                        │                    NULL │ NULL              │ P94_109_143           │ NULL                │                   NULL │              NULL │                     NULL │ a55475b1                    │ a55475b1                       │                   54000.0 │ 2018-11-20         │ P198_131_9            │ P94_109_143                     │ NULL                  │ D           │         NULL │                        0.0 │                   0.0 │             0.0 │                NULL │                     NULL │                         NULL │                        NULL │           0.0 │                        0.0 │ NULL                     │                    NULL │                0.0 │                0.0 │               0.0 │               0.0 │                0.0 │                  0.0 │                      NULL │                     NULL │                            NULL │                  NULL │                      NULL │                      NULL │               2.0 │               NULL │                 0.0 │                        0.0 │                    0.0 │                   0.0 │                     NULL │                        NULL │            0.0 │                  NULL │                    NULL │                       NULL │                     NULL │                            NULL │                 NULL │                        NULL │                           NULL │                     NULL │                           NULL │                           NULL │                  NULL │                         NULL │                          NULL │                       NULL │                    NULL │                           NULL │                NULL │                       NULL │                      NULL │                         NULL │                   0.0 │                  0.0 │               0.0 │ False         │ OTHER           │ OTHER        │ NULL                         │                        NULL │                        NULL │                            NULL │                            NULL │                            NULL │        12.0 │                   NULL │                       NULL │                          NULL │ a55475b1                  │        NULL │                 0.0 │                  0.0 │                      NULL │                         NULL │          0.0 │               0.0 │                     NULL │ FO                 │ AL             │ NULL            │\n│   40734 │                    NULL │                           NULL │    3731.2000 │                  0.0 │                 0.0 │                  1.0 │                   0.0 │                  0.0 │                  0.0 │                  1.0 │                      NULL │                     NULL │                      NULL │                        NULL │                      NULL │                      NULL │                     NULL │                           NULL │                   NULL │ NULL             │ NULL         │                    0.0 │                   0.0 │                   0.0 │             0.0 │              0.0 │              0.0 │              0.0 │            NULL │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             1.0 │             0.0 │             0.0 │                     NULL │               NULL │                      0.0 │         30000.0 │ CAL           │          0.0 │                        0.0 │ NULL                 │ NULL                     │ NULL                    │                            NULL │                   0.0 │                   30000.0 │ GBA                  │          0.0 │ NULL                      │     0.45 │ NULL                       │ NULL                │ NULL                   │ NULL              │             0.0 │                       NULL │ CASH                     │              0.45 │                  NULL │ False              │ NULL                     │ NULL             │ NULL                   │ 2019-12-26               │ a55475b1                   │ a55475b1                        │                    NULL │ NULL              │ P94_109_143           │ NULL                │                   NULL │              NULL │                     NULL │ a55475b1                    │ a55475b1                       │                   50000.0 │ 2019-12-26         │ P45_84_106            │ P94_109_143                     │ NULL                  │ D           │         NULL │                        0.0 │                   0.0 │             0.0 │                NULL │                     NULL │                         NULL │                        NULL │           0.0 │                        0.0 │ NULL                     │                    NULL │                0.0 │                0.0 │               0.0 │               0.0 │                0.0 │                  0.0 │                      NULL │                     NULL │                            NULL │                  NULL │                      NULL │                      NULL │               1.0 │               NULL │                 0.0 │                        0.0 │                    0.0 │                   1.0 │                     NULL │                        NULL │            0.0 │                  NULL │                    NULL │                       NULL │                     NULL │                            NULL │                 NULL │                        NULL │                           NULL │                     NULL │                           NULL │                           NULL │                  NULL │                         NULL │                          NULL │                       NULL │                    NULL │                           NULL │                NULL │                       NULL │                      NULL │                         NULL │                   0.0 │                  0.0 │               1.0 │ False         │ OTHER           │ OTHER        │ NULL                         │                        NULL │                        NULL │                            NULL │                            NULL │                            NULL │        12.0 │                    0.0 │                        0.0 │                          NULL │ a55475b1                  │        NULL │                 1.0 │                  1.0 │                      NULL │                         NULL │          0.0 │               0.0 │                     NULL │ FO                 │ AL             │ NULL            │\n└─────────┴─────────────────────────┴────────────────────────────────┴──────────────┴──────────────────────┴─────────────────────┴──────────────────────┴───────────────────────┴──────────────────────┴──────────────────────┴──────────────────────┴───────────────────────────┴──────────────────────────┴───────────────────────────┴─────────────────────────────┴───────────────────────────┴───────────────────────────┴──────────────────────────┴────────────────────────────────┴────────────────────────┴──────────────────┴──────────────┴────────────────────────┴───────────────────────┴───────────────────────┴─────────────────┴──────────────────┴──────────────────┴──────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴──────────────────────────┴────────────────────┴──────────────────────────┴─────────────────┴───────────────┴──────────────┴────────────────────────────┴──────────────────────┴──────────────────────────┴─────────────────────────┴─────────────────────────────────┴───────────────────────┴───────────────────────────┴──────────────────────┴──────────────┴───────────────────────────┴──────────┴────────────────────────────┴─────────────────────┴────────────────────────┴───────────────────┴─────────────────┴────────────────────────────┴──────────────────────────┴───────────────────┴───────────────────────┴────────────────────┴──────────────────────────┴──────────────────┴────────────────────────┴──────────────────────────┴────────────────────────────┴─────────────────────────────────┴─────────────────────────┴───────────────────┴───────────────────────┴─────────────────────┴────────────────────────┴───────────────────┴──────────────────────────┴─────────────────────────────┴────────────────────────────────┴───────────────────────────┴────────────────────┴───────────────────────┴─────────────────────────────────┴───────────────────────┴─────────────┴──────────────┴────────────────────────────┴───────────────────────┴─────────────────┴─────────────────────┴──────────────────────────┴──────────────────────────────┴─────────────────────────────┴───────────────┴────────────────────────────┴──────────────────────────┴─────────────────────────┴────────────────────┴────────────────────┴───────────────────┴───────────────────┴────────────────────┴──────────────────────┴───────────────────────────┴──────────────────────────┴─────────────────────────────────┴───────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────┴────────────────────┴─────────────────────┴────────────────────────────┴────────────────────────┴───────────────────────┴──────────────────────────┴─────────────────────────────┴────────────────┴───────────────────────┴─────────────────────────┴────────────────────────────┴──────────────────────────┴─────────────────────────────────┴──────────────────────┴─────────────────────────────┴────────────────────────────────┴──────────────────────────┴────────────────────────────────┴────────────────────────────────┴───────────────────────┴──────────────────────────────┴───────────────────────────────┴────────────────────────────┴─────────────────────────┴────────────────────────────────┴─────────────────────┴────────────────────────────┴───────────────────────────┴──────────────────────────────┴───────────────────────┴──────────────────────┴───────────────────┴───────────────┴─────────────────┴──────────────┴──────────────────────────────┴─────────────────────────────┴─────────────────────────────┴─────────────────────────────────┴─────────────────────────────────┴─────────────────────────────────┴─────────────┴────────────────────────┴────────────────────────────┴───────────────────────────────┴───────────────────────────┴─────────────┴─────────────────────┴──────────────────────┴───────────────────────────┴──────────────────────────────┴──────────────┴───────────────────┴──────────────────────────┴────────────────────┴────────────────┴─────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ employername_160M ┃ num_group1 ┃ pmtamount_36A ┃ processingdate_168D ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ string            │ int64      │ float64       │ string              │\n├─────────┼───────────────────┼────────────┼───────────────┼─────────────────────┤\n│     357 │ c91b12ff          │          5 │        1100.0 │ 2018-08-08          │\n│     357 │ c91b12ff          │          1 │        1200.0 │ 2018-11-28          │\n│     357 │ c91b12ff          │          4 │        1200.0 │ 2018-09-10          │\n└─────────┴───────────────────┴────────────┴───────────────┴─────────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ birth_259D ┃ birthdate_87D ┃ childnum_185L ┃ contaddr_district_15M ┃ contaddr_matchlist_1032L ┃ contaddr_smempladdr_334L ┃ contaddr_zipcode_807M ┃ education_927M ┃ empl_employedfrom_271D ┃ empl_employedtotal_800L ┃ empl_industry_691L ┃ empladdr_district_926M ┃ empladdr_zipcode_114M ┃ familystate_447L ┃ gender_992L ┃ housetype_905L ┃ housingtype_772L ┃ incometype_1044T ┃ isreference_387L ┃ language1_981M ┃ mainoccupationinc_384A ┃ maritalst_703L ┃ num_group1 ┃ personindex_1023L ┃ persontype_1072L ┃ persontype_792L ┃ registaddr_district_1083M ┃ registaddr_zipcode_184M ┃ relationshiptoclient_415T ┃ relationshiptoclient_642T ┃ remitter_829L ┃ role_1084L ┃ role_993L ┃ safeguarantyflag_411L ┃ sex_738L ┃ type_25L       ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ int64   │ string     │ string        │ float64       │ string                │ boolean                  │ boolean                  │ string                │ string         │ string                 │ string                  │ string             │ string                 │ string                │ string           │ string      │ string         │ string           │ string           │ boolean          │ string         │ float64                │ string         │ int64      │ float64           │ float64          │ float64         │ string                    │ string                  │ string                    │ string                    │ boolean       │ string     │ string    │ boolean               │ string   │ string         │\n├─────────┼────────────┼───────────────┼───────────────┼───────────────────────┼──────────────────────────┼──────────────────────────┼───────────────────────┼────────────────┼────────────────────────┼─────────────────────────┼────────────────────┼────────────────────────┼───────────────────────┼──────────────────┼─────────────┼────────────────┼──────────────────┼──────────────────┼──────────────────┼────────────────┼────────────────────────┼────────────────┼────────────┼───────────────────┼──────────────────┼─────────────────┼───────────────────────────┼─────────────────────────┼───────────────────────────┼───────────────────────────┼───────────────┼────────────┼───────────┼───────────────────────┼──────────┼────────────────┤\n│       0 │ 1986-07-01 │ NULL          │          NULL │ P88_18_84             │ False                    │ False                    │ P167_100_165          │ P97_36_170     │ 2017-09-15             │ MORE_FIVE               │ OTHER              │ P142_57_166            │ P167_100_165          │ MARRIED          │ NULL        │ NULL           │ NULL             │ SALARIED_GOVT    │ NULL             │ P10_39_147     │                10800.0 │ NULL           │          0 │               0.0 │              1.0 │             1.0 │ P88_18_84                 │ P167_100_165            │ NULL                      │ NULL                      │ NULL          │ CL         │ NULL      │ True                  │ F        │ PRIMARY_MOBILE │\n│       0 │ NULL       │ NULL          │          NULL │ a55475b1              │ NULL                     │ NULL                     │ a55475b1              │ a55475b1       │ NULL                   │ NULL                    │ NULL               │ a55475b1               │ a55475b1              │ NULL             │ NULL        │ NULL           │ NULL             │ NULL             │ NULL             │ a55475b1       │                   NULL │ NULL           │          1 │               1.0 │              1.0 │             4.0 │ a55475b1                  │ a55475b1                │ SPOUSE                    │ NULL                      │ False         │ EM         │ NULL      │ NULL                  │ NULL     │ PHONE          │\n│       0 │ NULL       │ NULL          │          NULL │ a55475b1              │ NULL                     │ NULL                     │ a55475b1              │ a55475b1       │ NULL                   │ NULL                    │ NULL               │ a55475b1               │ a55475b1              │ NULL             │ NULL        │ NULL           │ NULL             │ NULL             │ NULL             │ a55475b1       │                   NULL │ NULL           │          2 │               2.0 │              4.0 │             5.0 │ a55475b1                  │ a55475b1                │ COLLEAGUE                 │ SPOUSE                    │ False         │ PE         │ NULL      │ NULL                  │ NULL     │ PHONE          │\n└─────────┴────────────┴───────────────┴───────────────┴───────────────────────┴──────────────────────────┴──────────────────────────┴───────────────────────┴────────────────┴────────────────────────┴─────────────────────────┴────────────────────┴────────────────────────┴───────────────────────┴──────────────────┴─────────────┴────────────────┴──────────────────┴──────────────────┴──────────────────┴────────────────┴────────────────────────┴────────────────┴────────────┴───────────────────┴──────────────────┴─────────────────┴───────────────────────────┴─────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────┴────────────┴───────────┴───────────────────────┴──────────┴────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ amount_1115A ┃ classificationofcontr_1114M ┃ contractdate_551D ┃ contractmaturitydate_151D ┃ contractst_516M ┃ contracttype_653M ┃ credlmt_1052A ┃ credlmt_228A ┃ credlmt_3940954A ┃ credor_3940957M ┃ credquantity_1099L ┃ credquantity_984L ┃ debtpastduevalue_732A ┃ debtvalue_227A ┃ dpd_550P ┃ dpd_733P ┃ dpdmax_851P ┃ dpdmaxdatemonth_804T ┃ dpdmaxdateyear_742T ┃ installmentamount_644A ┃ installmentamount_833A ┃ instlamount_892A ┃ interesteffectiverate_369L ┃ interestrateyearly_538L ┃ lastupdate_260D ┃ maxdebtpduevalodued_3940955A ┃ num_group1 ┃ numberofinstls_810L ┃ overdueamountmax_950A ┃ overdueamountmaxdatemonth_494T ┃ overdueamountmaxdateyear_432T ┃ periodicityofpmts_997L ┃ periodicityofpmts_997M ┃ pmtdaysoverdue_1135P ┃ pmtmethod_731M ┃ pmtnumpending_403L ┃ purposeofcred_722M ┃ residualamount_1093A ┃ residualamount_127A ┃ residualamount_3940956A ┃ subjectrole_326M ┃ subjectrole_43M ┃ totalamount_503A ┃ totalamount_881A ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64      │ string                      │ string            │ string                    │ string          │ string            │ float64       │ float64      │ float64          │ string          │ float64            │ float64           │ float64               │ float64        │ float64  │ float64  │ float64     │ float64              │ float64             │ float64                │ float64                │ float64          │ float64                    │ float64                 │ string          │ float64                      │ int64      │ float64             │ float64               │ float64                        │ float64                       │ string                 │ string                 │ float64              │ string         │ float64            │ string             │ float64              │ float64             │ float64                 │ string           │ string          │ float64          │ float64          │\n├─────────┼──────────────┼─────────────────────────────┼───────────────────┼───────────────────────────┼─────────────────┼───────────────────┼───────────────┼──────────────┼──────────────────┼─────────────────┼────────────────────┼───────────────────┼───────────────────────┼────────────────┼──────────┼──────────┼─────────────┼──────────────────────┼─────────────────────┼────────────────────────┼────────────────────────┼──────────────────┼────────────────────────────┼─────────────────────────┼─────────────────┼──────────────────────────────┼────────────┼─────────────────────┼───────────────────────┼────────────────────────────────┼───────────────────────────────┼────────────────────────┼────────────────────────┼──────────────────────┼────────────────┼────────────────────┼────────────────────┼──────────────────────┼─────────────────────┼─────────────────────────┼──────────────────┼─────────────────┼──────────────────┼──────────────────┤\n│     467 │         NULL │ ea6782cc                    │ 2011-06-15        │ 2031-06-13                │ 7241344e        │ 724be82a          │  3.000000e+06 │      10000.0 │     3.000000e+06 │ P164_34_168     │                2.0 │               1.0 │                  NULL │           NULL │      0.0 │      0.0 │        NULL │                 NULL │                NULL │                    0.0 │                  0.000 │             NULL │                       NULL │                    NULL │ 2019-01-20      │                         NULL │          0 │                NULL │                  NULL │                           NULL │                          NULL │ NULL                   │ a55475b1               │                 NULL │ a55475b1       │               NULL │ 96a8fdfe           │                  0.0 │                 0.0 │                    NULL │ fa4f56f1         │ ab3c25cf        │     3.000000e+06 │          10000.0 │\n│     467 │         NULL │ ea6782cc                    │ 2019-01-04        │ 2021-08-04                │ 7241344e        │ 724be82a          │          NULL │         NULL │     1.303650e+05 │ P164_34_168     │                1.0 │               2.0 │                  NULL │           NULL │      0.0 │      0.0 │        NULL │                 NULL │                NULL │                    0.0 │              26571.969 │             NULL │                       NULL │                    NULL │ 2019-01-20      │                         NULL │          1 │                NULL │                  NULL │                           NULL │                          NULL │ NULL                   │ a55475b1               │                 NULL │ a55475b1       │               NULL │ 96a8fdfe           │                 NULL │                NULL │                    NULL │ ab3c25cf         │ ab3c25cf        │     7.800000e+04 │         960000.0 │\n│     467 │      78000.0 │ ea6782cc                    │ 2016-10-25        │ 2019-10-25                │ 7241344e        │ 4257cbed          │          NULL │         NULL │             NULL │ c5a72b57        │               NULL │              NULL │                   0.0 │      26571.969 │     NULL │     NULL │         0.0 │                 11.0 │              2016.0 │                   NULL │                   NULL │          2898.76 │                       NULL │                    NULL │ 2019-01-10      │                          0.0 │          2 │                36.0 │                   0.0 │                           11.0 │                        2016.0 │ NULL                   │ a0b598e4               │                  0.0 │ e914c86c       │               10.0 │ 96a8fdfe           │                 NULL │                NULL │                    NULL │ a55475b1         │ a55475b1        │             NULL │             NULL │\n└─────────┴──────────────┴─────────────────────────────┴───────────────────┴───────────────────────────┴─────────────────┴───────────────────┴───────────────┴──────────────┴──────────────────┴─────────────────┴────────────────────┴───────────────────┴───────────────────────┴────────────────┴──────────┴──────────┴─────────────┴──────────────────────┴─────────────────────┴────────────────────────┴────────────────────────┴──────────────────┴────────────────────────────┴─────────────────────────┴─────────────────┴──────────────────────────────┴────────────┴─────────────────────┴───────────────────────┴────────────────────────────────┴───────────────────────────────┴────────────────────────┴────────────────────────┴──────────────────────┴────────────────┴────────────────────┴────────────────────┴──────────────────────┴─────────────────────┴─────────────────────────┴──────────────────┴─────────────────┴──────────────────┴──────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ case_id ┃ amount_4917619A ┃ deductiondate_4917603D ┃ name_4917606M ┃ num_group1 ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ int64   │ float64         │ string                 │ string        │ int64      │\n├─────────┼─────────────────┼────────────────────────┼───────────────┼────────────┤\n│   49435 │          6885.0 │ 2019-10-16             │ 6b730375      │          7 │\n│   49435 │          6885.0 │ 2019-10-16             │ 6b730375      │          1 │\n│   49435 │          6885.0 │ 2019-10-16             │ 6b730375      │          8 │\n└─────────┴─────────────────┴────────────────────────┴───────────────┴────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ last180dayaveragebalance_704A ┃ last180dayturnover_1134A ┃ last30dayturnover_651A ┃ num_group1 ┃ openingdate_857D ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64                       │ float64                  │ float64                │ int64      │ string           │\n├─────────┼───────────────────────────────┼──────────────────────────┼────────────────────────┼────────────┼──────────────────┤\n│     225 │                          NULL │                     NULL │                   NULL │          0 │ 2016-08-16       │\n│     331 │                          NULL │                     NULL │                   NULL │          0 │ 2015-03-19       │\n│     358 │                          NULL │                     NULL │                   NULL │          0 │ 2014-09-02       │\n└─────────┴───────────────────────────────┴──────────────────────────┴────────────────────────┴────────────┴──────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ case_id ┃ actualdpd_943P ┃ annuity_853A ┃ approvaldate_319D ┃ byoccupationinc_3656910L ┃ cancelreason_3545846M ┃ childnum_21L ┃ creationdate_885D ┃ credacc_actualbalance_314A ┃ credacc_credlmt_575A ┃ credacc_maxhisbal_375A ┃ credacc_minhisbal_90A ┃ credacc_status_367L ┃ credacc_transactions_402L ┃ credamount_590A ┃ credtype_587L ┃ currdebt_94A ┃ dateactivated_425D ┃ district_544M ┃ downpmt_134A ┃ dtlastpmt_581D ┃ dtlastpmtallstes_3545839D ┃ education_1138M ┃ employedfrom_700D ┃ familystate_726L ┃ firstnonzeroinstldate_307D ┃ inittransactioncode_279L ┃ isbidproduct_390L ┃ isdebitcard_527L ┃ mainoccupationinc_437A ┃ maxdpdtolerance_577P ┃ num_group1 ┃ outstandingdebt_522A ┃ pmtnum_8L ┃ postype_4733339M ┃ profession_152M ┃ rejectreason_755M ┃ rejectreasonclient_4145042M ┃ revolvingaccount_394A ┃ status_219L ┃ tenor_203L ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ int64   │ float64        │ float64      │ string            │ float64                  │ string                │ float64      │ string            │ float64                    │ float64              │ float64                │ float64               │ string              │ float64                   │ float64         │ string        │ float64      │ string             │ string        │ float64      │ string         │ string                    │ string          │ string            │ string           │ string                     │ string                   │ boolean           │ boolean          │ float64                │ float64              │ int64      │ float64              │ float64   │ string           │ string          │ string            │ string                      │ float64               │ string      │ float64    │\n├─────────┼────────────────┼──────────────┼───────────────────┼──────────────────────────┼───────────────────────┼──────────────┼───────────────────┼────────────────────────────┼──────────────────────┼────────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────┼─────────────────┼───────────────┼──────────────┼────────────────────┼───────────────┼──────────────┼────────────────┼───────────────────────────┼─────────────────┼───────────────────┼──────────────────┼────────────────────────────┼──────────────────────────┼───────────────────┼──────────────────┼────────────────────────┼──────────────────────┼────────────┼──────────────────────┼───────────┼──────────────────┼─────────────────┼───────────────────┼─────────────────────────────┼───────────────────────┼─────────────┼────────────┤\n│   40704 │            0.0 │    7204.6000 │ NULL              │                     NULL │ P94_109_143           │         NULL │ 2018-11-20        │                       NULL │                  0.0 │                   NULL │                  NULL │ NULL                │                      NULL │         54000.0 │ CAL           │         NULL │ NULL               │ P147_6_101    │          0.0 │ NULL           │ NULL                      │ a55475b1        │ NULL              │ NULL             │ 2018-12-20                 │ CASH                     │ False             │ NULL             │                40000.0 │                 NULL │          0 │                 NULL │      12.0 │ P46_145_78       │ a55475b1        │ P198_131_9        │ P94_109_143                 │                  NULL │ D           │       12.0 │\n│   40734 │            0.0 │    3870.2000 │ NULL              │                     NULL │ P94_109_143           │         NULL │ 2019-12-26        │                       NULL │                  0.0 │                   NULL │                  NULL │ NULL                │                      NULL │         50000.0 │ CAL           │         NULL │ NULL               │ P111_148_100  │          0.0 │ NULL           │ NULL                      │ a55475b1        │ NULL              │ NULL             │ 2020-01-26                 │ CASH                     │ False             │ NULL             │                50000.0 │                 NULL │          0 │                 NULL │      18.0 │ P149_40_170      │ a55475b1        │ P45_84_106        │ P94_109_143                 │                  NULL │ D           │       18.0 │\n│   40737 │            0.0 │    2324.4001 │ NULL              │                      1.0 │ a55475b1              │          0.0 │ 2014-07-17        │                       NULL │                  0.0 │                   NULL │                  NULL │ NULL                │                      NULL │         30000.0 │ CAL           │          0.0 │ NULL               │ a55475b1      │          0.0 │ NULL           │ NULL                      │ P97_36_170      │ 2014-01-15        │ MARRIED          │ 2014-08-17                 │ CASH                     │ False             │ NULL             │                16000.0 │                 NULL │          0 │                  0.0 │      18.0 │ P46_145_78       │ a55475b1        │ a55475b1          │ a55475b1                    │                  NULL │ D           │       18.0 │\n└─────────┴────────────────┴──────────────┴───────────────────┴──────────────────────────┴───────────────────────┴──────────────┴───────────────────┴────────────────────────────┴──────────────────────┴────────────────────────┴───────────────────────┴─────────────────────┴───────────────────────────┴─────────────────┴───────────────┴──────────────┴────────────────────┴───────────────┴──────────────┴────────────────┴───────────────────────────┴─────────────────┴───────────────────┴──────────────────┴────────────────────────────┴──────────────────────────┴───────────────────┴──────────────────┴────────────────────────┴──────────────────────┴────────────┴──────────────────────┴───────────┴──────────────────┴─────────────────┴───────────────────┴─────────────────────────────┴───────────────────────┴─────────────┴────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ amount_4527230A ┃ name_4527232M ┃ num_group1 ┃ recorddate_4527225D ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64         │ string        │ int64      │ string              │\n├─────────┼─────────────────┼───────────────┼────────────┼─────────────────────┤\n│   28631 │       1946.0000 │ f980a1ea      │          2 │ 2019-09-13          │\n│   28631 │        711.0000 │ f980a1ea      │          3 │ 2019-09-13          │\n│   28631 │       3616.4001 │ f980a1ea      │          0 │ 2019-09-13          │\n└─────────┴─────────────────┴───────────────┴────────────┴─────────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ amount_416A ┃ contractenddate_991D ┃ num_group1 ┃ openingdate_313D ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64     │ string               │ int64      │ string           │\n├─────────┼─────────────┼──────────────────────┼────────────┼──────────────────┤\n│     225 │       0.000 │ NULL                 │          0 │ 2016-08-16       │\n│     331 │     260.374 │ 2018-03-18           │          0 │ 2015-03-19       │\n│     358 │       0.000 │ NULL                 │          0 │ 2014-09-02       │\n└─────────┴─────────────┴──────────────────────┴────────────┴──────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ collater_typofvalofguarant_298M ┃ collater_typofvalofguarant_407M ┃ collater_valueofguarantee_1124L ┃ collater_valueofguarantee_876L ┃ collaterals_typeofguarante_359M ┃ collaterals_typeofguarante_669M ┃ num_group1 ┃ num_group2 ┃ pmts_dpd_1073P ┃ pmts_dpd_303P ┃ pmts_month_158T ┃ pmts_month_706T ┃ pmts_overdue_1140A ┃ pmts_overdue_1152A ┃ pmts_year_1139T ┃ pmts_year_507T ┃ subjectroles_name_541M ┃ subjectroles_name_838M ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ string                          │ string                          │ float64                         │ float64                        │ string                          │ string                          │ int64      │ int64      │ float64        │ float64       │ float64         │ float64         │ float64            │ float64            │ float64         │ float64        │ string                 │ string                 │\n├─────────┼─────────────────────────────────┼─────────────────────────────────┼─────────────────────────────────┼────────────────────────────────┼─────────────────────────────────┼─────────────────────────────────┼────────────┼────────────┼────────────────┼───────────────┼─────────────────┼─────────────────┼────────────────────┼────────────────────┼─────────────────┼────────────────┼────────────────────────┼────────────────────────┤\n│    6683 │ 8fd95e4b                        │ a55475b1                        │                    6.742800e+06 │                           NULL │ a55475b1                        │ 9276e4bb                        │          0 │          0 │           NULL │          NULL │             2.0 │            NULL │               NULL │               NULL │          2017.0 │           NULL │ a55475b1               │ ab3c25cf               │\n│    6683 │ 8fd95e4b                        │ a55475b1                        │                    6.700000e+06 │                           NULL │ a55475b1                        │ 0e63c0f0                        │          0 │          1 │           NULL │          NULL │             3.0 │            NULL │               NULL │               NULL │          2017.0 │           NULL │ a55475b1               │ a55475b1               │\n│    6683 │ a55475b1                        │ a55475b1                        │                            NULL │                           NULL │ a55475b1                        │ a55475b1                        │          0 │          2 │            0.0 │          NULL │             4.0 │            NULL │                0.0 │               NULL │          2017.0 │           NULL │ a55475b1               │ a55475b1               │\n└─────────┴─────────────────────────────────┴─────────────────────────────────┴─────────────────────────────────┴────────────────────────────────┴─────────────────────────────────┴─────────────────────────────────┴────────────┴────────────┴────────────────┴───────────────┴─────────────────┴─────────────────┴────────────────────┴────────────────────┴─────────────────┴────────────────┴────────────────────────┴────────────────────────┘\n\n\n\n\nWhere to find columns?\n\nc = {i.name:ibis.read_parquet(i).columns for i in p.glob(\"*.parquet\")}\n\n\n\nwith pd.option_context('display.max_rows', None):\n    fdef=pd.read_csv('home-credit-credit-risk-model-stability/feature_definitions.csv').set_index('Variable')\n    lookuptable=(pd.DataFrame([pd.Series({\"File\":i, \"Variable\":j}) for i, j in c.items()])\n        .explode('Variable')\n        .assign(File=lambda x: x[\"File\"].str.replace('\\.parquet$','', regex=True))\n        .groupby('Variable')\n        .agg({\"File\":lambda x: x.to_list()})\n        .join(fdef)\n        .reset_index())\n    display(lookuptable)\n\n\n\n\n\n\n\n\n\nVariable\nFile\nDescription\n\n\n\n\n0\nMONTH\n[train_base]\nNaN\n\n\n1\nWEEK_NUM\n[train_base]\nNaN\n\n\n2\nactualdpd_943P\n[train_applprev_1_0, train_applprev_1_1]\nDays Past Due (DPD) of previous contract (actu...\n\n\n3\nactualdpdtolerance_344P\n[train_static_0_0, train_static_0_1]\nDPD of client with tolerance.\n\n\n4\naddres_district_368M\n[train_person_2]\nDistrict of the person's address.\n\n\n5\naddres_role_871L\n[train_person_2]\nRole of person's address.\n\n\n6\naddres_zip_823M\n[train_person_2]\nZip code of the address.\n\n\n7\namount_1115A\n[train_credit_bureau_b_1]\nCredit amount of the active contract provided ...\n\n\n8\namount_416A\n[train_deposit_1]\nDeposit amount.\n\n\n9\namount_4527230A\n[train_tax_registry_a_1]\nTax deductions amount tracked by the governmen...\n\n\n10\namount_4917619A\n[train_tax_registry_b_1]\nTax deductions amount tracked by the governmen...\n\n\n11\namtdebitincoming_4809443A\n[train_other_1]\nIncoming debit card transactions amount.\n\n\n12\namtdebitoutgoing_4809440A\n[train_other_1]\nOutgoing debit card transactions amount.\n\n\n13\namtdepositbalance_4809441A\n[train_other_1]\nDeposit balance of client.\n\n\n14\namtdepositincoming_4809444A\n[train_other_1]\nAmount of incoming deposits to client's account.\n\n\n15\namtdepositoutgoing_4809442A\n[train_other_1]\nAmount of outgoing deposits from client's acco...\n\n\n16\namtinstpaidbefduel24m_4187115A\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid before due date in ...\n\n\n17\nannualeffectiverate_199L\n[train_credit_bureau_a_1_0, train_credit_burea...\nInterest rate of the closed contracts.\n\n\n18\nannualeffectiverate_63L\n[train_credit_bureau_a_1_0, train_credit_burea...\nInterest rate for the active contracts.\n\n\n19\nannuity_780A\n[train_static_0_0, train_static_0_1]\nMonthly annuity amount.\n\n\n20\nannuity_853A\n[train_applprev_1_0, train_applprev_1_1]\nMonthly annuity for previous applications.\n\n\n21\nannuitynextmonth_57A\n[train_static_0_0, train_static_0_1]\nNext month's amount of annuity.\n\n\n22\napplicationcnt_361L\n[train_static_0_0, train_static_0_1]\nNumber of applications associated with the sam...\n\n\n23\napplications30d_658L\n[train_static_0_0, train_static_0_1]\nNumber of applications made by the client in t...\n\n\n24\napplicationscnt_1086L\n[train_static_0_0, train_static_0_1]\nNumber of applications associated with the sam...\n\n\n25\napplicationscnt_464L\n[train_static_0_0, train_static_0_1]\nNumber of applications made in the last 30 day...\n\n\n26\napplicationscnt_629L\n[train_static_0_0, train_static_0_1]\nNumber of applications with the same employer ...\n\n\n27\napplicationscnt_867L\n[train_static_0_0, train_static_0_1]\nNumber of applications associated with the sam...\n\n\n28\napprovaldate_319D\n[train_applprev_1_0, train_applprev_1_1]\nApproval Date of Previous Application\n\n\n29\nassignmentdate_238D\n[train_static_cb_0]\nTax authority data - date of assignment.\n\n\n30\nassignmentdate_4527235D\n[train_static_cb_0]\nTax authority data - Date of assignment.\n\n\n31\nassignmentdate_4955616D\n[train_static_cb_0]\nTax authority assignment date.\n\n\n32\navgdbddpdlast24m_3658932P\n[train_static_0_0, train_static_0_1]\nAverage days past or before due of payment dur...\n\n\n33\navgdbddpdlast3m_4187120P\n[train_static_0_0, train_static_0_1]\nAverage days past or before due of payment dur...\n\n\n34\navgdbdtollast24m_4525197P\n[train_static_0_0, train_static_0_1]\nAverage days of payment before due date within...\n\n\n35\navgdpdtolclosure24_3658938P\n[train_static_0_0, train_static_0_1]\nAverage DPD (days past due) with tolerance wit...\n\n\n36\navginstallast24m_3658937A\n[train_static_0_0, train_static_0_1]\nAverage instalments paid by the client over th...\n\n\n37\navglnamtstart24m_4525187A\n[train_static_0_0, train_static_0_1]\nAverage loan amount in the last 24 months.\n\n\n38\navgmaxdpdlast9m_3716943P\n[train_static_0_0, train_static_0_1]\nAverage Days Past Due (DPD) of the client in l...\n\n\n39\navgoutstandbalancel6m_4187114A\n[train_static_0_0, train_static_0_1]\nAverage outstanding balance of applicant for t...\n\n\n40\navgpmtlast12m_4525200A\n[train_static_0_0, train_static_0_1]\nAverage of payments made by the client in the ...\n\n\n41\nbankacctype_710L\n[train_static_0_0, train_static_0_1]\nType of applicant's bank account.\n\n\n42\nbirth_259D\n[train_person_1]\nDate of birth of the person.\n\n\n43\nbirthdate_574D\n[train_static_cb_0]\nClient's date of birth (credit bureau data).\n\n\n44\nbirthdate_87D\n[train_person_1]\nBirth date of the person.\n\n\n45\nbyoccupationinc_3656910L\n[train_applprev_1_0, train_applprev_1_1]\nApplicant's income from previous applications.\n\n\n46\ncacccardblochreas_147M\n[train_applprev_2]\nCard blocking reason.\n\n\n47\ncancelreason_3545846M\n[train_applprev_1_0, train_applprev_1_1]\nApplication cancellation reason.\n\n\n48\ncardtype_51L\n[train_static_0_0, train_static_0_1]\nType of credit card.\n\n\n49\ncase_id\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n50\nchildnum_185L\n[train_person_1]\nNumber of children of the applicant.\n\n\n51\nchildnum_21L\n[train_applprev_1_0, train_applprev_1_1]\nNumber of children in the previous application.\n\n\n52\nclassificationofcontr_1114M\n[train_credit_bureau_b_1]\nClassificiation of the active contract.\n\n\n53\nclassificationofcontr_13M\n[train_credit_bureau_a_1_0, train_credit_burea...\nClassificiation of the active contract.\n\n\n54\nclassificationofcontr_400M\n[train_credit_bureau_a_1_0, train_credit_burea...\nClassificiation of the closed contract.\n\n\n55\nclientscnt12m_3712952L\n[train_static_0_0, train_static_0_1]\nNumber of clients that have used the same mobi...\n\n\n56\nclientscnt3m_3712950L\n[train_static_0_0, train_static_0_1]\nNumber of clients who have the same mobile pho...\n\n\n57\nclientscnt6m_3712949L\n[train_static_0_0, train_static_0_1]\nTotal number of clients who have used the same...\n\n\n58\nclientscnt_100L\n[train_static_0_0, train_static_0_1]\nNumber of applications with matching employer'...\n\n\n59\nclientscnt_1022L\n[train_static_0_0, train_static_0_1]\nNumber of clients sharing the same mobile phone.\n\n\n60\nclientscnt_1071L\n[train_static_0_0, train_static_0_1]\nNumber of applications where the alternative p...\n\n\n61\nclientscnt_1130L\n[train_static_0_0, train_static_0_1]\nNumber of applications where client's phone nu...\n\n\n62\nclientscnt_136L\n[train_static_0_0, train_static_0_1]\nNumber of applications associated with same em...\n\n\n63\nclientscnt_157L\n[train_static_0_0, train_static_0_1]\nNumber of clients whose employer has the same ...\n\n\n64\nclientscnt_257L\n[train_static_0_0, train_static_0_1]\nNumber of clients that share an alternative ph...\n\n\n65\nclientscnt_304L\n[train_static_0_0, train_static_0_1]\nNumber of clients with the same phone number.\n\n\n66\nclientscnt_360L\n[train_static_0_0, train_static_0_1]\nNumber of clients that have the same alternati...\n\n\n67\nclientscnt_493L\n[train_static_0_0, train_static_0_1]\nNumber of clients with matching phone numbers ...\n\n\n68\nclientscnt_533L\n[train_static_0_0, train_static_0_1]\nNumber of clients with same client's and alter...\n\n\n69\nclientscnt_887L\n[train_static_0_0, train_static_0_1]\nNumber of clients sharing the same employer's ...\n\n\n70\nclientscnt_946L\n[train_static_0_0, train_static_0_1]\nNumber of clients with matching mobile and emp...\n\n\n71\ncntincpaycont9m_3716944L\n[train_static_0_0, train_static_0_1]\nNumber of incoming payments in the past 9 months.\n\n\n72\ncntpmts24_3658933L\n[train_static_0_0, train_static_0_1]\nNumber of months with any incoming payment in ...\n\n\n73\ncollater_typofvalofguarant_298M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral valuation type (active contract).\n\n\n74\ncollater_typofvalofguarant_407M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral valuation type (closed contract).\n\n\n75\ncollater_valueofguarantee_1124L\n[train_credit_bureau_a_2_4, train_credit_burea...\nValue of collateral for active contract.\n\n\n76\ncollater_valueofguarantee_876L\n[train_credit_bureau_a_2_4, train_credit_burea...\nValue of collateral for closed contract.\n\n\n77\ncollaterals_typeofguarante_359M\n[train_credit_bureau_a_2_4, train_credit_burea...\nType of collateral that was used as a guarante...\n\n\n78\ncollaterals_typeofguarante_669M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral type for the active contract.\n\n\n79\ncommnoinclast6m_3546845L\n[train_static_0_0, train_static_0_1]\nNumber of communications indicating low income...\n\n\n80\ncontaddr_district_15M\n[train_person_1]\nZip code of a contact person's address.\n\n\n81\ncontaddr_matchlist_1032L\n[train_person_1]\nIndicates whether the contact address is found...\n\n\n82\ncontaddr_smempladdr_334L\n[train_person_1]\nIndicates whether the contact address is the s...\n\n\n83\ncontaddr_zipcode_807M\n[train_person_1]\nZip code of contact address.\n\n\n84\ncontractdate_551D\n[train_credit_bureau_b_1]\nContract date of the active contract\n\n\n85\ncontractenddate_991D\n[train_deposit_1]\nEnd date of deposit contract.\n\n\n86\ncontractmaturitydate_151D\n[train_credit_bureau_b_1]\nEnd date of active contract.\n\n\n87\ncontractssum_5085716L\n[train_static_cb_0]\nTotal sum of values of contracts retrieved fro...\n\n\n88\ncontractst_516M\n[train_credit_bureau_b_1]\nContract status.\n\n\n89\ncontractst_545M\n[train_credit_bureau_a_1_0, train_credit_burea...\nContract status.\n\n\n90\ncontractst_964M\n[train_credit_bureau_a_1_0, train_credit_burea...\nContract status of terminated credit contract.\n\n\n91\ncontractsum_5085717L\n[train_credit_bureau_a_1_0, train_credit_burea...\nSum of other contract values.\n\n\n92\ncontracttype_653M\n[train_credit_bureau_b_1]\nContract Type\n\n\n93\nconts_role_79M\n[train_person_2]\nType of contact role of a person.\n\n\n94\nconts_type_509L\n[train_applprev_2]\nPerson contact type in previous application.\n\n\n95\ncreationdate_885D\n[train_applprev_1_0, train_applprev_1_1]\nDate when previous application was created.\n\n\n96\ncredacc_actualbalance_314A\n[train_applprev_1_0, train_applprev_1_1]\nActual balance on credit account.\n\n\n97\ncredacc_cards_status_52L\n[train_applprev_2]\nCard status of the previous credit account.\n\n\n98\ncredacc_credlmt_575A\n[train_applprev_1_0, train_applprev_1_1]\nCredit card credit limit provided for previous...\n\n\n99\ncredacc_maxhisbal_375A\n[train_applprev_1_0, train_applprev_1_1]\nMaximal historical balance of previous credit ...\n\n\n100\ncredacc_minhisbal_90A\n[train_applprev_1_0, train_applprev_1_1]\nMinimum historical balance of previous credit ...\n\n\n101\ncredacc_status_367L\n[train_applprev_1_0, train_applprev_1_1]\nAccount status of previous credit applications.\n\n\n102\ncredacc_transactions_402L\n[train_applprev_1_0, train_applprev_1_1]\nNumber of transactions made with the previous ...\n\n\n103\ncredamount_590A\n[train_applprev_1_0, train_applprev_1_1]\nLoan amount or card limit of previous applicat...\n\n\n104\ncredamount_770A\n[train_static_0_0, train_static_0_1]\nLoan amount or credit card limit.\n\n\n105\ncredlmt_1052A\n[train_credit_bureau_b_1]\nCredit limit of an active loan.\n\n\n106\ncredlmt_228A\n[train_credit_bureau_b_1]\nCredit limit for closed loans.\n\n\n107\ncredlmt_230A\n[train_credit_bureau_a_1_0, train_credit_burea...\nCredit limit of the closed credit contracts fr...\n\n\n108\ncredlmt_3940954A\n[train_credit_bureau_b_1]\nCredit limit for active loan.\n\n\n109\ncredlmt_935A\n[train_credit_bureau_a_1_0, train_credit_burea...\nCredit limit for active loan.\n\n\n110\ncredor_3940957M\n[train_credit_bureau_b_1]\nCreditor's name\n\n\n111\ncredquantity_1099L\n[train_credit_bureau_b_1]\nNumber of credits in credit bureau\n\n\n112\ncredquantity_984L\n[train_credit_bureau_b_1]\nNumber of closed credits in credit bureau.\n\n\n113\ncredtype_322L\n[train_static_0_0, train_static_0_1]\nType of credit.\n\n\n114\ncredtype_587L\n[train_applprev_1_0, train_applprev_1_1]\nCredit type of previous application.\n\n\n115\ncurrdebt_22A\n[train_static_0_0, train_static_0_1]\nCurrent debt amount of the client.\n\n\n116\ncurrdebt_94A\n[train_applprev_1_0, train_applprev_1_1]\nPrevious application's current debt.\n\n\n117\ncurrdebtcredtyperange_828A\n[train_static_0_0, train_static_0_1]\nCurrent amount of debt of the applicant.\n\n\n118\ndate_decision\n[train_base]\nNaN\n\n\n119\ndateactivated_425D\n[train_applprev_1_0, train_applprev_1_1]\nContract activation date of the applicant's pr...\n\n\n120\ndatefirstoffer_1144D\n[train_static_0_0, train_static_0_1]\nDate of first customer relationship management...\n\n\n121\ndatelastinstal40dpd_247D\n[train_static_0_0, train_static_0_1]\nDate of last instalment that was more than 40 ...\n\n\n122\ndatelastunpaid_3546854D\n[train_static_0_0, train_static_0_1]\nDate of the last unpaid instalment.\n\n\n123\ndateofbirth_337D\n[train_static_cb_0]\nClient's date of birth.\n\n\n124\ndateofbirth_342D\n[train_static_cb_0]\nClient's date of birth.\n\n\n125\ndateofcredend_289D\n[train_credit_bureau_a_1_0, train_credit_burea...\nEnd date of an active credit contract.\n\n\n126\ndateofcredend_353D\n[train_credit_bureau_a_1_0, train_credit_burea...\nEnd date of a closed credit contract.\n\n\n127\ndateofcredstart_181D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate when the credit contract was closed.\n\n\n128\ndateofcredstart_739D\n[train_credit_bureau_a_1_0, train_credit_burea...\nStart date of a closed credit contract.\n\n\n129\ndateofrealrepmt_138D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of credit's closure (contract termination...\n\n\n130\ndays120_123L\n[train_static_cb_0]\nNumber of credit bureau queries for the last 1...\n\n\n131\ndays180_256L\n[train_static_cb_0]\nNumber of credit bureau queries for last 180 d...\n\n\n132\ndays30_165L\n[train_static_cb_0]\nNumber of credit bureau queries for the last 3...\n\n\n133\ndays360_512L\n[train_static_cb_0]\nNumber of Credit Bureau queries for last 360 d...\n\n\n134\ndays90_310L\n[train_static_cb_0]\nNumber of credit bureau queries for the last 9...\n\n\n135\ndaysoverduetolerancedd_3976961L\n[train_static_0_0, train_static_0_1]\nNumber of days that past after the due date (w...\n\n\n136\ndebtoutstand_525A\n[train_credit_bureau_a_1_0, train_credit_burea...\nOutstanding amount of existing contract.\n\n\n137\ndebtoverdue_47A\n[train_credit_bureau_a_1_0, train_credit_burea...\nAmount that is currently past due on a client'...\n\n\n138\ndebtpastduevalue_732A\n[train_credit_bureau_b_1]\nAmount of unpaid debt for existing contracts.\n\n\n139\ndebtvalue_227A\n[train_credit_bureau_b_1]\nOutstanding amount for existing debt contracts.\n\n\n140\ndeductiondate_4917603D\n[train_tax_registry_b_1]\nTax deduction date.\n\n\n141\ndeferredmnthsnum_166L\n[train_static_0_0, train_static_0_1]\nNumber of deferred months.\n\n\n142\ndescription_351M\n[train_credit_bureau_a_1_0, train_credit_burea...\nCategorization of clients by credit bureau.\n\n\n143\ndescription_5085714M\n[train_static_cb_0]\nCategorization of clients by credit bureau.\n\n\n144\ndisbursedcredamount_1113A\n[train_static_0_0, train_static_0_1]\nDisbursed credit amount after consolidation.\n\n\n145\ndisbursementtype_67L\n[train_static_0_0, train_static_0_1]\nType of disbursement.\n\n\n146\ndistrict_544M\n[train_applprev_1_0, train_applprev_1_1]\nDistrict of the address used in the previous l...\n\n\n147\ndownpmt_116A\n[train_static_0_0, train_static_0_1]\nAmount of downpayment.\n\n\n148\ndownpmt_134A\n[train_applprev_1_0, train_applprev_1_1]\nPrevious application downpayment amount.\n\n\n149\ndpd_550P\n[train_credit_bureau_b_1]\nThe number of days past due for active loans w...\n\n\n150\ndpd_733P\n[train_credit_bureau_b_1]\nDays past due (DPD) for guaranteed loans that ...\n\n\n151\ndpdmax_139P\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximal days past due for active contract.\n\n\n152\ndpdmax_757P\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximum days past due for a closed contract.\n\n\n153\ndpdmax_851P\n[train_credit_bureau_b_1]\nMaximal past due days for active contracts in ...\n\n\n154\ndpdmaxdatemonth_442T\n[train_credit_bureau_a_1_0, train_credit_burea...\nMax DPD occurrence month for terminated contra...\n\n\n155\ndpdmaxdatemonth_804T\n[train_credit_bureau_b_1]\nMonth when the maximum Day Past Due (DPD) occu...\n\n\n156\ndpdmaxdatemonth_89T\n[train_credit_bureau_a_1_0, train_credit_burea...\nMonth when maximum days past due occurred on t...\n\n\n157\ndpdmaxdateyear_596T\n[train_credit_bureau_a_1_0, train_credit_burea...\nYear when maximum Days Past Due (DPD) occurred...\n\n\n158\ndpdmaxdateyear_742T\n[train_credit_bureau_b_1]\nYear of the maximum Days Past Due (DPD) on an ...\n\n\n159\ndpdmaxdateyear_896T\n[train_credit_bureau_a_1_0, train_credit_burea...\nYear of maximum Days Past Due of closed contra...\n\n\n160\ndtlastpmt_581D\n[train_applprev_1_0, train_applprev_1_1]\nDate of last payment made by the applicant.\n\n\n161\ndtlastpmtallstes_3545839D\n[train_applprev_1_0, train_applprev_1_1]\nDate of the applicant's last payment.\n\n\n162\ndtlastpmtallstes_4499206D\n[train_static_0_0, train_static_0_1]\nDate of last payment made by the applicant.\n\n\n163\neducation_1103M\n[train_static_cb_0]\nLevel of education of the client provided by e...\n\n\n164\neducation_1138M\n[train_applprev_1_0, train_applprev_1_1]\nApplicant's education level from their previou...\n\n\n165\neducation_88M\n[train_static_cb_0]\nEducation level of the client.\n\n\n166\neducation_927M\n[train_person_1]\nEducation level of the person.\n\n\n167\neir_270L\n[train_static_0_0, train_static_0_1]\nInterest rate.\n\n\n168\nempl_employedfrom_271D\n[train_person_1]\nStart date of employment.\n\n\n169\nempl_employedtotal_800L\n[train_person_1]\nEmployment length of a person.\n\n\n170\nempl_industry_691L\n[train_person_1]\nEmployment Industry of the person.\n\n\n171\nempladdr_district_926M\n[train_person_1]\nDistrict where the employer's address is located.\n\n\n172\nempladdr_zipcode_114M\n[train_person_1]\nZipcode of employer's address.\n\n\n173\nemployedfrom_700D\n[train_applprev_1_0, train_applprev_1_1]\nEmployment start date from the previous applic...\n\n\n174\nemployername_160M\n[train_tax_registry_c_1]\nEmployer's name.\n\n\n175\nempls_economicalst_849M\n[train_person_2]\nThe economical status of the person (num_group...\n\n\n176\nempls_employedfrom_796D\n[train_person_2]\nStart of employment (num_group1 - person, num_...\n\n\n177\nempls_employer_name_740M\n[train_person_2]\nEmployer's name (num_group1 - person, num_grou...\n\n\n178\nequalitydataagreement_891L\n[train_static_0_0, train_static_0_1]\nFlag indicating sudden changes in client's soc...\n\n\n179\nequalityempfrom_62L\n[train_static_0_0, train_static_0_1]\nFlag indicating a sudden change in the client'...\n\n\n180\nfamilystate_447L\n[train_person_1]\nFamily state of the person.\n\n\n181\nfamilystate_726L\n[train_applprev_1_0, train_applprev_1_1]\nFamily State in previous application of applic...\n\n\n182\nfinancialinstitution_382M\n[train_credit_bureau_a_1_0, train_credit_burea...\nName of financial institution that is linked t...\n\n\n183\nfinancialinstitution_591M\n[train_credit_bureau_a_1_0, train_credit_burea...\nFinancial institution name of the active contr...\n\n\n184\nfirstclxcampaign_1125D\n[train_static_0_0, train_static_0_1]\nDate of the client's first campaign.\n\n\n185\nfirstdatedue_489D\n[train_static_0_0, train_static_0_1]\nDate of the first due date.\n\n\n186\nfirstnonzeroinstldate_307D\n[train_applprev_1_0, train_applprev_1_1]\nDate of first instalment in the previous appli...\n\n\n187\nfirstquarter_103L\n[train_static_cb_0]\nNumber of results obtained from credit bureau ...\n\n\n188\nfor3years_128L\n[train_static_cb_0]\nNumber of rejected applications in the past 3 ...\n\n\n189\nfor3years_504L\n[train_static_cb_0]\nClient's credit history data over the last thr...\n\n\n190\nfor3years_584L\n[train_static_cb_0]\nNumber of cancellations in the last 3 years.\n\n\n191\nformonth_118L\n[train_static_cb_0]\nNumber of rejections in a month.\n\n\n192\nformonth_206L\n[train_static_cb_0]\nNumber of cancelations in the previous month.\n\n\n193\nformonth_535L\n[train_static_cb_0]\nCredit history for the last month.\n\n\n194\nforquarter_1017L\n[train_static_cb_0]\nNumber of cancellations recorded in the credit...\n\n\n195\nforquarter_462L\n[train_static_cb_0]\nNumber of credit applications that were reject...\n\n\n196\nforquarter_634L\n[train_static_cb_0]\nCredit history for the last quarter.\n\n\n197\nfortoday_1092L\n[train_static_cb_0]\nClient's credit history for today.\n\n\n198\nforweek_1077L\n[train_static_cb_0]\nNumber of cancelations in the last week.\n\n\n199\nforweek_528L\n[train_static_cb_0]\nCredit history for the last week.\n\n\n200\nforweek_601L\n[train_static_cb_0]\nNumber of rejected applications in the last week.\n\n\n201\nforyear_618L\n[train_static_cb_0]\nNumber of application rejections in the previo...\n\n\n202\nforyear_818L\n[train_static_cb_0]\nNumber of cancelations that occurred in last y...\n\n\n203\nforyear_850L\n[train_static_cb_0]\nCredit history for the last year.\n\n\n204\nfourthquarter_440L\n[train_static_cb_0]\nNumber of results in fourth quarter.\n\n\n205\ngender_992L\n[train_person_1]\nGender of a person.\n\n\n206\nhomephncnt_628L\n[train_static_0_0, train_static_0_1]\nNumber of distinct home phones on client's app...\n\n\n207\nhousetype_905L\n[train_person_1]\nHouse type of the person.\n\n\n208\nhousingtype_772L\n[train_person_1]\nType of housing of the person.\n\n\n209\nincometype_1044T\n[train_person_1]\nType of income of the person\n\n\n210\ninittransactionamount_650A\n[train_static_0_0, train_static_0_1]\nInitial transaction amount of the credit appli...\n\n\n211\ninittransactioncode_186L\n[train_static_0_0, train_static_0_1]\nTransaction type of the initial credit transac...\n\n\n212\ninittransactioncode_279L\n[train_applprev_1_0, train_applprev_1_1]\nType of the initial transaction made in the pr...\n\n\n213\ninstallmentamount_644A\n[train_credit_bureau_b_1]\nInstalment amount of a closed and secured cred...\n\n\n214\ninstallmentamount_833A\n[train_credit_bureau_b_1]\nInstalment amount for a secured and active con...\n\n\n215\ninstlamount_768A\n[train_credit_bureau_a_1_0, train_credit_burea...\nInstalment amount for the active contract in c...\n\n\n216\ninstlamount_852A\n[train_credit_bureau_a_1_0, train_credit_burea...\nInstalment amount for closed contract.\n\n\n217\ninstlamount_892A\n[train_credit_bureau_b_1]\nInstalment amount for active credit contract.\n\n\n218\ninteresteffectiverate_369L\n[train_credit_bureau_b_1]\nInterest rate on active contract.\n\n\n219\ninterestrate_311L\n[train_static_0_0, train_static_0_1]\nThe interest rate of the active credit contract.\n\n\n220\ninterestrate_508L\n[train_credit_bureau_a_1_0, train_credit_burea...\nInterest rate for a closed contract in the cre...\n\n\n221\ninterestrategrace_34L\n[train_static_0_0, train_static_0_1]\nInterest rate during the grace period.\n\n\n222\ninterestrateyearly_538L\n[train_credit_bureau_b_1]\nAnnual interest rate for existing contract obt...\n\n\n223\nisbidproduct_1095L\n[train_static_0_0, train_static_0_1]\nFlag indicating if the product is a cross-sell.\n\n\n224\nisbidproduct_390L\n[train_applprev_1_0, train_applprev_1_1]\nFlag for determining if the product is a cross...\n\n\n225\nisbidproductrequest_292L\n[train_static_0_0, train_static_0_1]\nFlag indicating if the product is a cross-sell.\n\n\n226\nisdebitcard_527L\n[train_applprev_1_0, train_applprev_1_1]\nPrevious application flag indicating if produc...\n\n\n227\nisdebitcard_729L\n[train_static_0_0, train_static_0_1]\nFlag indicating if the product is a debit card.\n\n\n228\nisreference_387L\n[train_person_1]\nFlag indicating whether the person is a refere...\n\n\n229\nlanguage1_981M\n[train_person_1]\nThe primary language of the person.\n\n\n230\nlast180dayaveragebalance_704A\n[train_debitcard_1]\nAverage balance on debit card in the last 180 ...\n\n\n231\nlast180dayturnover_1134A\n[train_debitcard_1]\nDebit card's turnover within the last 180 days.\n\n\n232\nlast30dayturnover_651A\n[train_debitcard_1]\nDebit card turnover for the last 30 days.\n\n\n233\nlastactivateddate_801D\n[train_static_0_0, train_static_0_1]\nContract activation date for previous applicat...\n\n\n234\nlastapplicationdate_877D\n[train_static_0_0, train_static_0_1]\nDate of previous customer's application.\n\n\n235\nlastapprcommoditycat_1041M\n[train_static_0_0, train_static_0_1]\nCommodity category of the last loan applicatio...\n\n\n236\nlastapprcommoditytypec_5251766M\n[train_static_0_0, train_static_0_1]\nCommodity type of the last application.\n\n\n237\nlastapprcredamount_781A\n[train_static_0_0, train_static_0_1]\nCredit amount from the client's last application.\n\n\n238\nlastapprdate_640D\n[train_static_0_0, train_static_0_1]\nDate of approval on client's most recent previ...\n\n\n239\nlastcancelreason_561M\n[train_static_0_0, train_static_0_1]\nCancellation reason of the last application.\n\n\n240\nlastdelinqdate_224D\n[train_static_0_0, train_static_0_1]\nDate of the last delinquency occurrence.\n\n\n241\nlastdependentsnum_448L\n[train_static_0_0, train_static_0_1]\nNumber of dependents in the client's last loan...\n\n\n242\nlastotherinc_902A\n[train_static_0_0, train_static_0_1]\nAmount of other income reported by the client ...\n\n\n243\nlastotherlnsexpense_631A\n[train_static_0_0, train_static_0_1]\nMonthly expenses on other loans from the last ...\n\n\n244\nlastrejectcommoditycat_161M\n[train_static_0_0, train_static_0_1]\nCategory of commodity in the applicant's last ...\n\n\n245\nlastrejectcommodtypec_5251769M\n[train_static_0_0, train_static_0_1]\nCommodity type of the last rejected application.\n\n\n246\nlastrejectcredamount_222A\n[train_static_0_0, train_static_0_1]\nCredit amount on last rejected application.\n\n\n247\nlastrejectdate_50D\n[train_static_0_0, train_static_0_1]\nDate of most recent rejected application by th...\n\n\n248\nlastrejectreason_759M\n[train_static_0_0, train_static_0_1]\nReason for rejection on the most recent reject...\n\n\n249\nlastrejectreasonclient_4145040M\n[train_static_0_0, train_static_0_1]\nReason for the client's last loan rejection.\n\n\n250\nlastrepayingdate_696D\n[train_static_0_0, train_static_0_1]\nDate of the last payment made by the applicant.\n\n\n251\nlastst_736L\n[train_static_0_0, train_static_0_1]\nStatus of the client's previous credit applica...\n\n\n252\nlastupdate_1112D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of last update for an active contract fro...\n\n\n253\nlastupdate_260D\n[train_credit_bureau_b_1]\nLast update date for the active contracts.\n\n\n254\nlastupdate_388D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of last update for a closed contract in t...\n\n\n255\nmaininc_215A\n[train_static_0_0, train_static_0_1]\nClient's primary income amount.\n\n\n256\nmainoccupationinc_384A\n[train_person_1]\nAmount of the main income of the client.\n\n\n257\nmainoccupationinc_437A\n[train_applprev_1_0, train_applprev_1_1]\nClient's main income amount in their previous ...\n\n\n258\nmaritalst_385M\n[train_static_cb_0]\nMarital status of the client.\n\n\n259\nmaritalst_703L\n[train_person_1]\nMarital status of the client.\n\n\n260\nmaritalst_893M\n[train_static_cb_0]\nMarital status of the client\n\n\n261\nmastercontrelectronic_519L\n[train_static_0_0, train_static_0_1]\nFlag indicating the existence of the master co...\n\n\n262\nmastercontrexist_109L\n[train_static_0_0, train_static_0_1]\nFlag indicating whether or not the applicant h...\n\n\n263\nmaxannuity_159A\n[train_static_0_0, train_static_0_1]\nMaximum annuity previously obtained by client.\n\n\n264\nmaxannuity_4075009A\n[train_static_0_0, train_static_0_1]\nMaximal annuity offered to the client in the c...\n\n\n265\nmaxdbddpdlast1m_3658939P\n[train_static_0_0, train_static_0_1]\nMaximum number of days past due in the last mo...\n\n\n266\nmaxdbddpdtollast12m_3658940P\n[train_static_0_0, train_static_0_1]\nMaximum number of days past due in last 12 mon...\n\n\n267\nmaxdbddpdtollast6m_4187119P\n[train_static_0_0, train_static_0_1]\nMaximum number of days past due in last 6 mont...\n\n\n268\nmaxdebt4_972A\n[train_static_0_0, train_static_0_1]\nMaximal principal debt of the client in the hi...\n\n\n269\nmaxdebtpduevalodued_3940955A\n[train_credit_bureau_b_1]\nDays past due at the time of the maximum debt.\n\n\n270\nmaxdpdfrom6mto36m_3546853P\n[train_static_0_0, train_static_0_1]\nMaximum Days Past Due (DPD) in the period rang...\n\n\n271\nmaxdpdinstldate_3546855D\n[train_static_0_0, train_static_0_1]\nDate of instalment on which client was most da...\n\n\n272\nmaxdpdinstlnum_3546846P\n[train_static_0_0, train_static_0_1]\nInstalment number of which client was most day...\n\n\n273\nmaxdpdlast12m_727P\n[train_static_0_0, train_static_0_1]\nMaximum days past due in the past 12 months.\n\n\n274\nmaxdpdlast24m_143P\n[train_static_0_0, train_static_0_1]\nMaximal days past due in the last 24 months.\n\n\n275\nmaxdpdlast3m_392P\n[train_static_0_0, train_static_0_1]\nMaximum number of days past due in last 3 months.\n\n\n276\nmaxdpdlast6m_474P\n[train_static_0_0, train_static_0_1]\nMaximum days past due in the last 6 months.\n\n\n277\nmaxdpdlast9m_1059P\n[train_static_0_0, train_static_0_1]\nMaximum days past due in last 9 months.\n\n\n278\nmaxdpdtolerance_374P\n[train_static_0_0, train_static_0_1]\nMaximum number of days past due (with tolerance).\n\n\n279\nmaxdpdtolerance_577P\n[train_applprev_1_0, train_applprev_1_1]\nMaximum DPD with tolerance (on previous applic...\n\n\n280\nmaxinstallast24m_3658928A\n[train_static_0_0, train_static_0_1]\nMaximum instalment in the last 24 months\n\n\n281\nmaxlnamtstart6m_4525199A\n[train_static_0_0, train_static_0_1]\nMaximum loan amount started in the last 6 months.\n\n\n282\nmaxoutstandbalancel12m_4187113A\n[train_static_0_0, train_static_0_1]\nMaximum outstanding balance in the last 12 mon...\n\n\n283\nmaxpmtlast3m_4525190A\n[train_static_0_0, train_static_0_1]\nMaximum payment made by the client in the last...\n\n\n284\nmindbddpdlast24m_3658935P\n[train_static_0_0, train_static_0_1]\nMinimum days past due (or days before due) in ...\n\n\n285\nmindbdtollast24m_4525191P\n[train_static_0_0, train_static_0_1]\nMinimum days before due in last 24 months.\n\n\n286\nmobilephncnt_593L\n[train_static_0_0, train_static_0_1]\nNumber of persons with the same mobile phone n...\n\n\n287\nmonthlyinstlamount_332A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMonthly instalment amount for active contract.\n\n\n288\nmonthlyinstlamount_674A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMonthly amount of instalment payment on a clos...\n\n\n289\nmonthsannuity_845L\n[train_static_0_0, train_static_0_1]\nMonthly annuity amount for the applicant.\n\n\n290\nname_4527232M\n[train_tax_registry_a_1]\nName of employer.\n\n\n291\nname_4917606M\n[train_tax_registry_b_1]\nName of employer.\n\n\n292\nnominalrate_281L\n[train_credit_bureau_a_1_0, train_credit_burea...\nInterest rate of the active contract.\n\n\n293\nnominalrate_498L\n[train_credit_bureau_a_1_0, train_credit_burea...\nInterest rate for closed contract.\n\n\n294\nnum_group1\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n295\nnum_group2\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n296\nnumactivecreds_622L\n[train_static_0_0, train_static_0_1]\nNumber of active credits.\n\n\n297\nnumactivecredschannel_414L\n[train_static_0_0, train_static_0_1]\nNumber of active credits.\n\n\n298\nnumactiverelcontr_750L\n[train_static_0_0, train_static_0_1]\nNumber of active revolving credits.\n\n\n299\nnumberofcontrsvalue_258L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of active contracts in credit bureau.\n\n\n300\nnumberofcontrsvalue_358L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of closed credit contracts.\n\n\n301\nnumberofinstls_229L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of instalments on closed contract.\n\n\n302\nnumberofinstls_320L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of instalments of the active contract.\n\n\n303\nnumberofinstls_810L\n[train_credit_bureau_b_1]\nNumber of instalments for the active contract.\n\n\n304\nnumberofoutstandinstls_520L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of outstanding instalment for closed co...\n\n\n305\nnumberofoutstandinstls_59L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of outstanding instalments for the acti...\n\n\n306\nnumberofoverdueinstlmax_1039L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of outstanding instalments for active c...\n\n\n307\nnumberofoverdueinstlmax_1151L\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximum number of past due installments for a ...\n\n\n308\nnumberofoverdueinstlmaxdat_148D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of maximum number of past due instalments...\n\n\n309\nnumberofoverdueinstlmaxdat_641D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of maximum number of past due instalments...\n\n\n310\nnumberofoverdueinstls_725L\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximum number of past due instalments for an ...\n\n\n311\nnumberofoverdueinstls_834L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of past due instalments for a closed co...\n\n\n312\nnumberofqueries_373L\n[train_static_cb_0]\nNumber of queries to credit bureau.\n\n\n313\nnumcontrs3months_479L\n[train_static_0_0, train_static_0_1]\nNumber of contracts in last 3 months.\n\n\n314\nnumincomingpmts_3546848L\n[train_static_0_0, train_static_0_1]\nNumber of incoming payments.\n\n\n315\nnuminstlallpaidearly3d_817L\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid at least 3 days pri...\n\n\n316\nnuminstls_657L\n[train_static_0_0, train_static_0_1]\nNumber of instalments.\n\n\n317\nnuminstlsallpaid_934L\n[train_static_0_0, train_static_0_1]\nNumber of paid instalments.\n\n\n318\nnuminstlswithdpd10_728L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that were overdue for 10...\n\n\n319\nnuminstlswithdpd5_4187116L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that were overdue by at ...\n\n\n320\nnuminstlswithoutdpd_562L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that were not past due d...\n\n\n321\nnuminstmatpaidtearly2d_4499204L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that have been paid more...\n\n\n322\nnuminstpaid_4499208L\n[train_static_0_0, train_static_0_1]\nNumber of paid instalments.\n\n\n323\nnuminstpaidearly3d_3546850L\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid more than three day...\n\n\n324\nnuminstpaidearly3dest_4493216L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that have been paid more...\n\n\n325\nnuminstpaidearly5d_1087L\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid more than 5 days pr...\n\n\n326\nnuminstpaidearly5dest_4493211L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that were paid more than...\n\n\n327\nnuminstpaidearly5dobd_4499205L\n[train_static_0_0, train_static_0_1]\nNumber of installments paid more than 5 days p...\n\n\n328\nnuminstpaidearly_338L\n[train_static_0_0, train_static_0_1]\nNumber of installments paid prior to the due d...\n\n\n329\nnuminstpaidearlyest_4493214L\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid before the due date.\n\n\n330\nnuminstpaidlastcontr_4325080L\n[train_static_0_0, train_static_0_1]\nNumber of paid installments from the client's ...\n\n\n331\nnuminstpaidlate1d_3546852L\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid more than 1 day pas...\n\n\n332\nnuminstregularpaid_973L\n[train_static_0_0, train_static_0_1]\nNumber of fully paid regular installments in t...\n\n\n333\nnuminstregularpaidest_4493210L\n[train_static_0_0, train_static_0_1]\nNumber of fully paid regular installments on c...\n\n\n334\nnuminsttopaygr_769L\n[train_static_0_0, train_static_0_1]\nNumber of unpaid instalments.\n\n\n335\nnuminsttopaygrest_4493213L\n[train_static_0_0, train_static_0_1]\nNumber of unpaid instalments.\n\n\n336\nnuminstunpaidmax_3546851L\n[train_static_0_0, train_static_0_1]\nMaximum number of unpaid instalments.\n\n\n337\nnuminstunpaidmaxest_4493212L\n[train_static_0_0, train_static_0_1]\nMaximum number of unpaid instalments.\n\n\n338\nnumnotactivated_1143L\n[train_static_0_0, train_static_0_1]\nNumber of non-activated credits.\n\n\n339\nnumpmtchanneldd_318L\n[train_static_0_0, train_static_0_1]\nNumber of previous loan contracts for the appl...\n\n\n340\nnumrejects9m_859L\n[train_static_0_0, train_static_0_1]\nNumber of credit applications that were reject...\n\n\n341\nopencred_647L\n[train_static_0_0, train_static_0_1]\nNumber of active loans from the previous appli...\n\n\n342\nopeningdate_313D\n[train_deposit_1]\nDeposit account opening date.\n\n\n343\nopeningdate_857D\n[train_debitcard_1]\nDebit card opening date.\n\n\n344\noutstandingamount_354A\n[train_credit_bureau_a_1_0, train_credit_burea...\nOutstanding amount for closed credit contract ...\n\n\n345\noutstandingamount_362A\n[train_credit_bureau_a_1_0, train_credit_burea...\nActive contract's outstanding amount.\n\n\n346\noutstandingdebt_522A\n[train_applprev_1_0, train_applprev_1_1]\nAmount of outstanding debt on the client's pre...\n\n\n347\noverdueamount_31A\n[train_credit_bureau_a_1_0, train_credit_burea...\nPast due amount for a closed contract.\n\n\n348\noverdueamount_659A\n[train_credit_bureau_a_1_0, train_credit_burea...\nPast due amount for active contract.\n\n\n349\noverdueamountmax2_14A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximal past due amount for an active contract.\n\n\n350\noverdueamountmax2_398A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximal overdue amount for a closed contract.\n\n\n351\noverdueamountmax2date_1002D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of maximal past due amount for a closed c...\n\n\n352\noverdueamountmax2date_1142D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of maximal past due amount for an active ...\n\n\n353\noverdueamountmax_155A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximal past due amount for active contract.\n\n\n354\noverdueamountmax_35A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximal past due amount for a closed contract.\n\n\n355\noverdueamountmax_950A\n[train_credit_bureau_b_1]\nMaximal past due amount for active contract.\n\n\n356\noverdueamountmaxdatemonth_284T\n[train_credit_bureau_a_1_0, train_credit_burea...\nMonth when the maximum past due amount occurre...\n\n\n357\noverdueamountmaxdatemonth_365T\n[train_credit_bureau_a_1_0, train_credit_burea...\nMonth when maximum past due amount occurred fo...\n\n\n358\noverdueamountmaxdatemonth_494T\n[train_credit_bureau_b_1]\nMonth when the maximum past due amount was rec...\n\n\n359\noverdueamountmaxdateyear_2T\n[train_credit_bureau_a_1_0, train_credit_burea...\nYear when the maximum past due amount occurred...\n\n\n360\noverdueamountmaxdateyear_432T\n[train_credit_bureau_b_1]\nYear when max past due amount occurred for act...\n\n\n361\noverdueamountmaxdateyear_994T\n[train_credit_bureau_a_1_0, train_credit_burea...\nYear when maximum past due amount occurred for...\n\n\n362\npaytype1st_925L\n[train_static_0_0, train_static_0_1]\nType of first payment of the client.\n\n\n363\npaytype_783L\n[train_static_0_0, train_static_0_1]\nType of payment.\n\n\n364\npayvacationpostpone_4187118D\n[train_static_0_0, train_static_0_1]\nDate of last payment holiday instalment.\n\n\n365\npctinstlsallpaidearl3d_427L\n[train_static_0_0, train_static_0_1]\nPercentage of installments paid at least 3 day...\n\n\n366\npctinstlsallpaidlat10d_839L\n[train_static_0_0, train_static_0_1]\nPercentage of installments that were paid 10 o...\n\n\n367\npctinstlsallpaidlate1d_3546856L\n[train_static_0_0, train_static_0_1]\nPercentage of installments that are paid 1 or ...\n\n\n368\npctinstlsallpaidlate4d_3546849L\n[train_static_0_0, train_static_0_1]\nPercentage of installments that were paid 4 or...\n\n\n369\npctinstlsallpaidlate6d_3546844L\n[train_static_0_0, train_static_0_1]\nPercentage of installments that were paid 6 or...\n\n\n370\nperiodicityofpmts_1102L\n[train_credit_bureau_a_1_0, train_credit_burea...\nFrequency of instalments for a closed contract.\n\n\n371\nperiodicityofpmts_837L\n[train_credit_bureau_a_1_0, train_credit_burea...\nFrequency of instalments for an active contract.\n\n\n372\nperiodicityofpmts_997L\n[train_credit_bureau_b_1]\nFrequency of instalments for active credit con...\n\n\n373\nperiodicityofpmts_997M\n[train_credit_bureau_b_1]\nFrequency of instalments for active credit con...\n\n\n374\npersonindex_1023L\n[train_person_1]\nOrder of the person specified on the applicati...\n\n\n375\npersontype_1072L\n[train_person_1]\nPerson type.\n\n\n376\npersontype_792L\n[train_person_1]\nPerson type.\n\n\n377\npmtamount_36A\n[train_tax_registry_c_1]\nTax deductions amount for credit bureau payments.\n\n\n378\npmtaverage_3A\n[train_static_cb_0]\nAverage of tax deductions.\n\n\n379\npmtaverage_4527227A\n[train_static_cb_0]\nAverage of tax deductions.\n\n\n380\npmtaverage_4955615A\n[train_static_cb_0]\nAverage of tax deductions.\n\n\n381\npmtcount_4527229L\n[train_static_cb_0]\nNumber of tax deductions.\n\n\n382\npmtcount_4955617L\n[train_static_cb_0]\nNumber of tax deductions.\n\n\n383\npmtcount_693L\n[train_static_cb_0]\nNumber of tax deductions.\n\n\n384\npmtdaysoverdue_1135P\n[train_credit_bureau_b_1]\nNumber of days past due for existing contracts...\n\n\n385\npmtmethod_731M\n[train_credit_bureau_b_1]\nInstalment payment method for existing contrac...\n\n\n386\npmtnum_254L\n[train_static_0_0, train_static_0_1]\nTotal number of loan payments made by the client.\n\n\n387\npmtnum_8L\n[train_applprev_1_0, train_applprev_1_1]\nNumber of payments made for the previous appli...\n\n\n388\npmtnumpending_403L\n[train_credit_bureau_b_1]\nNumber of pending payments for active contract.\n\n\n389\npmts_date_1107D\n[train_credit_bureau_b_2]\nPayment date for an active contract according ...\n\n\n390\npmts_dpd_1073P\n[train_credit_bureau_a_2_4, train_credit_burea...\nDays past due of the payment for the active co...\n\n\n391\npmts_dpd_303P\n[train_credit_bureau_a_2_4, train_credit_burea...\nDays past due of the payment for terminated co...\n\n\n392\npmts_dpdvalue_108P\n[train_credit_bureau_b_2]\nValue of past due payment for active contract ...\n\n\n393\npmts_month_158T\n[train_credit_bureau_a_2_4, train_credit_burea...\nMonth of payment for a closed contract (num_gr...\n\n\n394\npmts_month_706T\n[train_credit_bureau_a_2_4, train_credit_burea...\nMonth of payment for active contract (num_grou...\n\n\n395\npmts_overdue_1140A\n[train_credit_bureau_a_2_4, train_credit_burea...\nOverdue payment for an active contract (num_gr...\n\n\n396\npmts_overdue_1152A\n[train_credit_bureau_a_2_4, train_credit_burea...\nOverdue payment for a closed contract (num_gro...\n\n\n397\npmts_pmtsoverdue_635A\n[train_credit_bureau_b_2]\nActive contract that has overdue payments (num...\n\n\n398\npmts_year_1139T\n[train_credit_bureau_a_2_4, train_credit_burea...\nYear of payment for an active contract (num_gr...\n\n\n399\npmts_year_507T\n[train_credit_bureau_a_2_4, train_credit_burea...\nPayment year for a closed credit contract (num...\n\n\n400\npmtscount_423L\n[train_static_cb_0]\nNumber of tax deduction payments.\n\n\n401\npmtssum_45A\n[train_static_cb_0]\nSum of tax deductions for the client.\n\n\n402\nposfpd10lastmonth_333P\n[train_static_0_0, train_static_0_1]\nAverage FPD10 (Share of contracts with first i...\n\n\n403\nposfpd30lastmonth_3976960P\n[train_static_0_0, train_static_0_1]\nAverage FPD30 (Share of contracts with first i...\n\n\n404\nposfstqpd30lastmonth_3976962P\n[train_static_0_0, train_static_0_1]\nAverage FSTPD30 (share of contracts with first...\n\n\n405\npostype_4733339M\n[train_applprev_1_0, train_applprev_1_1]\nType of point of sale.\n\n\n406\npreviouscontdistrict_112M\n[train_static_0_0, train_static_0_1]\nContact district of the client's previous appr...\n\n\n407\nprice_1097A\n[train_static_0_0, train_static_0_1]\nCredit price.\n\n\n408\nprocessingdate_168D\n[train_tax_registry_c_1]\nDate when the tax deduction is processed.\n\n\n409\nprofession_152M\n[train_applprev_1_0, train_applprev_1_1]\nProfession of the client during their previous...\n\n\n410\nprolongationcount_1120L\n[train_credit_bureau_a_1_0, train_credit_burea...\nCount of prolongations on terminated contract ...\n\n\n411\nprolongationcount_599L\n[train_credit_bureau_a_1_0, train_credit_burea...\nCount of active contract prolongations.\n\n\n412\npurposeofcred_426M\n[train_credit_bureau_a_1_0, train_credit_burea...\nPurpose of credit for active contract.\n\n\n413\npurposeofcred_722M\n[train_credit_bureau_b_1]\nPurpose of credit for active contracts.\n\n\n414\npurposeofcred_874M\n[train_credit_bureau_a_1_0, train_credit_burea...\nPurpose of credit on a closed contract.\n\n\n415\nrecorddate_4527225D\n[train_tax_registry_a_1]\nDate of tax deduction record.\n\n\n416\nrefreshdate_3813885D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate when the credit bureau's public sources h...\n\n\n417\nregistaddr_district_1083M\n[train_person_1]\nDistrict of person's registered address.\n\n\n418\nregistaddr_zipcode_184M\n[train_person_1]\nRegistered address's zip code of a person.\n\n\n419\nrejectreason_755M\n[train_applprev_1_0, train_applprev_1_1]\nReason for previous application rejection.\n\n\n420\nrejectreasonclient_4145042M\n[train_applprev_1_0, train_applprev_1_1]\nReason for rejection of the client's previous ...\n\n\n421\nrelatedpersons_role_762T\n[train_person_2]\nRelationship type of a client's related person...\n\n\n422\nrelationshiptoclient_415T\n[train_person_1]\nRelationship to the client.\n\n\n423\nrelationshiptoclient_642T\n[train_person_1]\nRelationship to the client.\n\n\n424\nremitter_829L\n[train_person_1]\nFlag indicating whether the client is a remitter.\n\n\n425\nrequesttype_4525192L\n[train_static_cb_0]\nTax authority request type.\n\n\n426\nresidualamount_1093A\n[train_credit_bureau_b_1]\nResidual amount of closed guarantee contract.\n\n\n427\nresidualamount_127A\n[train_credit_bureau_b_1]\nResidual amount of active guarantee contract.\n\n\n428\nresidualamount_3940956A\n[train_credit_bureau_b_1]\nResidual amount for the active contract.\n\n\n429\nresidualamount_488A\n[train_credit_bureau_a_1_0, train_credit_burea...\nResidual amount of a closed contract.\n\n\n430\nresidualamount_856A\n[train_credit_bureau_a_1_0, train_credit_burea...\nResidual amount for the active contract.\n\n\n431\nresponsedate_1012D\n[train_static_cb_0]\nTax authority's response date.\n\n\n432\nresponsedate_4527233D\n[train_static_cb_0]\nTax authority's response date.\n\n\n433\nresponsedate_4917613D\n[train_static_cb_0]\nTax authority's response date.\n\n\n434\nrevolvingaccount_394A\n[train_applprev_1_0, train_applprev_1_1]\nRevolving account that was present in the appl...\n\n\n435\nriskassesment_302T\n[train_static_cb_0]\nEstimated probability that the client will def...\n\n\n436\nriskassesment_940T\n[train_static_cb_0]\nEstimate of client's creditworthiness.\n\n\n437\nrole_1084L\n[train_person_1]\nType of contact role.\n\n\n438\nrole_993L\n[train_person_1]\nPerson's role.\n\n\n439\nsafeguarantyflag_411L\n[train_person_1]\nFlag indicating if client is using a flexible ...\n\n\n440\nsecondquarter_766L\n[train_static_cb_0]\nNumber of results in second quarter.\n\n\n441\nsellerplacecnt_915L\n[train_static_0_0, train_static_0_1]\nNumber of sellerplaces where the same client's...\n\n\n442\nsellerplacescnt_216L\n[train_static_0_0, train_static_0_1]\nNumber of sellerplaces where the same client's...\n\n\n443\nsex_738L\n[train_person_1]\nGender of the client.\n\n\n444\nstatus_219L\n[train_applprev_1_0, train_applprev_1_1]\nPrevious application status.\n\n\n445\nsubjectrole_182M\n[train_credit_bureau_a_1_0, train_credit_burea...\nSubject role in active credit contract.\n\n\n446\nsubjectrole_326M\n[train_credit_bureau_b_1]\nSubject role in active credit contract.\n\n\n447\nsubjectrole_43M\n[train_credit_bureau_b_1]\nSubject role in closed credit contract.\n\n\n448\nsubjectrole_93M\n[train_credit_bureau_a_1_0, train_credit_burea...\nSubject role in closed credit contract.\n\n\n449\nsubjectroles_name_541M\n[train_credit_bureau_a_2_4, train_credit_burea...\nName of subject role in closed credit contract...\n\n\n450\nsubjectroles_name_838M\n[train_credit_bureau_a_2_4, train_credit_burea...\nName of subject role in active credit contract...\n\n\n451\nsumoutstandtotal_3546847A\n[train_static_0_0, train_static_0_1]\nSum of total outstanding amount.\n\n\n452\nsumoutstandtotalest_4493215A\n[train_static_0_0, train_static_0_1]\nSum of total outstanding amount.\n\n\n453\ntarget\n[train_base]\nNaN\n\n\n454\ntenor_203L\n[train_applprev_1_0, train_applprev_1_1]\nNumber of instalments in the previous applicat...\n\n\n455\nthirdquarter_1082L\n[train_static_cb_0]\nNumber of results in third quarter.\n\n\n456\ntotalamount_503A\n[train_credit_bureau_b_1]\nTotal amount of active secured credit for a cl...\n\n\n457\ntotalamount_6A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal amount of closed contracts.\n\n\n458\ntotalamount_881A\n[train_credit_bureau_b_1]\nTotal amount of secured credit from closed con...\n\n\n459\ntotalamount_996A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal amount of active contracts in the credit...\n\n\n460\ntotaldebt_9A\n[train_static_0_0, train_static_0_1]\nTotal amount of debt.\n\n\n461\ntotaldebtoverduevalue_178A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal amount of past due debt on active contra...\n\n\n462\ntotaldebtoverduevalue_718A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal overdue debt amount for closed credit co...\n\n\n463\ntotaloutstanddebtvalue_39A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal outstanding debt for active contracts in...\n\n\n464\ntotaloutstanddebtvalue_668A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal outstanding debt for the closed contract...\n\n\n465\ntotalsettled_863A\n[train_static_0_0, train_static_0_1]\nSum of all payments made by the client.\n\n\n466\ntotinstallast1m_4525188A\n[train_static_0_0, train_static_0_1]\nTotal amount of monthly instalments paid in th...\n\n\n467\ntwobodfilling_608L\n[train_static_0_0, train_static_0_1]\nType of application process.\n\n\n468\ntype_25L\n[train_person_1]\nContact type of a person.\n\n\n469\ntypesuite_864L\n[train_static_0_0, train_static_0_1]\nPersons accompanying the client during the loa...\n\n\n470\nvalidfrom_1069D\n[train_static_0_0, train_static_0_1]\nDate since the client has an active campaign.\n\n\n\n\n\n\n\n\n\nlookuptable[lookuptable.File.apply(lambda x: 'train_credit_bureau_a_2_6' in x)]\n\n\n\n\n\n\n\n\n\nVariable\nFile\nDescription\n\n\n\n\n49\ncase_id\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n73\ncollater_typofvalofguarant_298M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral valuation type (active contract).\n\n\n74\ncollater_typofvalofguarant_407M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral valuation type (closed contract).\n\n\n75\ncollater_valueofguarantee_1124L\n[train_credit_bureau_a_2_4, train_credit_burea...\nValue of collateral for active contract.\n\n\n76\ncollater_valueofguarantee_876L\n[train_credit_bureau_a_2_4, train_credit_burea...\nValue of collateral for closed contract.\n\n\n77\ncollaterals_typeofguarante_359M\n[train_credit_bureau_a_2_4, train_credit_burea...\nType of collateral that was used as a guarante...\n\n\n78\ncollaterals_typeofguarante_669M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral type for the active contract.\n\n\n294\nnum_group1\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n295\nnum_group2\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n390\npmts_dpd_1073P\n[train_credit_bureau_a_2_4, train_credit_burea...\nDays past due of the payment for the active co...\n\n\n391\npmts_dpd_303P\n[train_credit_bureau_a_2_4, train_credit_burea...\nDays past due of the payment for terminated co...\n\n\n393\npmts_month_158T\n[train_credit_bureau_a_2_4, train_credit_burea...\nMonth of payment for a closed contract (num_gr...\n\n\n394\npmts_month_706T\n[train_credit_bureau_a_2_4, train_credit_burea...\nMonth of payment for active contract (num_grou...\n\n\n395\npmts_overdue_1140A\n[train_credit_bureau_a_2_4, train_credit_burea...\nOverdue payment for an active contract (num_gr...\n\n\n396\npmts_overdue_1152A\n[train_credit_bureau_a_2_4, train_credit_burea...\nOverdue payment for a closed contract (num_gro...\n\n\n398\npmts_year_1139T\n[train_credit_bureau_a_2_4, train_credit_burea...\nYear of payment for an active contract (num_gr...\n\n\n399\npmts_year_507T\n[train_credit_bureau_a_2_4, train_credit_burea...\nPayment year for a closed credit contract (num...\n\n\n449\nsubjectroles_name_541M\n[train_credit_bureau_a_2_4, train_credit_burea...\nName of subject role in closed credit contract...\n\n\n450\nsubjectroles_name_838M\n[train_credit_bureau_a_2_4, train_credit_burea...\nName of subject role in active credit contract...\n\n\n\n\n\n\n\n\n\n# use jupyter notebook\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\nOUT = widgets.Output()\n\ndef f(feature=widgets.Text()):\n    # OUT.clear_output()\n    # with OUT:\n    display(feature)\n    # return(feature)\n    if feature ==\"\":\n        return display(lookuptable);\n    else:\n        return display(lookuptable.query(f'Variable.str.contains(\"{feature}\")'));\nw=interactive(f)"
  },
  {
    "objectID": "01-06 GUI Graphic Interface/Game of Life/Untitled.html",
    "href": "01-06 GUI Graphic Interface/Game of Life/Untitled.html",
    "title": "Game of Life Demo Project",
    "section": "",
    "text": "pip install tomli\n\nCollecting tomli\n  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\nInstalling collected packages: tomli\nSuccessfully installed tomli-2.0.1\n\n[notice] A new release of pip is available: 23.0.1 -&gt; 23.3.2\n[notice] To update, run: pip3.10 install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom platform import python_version\n#jupyter kernelspec list\n\n\nimport collections\nneighbors = (\n            (-1, -1),  # Above left\n            (-1, 0),  # Above\n            (-1, 1),  # Above right\n            (0, -1),  # Left\n            (0, 1),  # Right\n            (1, -1),  # Below left\n            (1, 0),  # Below\n            (1, 1),  # Below right\n        )\nnum_neighbors = collections.defaultdict(int)\nnum_neighbors[(0, 0)] =1\nnum_neighbors[(0, 1)] = 2\n\n\ndefault_dict = {}\ndefault_dict[(1,2)] = 1\n\n\nfrom rplife import grid, patterns\n\nblinker = patterns.Pattern(\"Blinker\", {(2, 1), (2, 2), (2, 3)})\ngrid = grid.LifeGrid(blinker)\n\nprint(grid.as_string([0, 0, 5, 5]))\nprint(grid.evolve().as_string([0,0,5,5]))\n\n Blinker  \n · · · · ·\n · · · · ·\n · ♥︎ ♥︎ ♥︎ ·\n · · · · ·\n · · · · ·\n Blinker  \n · · · · ·\n · · ♥︎ · ·\n · · ♥︎ · ·\n · · ♥︎ · ·\n · · · · ·\n\n\n\nprint(grid.evolve().as_string([0,0,5,5]))\n\n Blinker  \n · · · · ·\n · · ♥︎ · ·\n · · ♥︎ · ·\n · · ♥︎ · ·\n · · · · ·\n\n\n\nimport curses \nfrom rplife import patterns, views\nfrom rplife.cli import get_command_line_args\n\nargs=get_command_line_args()\n\nusage: rplife [-h] [--version] [-p {Blinker}] [-a] [-v {CursesView}]\n              [-g NUM_GENERATIONS] [-f FRAMES_PER_SECOND]\nrplife: error: argument -f/--fps: invalid int value: '/Users/frankliang/Library/Jupyter/runtime/kernel-v2-7226nR03oT0p6f7Z.json'\n\n\nAttributeError: 'tuple' object has no attribute 'tb_frame'"
  },
  {
    "objectID": "01-01 Setup Delta Lake/02.html",
    "href": "01-01 Setup Delta Lake/02.html",
    "title": "Setup Duckdb",
    "section": "",
    "text": "import duckdb\nduckdb.sql('SELECT 42').show()\n\n┌───────┐\n│  42   │\n│ int32 │\n├───────┤\n│    42 │\n└───────┘\n\n\n\n\ncon = duckdb.connect()\nduckdb.sql('SELECT * FROM \"ingest/bank-data.parquet\" limit 3')#.df() method return pandas dataframe\n\n┌─────────────────────┬──────────────────┬─────────┬──────────────┬───────────────┬─────────────┬───────────┐\n│        Date         │   Description    │  Type   │ Money In (£) │ Money Out (£) │ Balance (£) │ Category  │\n│      timestamp      │     varchar      │ varchar │    double    │    double     │   double    │  varchar  │\n├─────────────────────┼──────────────────┼─────────┼──────────────┼───────────────┼─────────────┼───────────┤\n│ 2023-10-02 00:00:00 │ STGCOACH/CTYLINK │ DEB     │          0.0 │           2.0 │      5770.0 │ Transport │\n│ 2023-10-02 00:00:00 │ PRET A MANGER    │ DEB     │          0.0 │           5.0 │      5764.0 │ Cafe      │\n│ 2023-10-02 00:00:00 │ STGCOACH/CTYLINK │ DEB     │          0.0 │           2.0 │      5762.0 │ Transport │\n└─────────────────────┴──────────────────┴─────────┴──────────────┴───────────────┴─────────────┴───────────┘\n\n\n\nimport duckdb\n\nwith duckdb.connect('file.db') as con:\n    con.sql('CREATE TABLE test(i INTEGER)')\n    con.sql('INSERT INTO test VALUES (42)')\n    con.table('test').show()\n\n\ncon=duckdb.connect('file.db')\ncon.table('test')\n\ncon.install_extension(\"spatial\")\ncon.install_extension(\"spatial\")"
  },
  {
    "objectID": "01-01 Setup Delta Lake/02.html#set-up-duckdb",
    "href": "01-01 Setup Delta Lake/02.html#set-up-duckdb",
    "title": "Setup Duckdb",
    "section": "",
    "text": "import duckdb\nduckdb.sql('SELECT 42').show()\n\n┌───────┐\n│  42   │\n│ int32 │\n├───────┤\n│    42 │\n└───────┘\n\n\n\n\ncon = duckdb.connect()\nduckdb.sql('SELECT * FROM \"ingest/bank-data.parquet\" limit 3')#.df() method return pandas dataframe\n\n┌─────────────────────┬──────────────────┬─────────┬──────────────┬───────────────┬─────────────┬───────────┐\n│        Date         │   Description    │  Type   │ Money In (£) │ Money Out (£) │ Balance (£) │ Category  │\n│      timestamp      │     varchar      │ varchar │    double    │    double     │   double    │  varchar  │\n├─────────────────────┼──────────────────┼─────────┼──────────────┼───────────────┼─────────────┼───────────┤\n│ 2023-10-02 00:00:00 │ STGCOACH/CTYLINK │ DEB     │          0.0 │           2.0 │      5770.0 │ Transport │\n│ 2023-10-02 00:00:00 │ PRET A MANGER    │ DEB     │          0.0 │           5.0 │      5764.0 │ Cafe      │\n│ 2023-10-02 00:00:00 │ STGCOACH/CTYLINK │ DEB     │          0.0 │           2.0 │      5762.0 │ Transport │\n└─────────────────────┴──────────────────┴─────────┴──────────────┴───────────────┴─────────────┴───────────┘\n\n\n\nimport duckdb\n\nwith duckdb.connect('file.db') as con:\n    con.sql('CREATE TABLE test(i INTEGER)')\n    con.sql('INSERT INTO test VALUES (42)')\n    con.table('test').show()\n\n\ncon=duckdb.connect('file.db')\ncon.table('test')\n\ncon.install_extension(\"spatial\")\ncon.install_extension(\"spatial\")"
  },
  {
    "objectID": "01-01 Setup Delta Lake/02.html#details",
    "href": "01-01 Setup Delta Lake/02.html#details",
    "title": "Setup Duckdb",
    "section": "Details",
    "text": "Details\nCreate a DuctDBPyConnection object. You can then use this connection with python with as live connection object. - If the database file does not exist, it will be created - the file extension may be .db, .duckdb, or anything else - The special value :memory: (the default) can be used to create an in-memory database. - Read-only mode is required if multiple Python processes want to access the same database file at the same time. - providing the special value :default: to connect.\n\nParametrised Query\n\nduckdb.execute(\"\"\"\n    SELECT\n        $my_param,\n        $other_param,\n        $also_param\n    \"\"\",\n    {\n        'my_param': 5,\n        'other_param': 'DuckDB',\n        'also_param': [42]\n    }\n).fetchall()\n\n[(5, 'DuckDB', [42])]\n\n\n\n# create a table\ncon.execute(\"CREATE TABLE items(item VARCHAR, value DECIMAL(10, 2), count INTEGER)\")\n# insert two items into the table\ncon.execute(\"INSERT INTO items VALUES ('jeans', 20.0, 1), ('hammer', 42.2, 2)\")\n\n# retrieve the items again\ncon.execute(\"SELECT * FROM items\")\nprint(con.fetchall())\n\n[('jeans', Decimal('20.00'), 1), ('hammer', Decimal('42.20'), 2)]\n\n\n\ncon.execute(\"SELECT * FROM items\")\nprint(con.fetchall())\n\n[('jeans', Decimal('20.00'), 1), ('hammer', Decimal('42.20'), 2)]\n\n\n\n\nReplacement Scan\n\nmy_statement = duckdb.sql('SELECT * FROM \"ingest/bank-data.parquet\"')\n\n\nduckdb.sql(\"select * from my_statement order by random() limit 3\")\n\n┌─────────────────────┬────────────────────┬─────────┬──────────────┬───────────────┬─────────────┬────────────────┐\n│        Date         │    Description     │  Type   │ Money In (£) │ Money Out (£) │ Balance (£) │    Category    │\n│      timestamp      │      varchar       │ varchar │    double    │    double     │   double    │    varchar     │\n├─────────────────────┼────────────────────┼─────────┼──────────────┼───────────────┼─────────────┼────────────────┤\n│ 2023-11-23 00:00:00 │ TESCO STORES 2487  │ DEB     │          0.0 │           3.0 │      3777.0 │ Food & Grocery │\n│ 2023-06-14 00:00:00 │ COSTA COFFEE 43011 │ DEB     │          0.0 │           6.0 │      3590.0 │ Cafe           │\n│ 2023-06-08 00:00:00 │ CO-OP GROUP 070527 │ DEB     │          0.0 │           4.0 │      4341.0 │ Food & Grocery │\n└─────────────────────┴────────────────────┴─────────┴──────────────┴───────────────┴─────────────┴────────────────┘\n\n\n\nduckdb.sql('''\n           select *, date_trunc('week', Date) as month\n           from my_statement\n           where Category != 'Rent & Essential'\n           ''')\\\n           .aggregate('month, round(sum(\"Money Out (£)\")/7, 2) as avg_per_day')\\\n           .aggregate('round(mean(avg_per_day), 2)')\n\n┌─────────────────────────────┐\n│ round(mean(avg_per_day), 2) │\n│           double            │\n├─────────────────────────────┤\n│                       67.77 │\n└─────────────────────────────┘\n\n\n\n\naggregate(): this is part of ibis frameworkd\n\nmy_statement.aggregate('Category as g, sum(\"Balance (£)\")')\n\n┌──────────────────┬────────────────────┐\n│        g         │ sum(\"Balance (£)\") │\n│     varchar      │       double       │\n├──────────────────┼────────────────────┤\n│ Rent & Essential │            28377.0 │\n│ Cafe             │           457548.0 │\n│ Cloth & Shopping │            19190.0 │\n│ Transport        │           644101.0 │\n│ Subscri & Apple  │           251391.0 │\n│ Other            │          1283628.0 │\n│ Food & Grocery   │           682436.0 │\n└──────────────────┴────────────────────┘\n\n\n\n\nJupyter Extensions\n\n%load_ext sql\nconn = duckdb.connect()\n%sql conn --alias duckdb\n\n\n%sql select * from my_statement order by random() limit 3\n\nRunning query in 'duckdb'\n\n\n\n\n\n\nDate\nDescription\nType\nMoney In (£)\nMoney Out (£)\nBalance (£)\nCategory\n\n\n\n\n2023-08-31 00:00:00\nFULL FIBRE LIMITED\nBGC\n2327.0\n0.0\n5153.0\nOther\n\n\n2023-08-31 00:00:00\nSTGCOACH/CTYLINK\nDEB\n0.0\n2.0\n5154.0\nTransport\n\n\n2023-10-02 00:00:00\nEXETER CITY COUNCI\nDD\n0.0\n29.0\n5666.0\nSubscri & Apple\n\n\n\n\n\n\n\n%%sql\nselect * \nfrom my_statement\nwhere \"Money Out (£)\" &gt; 100\n\nRunning query in 'duckdb'\n\n\n\n\n\n\nDate\nDescription\nType\nMoney In (£)\nMoney Out (£)\nBalance (£)\nCategory\n\n\n\n\n2023-10-16 00:00:00\nAllSaints Exeter\nDEB\n0.0\n130.0\n4753.0\nOther\n\n\n2023-10-26 00:00:00\nAPPLE\nDEB\n0.0\n1099.0\n3273.0\nSubscri & Apple\n\n\n2023-10-30 00:00:00\nSTEPHEN GIBSON\nFPO\n0.0\n350.0\n2808.0\nRent & Essential\n\n\n2023-11-06 00:00:00\nTIME FLIES\nDEB\n0.0\n175.0\n4485.0\nOther\n\n\n2023-11-20 00:00:00\nWH Smith Exeter\nDEB\n0.0\n102.0\n3892.0\nOther\n\n\n2023-11-27 00:00:00\nAMZNMktplace\nDEB\n0.0\n133.0\n3468.0\nOther\n\n\n2023-11-27 00:00:00\nSTEPHEN GIBSON\nFPO\n0.0\n350.0\n3065.0\nRent & Essential\n\n\n2023-11-29 00:00:00\nUKVISAFEE6JAA02219\nDEB\n0.0\n827.0\n2176.0\nRent & Essential\n\n\n2023-11-29 00:00:00\nIHS123411215PA01 2\nDEB\n0.0\n1248.0\n928.0\nOther\n\n\n2023-07-03 00:00:00\nTRAINLINE\nDEB\n0.0\n106.0\n4376.0\nTransport\n\n\n\n\nTruncated to displaylimit of 10.\n\n\n\n\n\nPloting without load data to in memory\n\n%sqlplot histogram --column \"Money Out (£)\" --table my_statement\n\n\n\n\n\n\n\n\n\n\nggplot api (in development)\n\n%%sql \nclean_statement &lt;&lt; select --save to a variable\n    Category,\n    \"Money Out (£)\" as Spending\n    from my_statement\n\nRunning query in 'duckdb'\n\n\n\ntype(clean_statement)\nimport pandas as pd\n\n\nclean_statement = clean_statement.DataFrame() if type(clean_statement) is not pd.DataFrame else clean_statement # the result must be converted to DataFrame to allow ploting in the next line\nfrom sql.ggplot import ggplot, aes, geom_boxplot, geom_histogram, facet_wrap\n(\n    ggplot(table=\"clean_statement\", mapping=aes(x=\"Spending\")) +\n    geom_histogram(bins=10,fill=\"Category\")\n)\n\n\n\n\n\n\n\n\n\n\nAbout Ibis Framework\n\n%pip install ibis\n\n11703.48s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\nCollecting ibis\n  Downloading ibis-3.3.0-py3-none-any.whl.metadata (1.3 kB)\nDownloading ibis-3.3.0-py3-none-any.whl (16 kB)\nDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg&lt;=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\nInstalling collected packages: ibis\nSuccessfully installed ibis-3.3.0\nNote: you may need to restart the kernel to use updated packages."
  },
  {
    "objectID": "01-01 Deep Learning Py/01-13.html",
    "href": "01-01 Deep Learning Py/01-13.html",
    "title": "First ML Model",
    "section": "",
    "text": "Top Note\nLoss Function, Optimizerm, Arch\n\nimport keras\ndef plot_history(self):\n    loss=self.history['loss']\n    acc=self.history['accuracy']\n    val_loss=self.history['val_loss']\n    val_acc=self.history['val_accuracy']\n\n    epochs=range(len(loss))\n\n    _, (ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n    ax1.plot(epochs, loss,'k.',label='Training Loss')\n    ax1.plot(epochs, val_loss,'k-', label='Validation Loss')\n    ax1.set_title('Loss Function')\n\n    ax2.plot(epochs,acc, 'k.')\n    ax2.plot(epochs,val_acc,'k-')\n    ax2.set_title('Accuracy')\nkeras.src.callbacks.History.plot=plot_history"
  },
  {
    "objectID": "01-01 Deep Learning Py/01-13.html#reuters-classification",
    "href": "01-01 Deep Learning Py/01-13.html#reuters-classification",
    "title": "First ML Model",
    "section": "Reuters Classification",
    "text": "Reuters Classification\nhere are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.\n\n# class utilities\n# you thought they'd have a plot method by now\nimport keras\ndef plot_history(self):\n    loss=self.history['loss']\n    acc=self.history['accuracy']\n    val_loss=self.history['val_loss']\n    val_acc=self.history['val_accuracy']\n\n    epochs=range(len(loss))\n\n    _, (ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n    ax1.plot(epochs, loss,'k.')\n    ax1.plot(epochs, val_loss,'k-')\n    ax1.set_title('Loss Function')\n\n    ax2.plot(epochs,acc, 'k.')\n    ax2.plot(epochs,val_acc,'k-')\n    ax2.set_title('Accuracy')\nkeras.src.callbacks.History.plot=plot_history\n\n\ndef to_one_hot(labels, dimension=46):\n    results = np.zeros((len(labels), dimension))\n    for i, label in enumerate(labels):\n        results[i, label] = 1.\n    return results\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results\n\n\nfrom keras.datasets import reuters\n\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n    num_words=10000)\n\n\nword_index = reuters.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n    train_data[0]])                                                          #1\ndecoded_newswire\n#1 - Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”\n         \n\n'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'\n\n\n\nimport numpy as np\n\n\n\nx_train = vectorize_sequences(train_data)            #1\nx_test = vectorize_sequences(test_data)              #2\n\n#1 - Vectorized training data\n#2 - Vectorized test data\n         \n\n\none_hot_train_labels = to_one_hot(train_labels)        #1\none_hot_test_labels = to_one_hot(test_labels)          #2\n\n#1 - Vectorized training labels\n#2 - Vectorized test labels\n\n\n\nfor para in train_data[:3]:\n    words=[reverse_word_index.get(i-3,'?') for i in range(len(para))]\n    sentence=' '.join(words)\n    print(textwrap.fill(sentence,70))\n\n? ? ? ? the of to in said and a mln 3 for vs dlrs it reuter 000 1 pct\non from is that its cts by at year be with 2 will was billion he u s\nnet has would an as 5 not loss 4 1986 company which but this shr last\nare lt have or 6 bank 7 were 8 had oil trade share one about 0 inc 9\nnew profit also market they two shares stock corp tonnes 10 up been\nrevs\n? ? ? ? the of to in said and a mln 3 for vs dlrs it reuter 000 1 pct\non from is that its cts by at year be with 2 will was billion he u s\nnet has would an as 5 not loss 4 1986 company which but this shr last\nare\n? ? ? ? the of to in said and a mln 3 for vs dlrs it reuter 000 1 pct\non from is that its cts by at year be with 2 will was billion he u s\nnet has would an as 5 not loss 4 1986 company which but this shr last\nare lt have or 6 bank 7 were 8 had oil trade share one about 0 inc 9\nnew profit also market they two shares stock corp tonnes 10 up been\nrevs prices sales 1987 per may after april march more price than\nquarter first other rate 15 group february 1985 government if exchange\nthree january co against dollar could we offer over told 20 agreement\nweek production note 30 their some foreign interest no japan tax 50\nexpected 12 total under all rose\n\n\n\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nx_val = x_train[:1000]\npartial_x_train = x_train[1000:]\n\ny_val = one_hot_train_labels[:1000]\npartial_y_train = one_hot_train_labels[1000:]\n\n# this coponent can be rerun often\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n\n\nhistory.plot()\nplt.gcf().suptitle('A rmsporp optimiser on categorical_crossentropy')\n\nText(0.5, 0.98, 'A rmsporp optimiser on categorical_crossentropy')"
  },
  {
    "objectID": "01-01 Deep Learning Py/01-13.html#the-boston-housing-price",
    "href": "01-01 Deep Learning Py/01-13.html#the-boston-housing-price",
    "title": "First ML Model",
    "section": "The Boston Housing Price",
    "text": "The Boston Housing Price\n\nfrom keras.datasets import boston_housing\n\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n57026/57026 [==============================] - 0s 1us/step\n\n\n\nmean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data /= std\n\ntest_data -= mean\ntest_data /= std\n\n\nfrom keras import models\nfrom keras import layers\n\ndef build_model():\n    model = models.Sequential()                                  #1\n    model.add(layers.Dense(64, activation='relu',\n                           input_shape=(train_data.shape[1],)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model\n\n#1 - Because you’ll need to instantiate the same model multiple times, you use a function to construct it.  \n         \n\n\nimport numpy as np\n\nk = 4\nnum_val_samples = len(train_data) // k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print('processing fold #', i)\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]    #1\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n\n    partial_train_data = np.concatenate(                                     #2\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n\n    model = build_model()                                                    #3\n    model.fit(partial_train_data, partial_train_targets,                     #4\n              epochs=num_epochs, batch_size=1, verbose=0)\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)      #5\n    all_scores.append(val_mae)\n\n#1 - Prepares the validation data: data from partition #k\n#2 - Prepares the training data: data from all other partitions\n#3 - Builds the Keras model (already compiled)\n#4 - Trains the model (in silent mode, verbose = 0)\n#5 - Evaluates the model on the validation data\n         \n\nprocessing fold # 0\nprocessing fold # 1\nprocessing fold # 2\nprocessing fold # 3\n\n\n\nall_scores\nnp.mean(all_scores)\n\n2.540156304836273"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gold Fish Projects Blog",
    "section": "",
    "text": "Frank Liang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Discriminant Analysis\n\n\n\n\n\n\nStatistics\n\n\nfrom-scratch\n\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Principle Component Analyis\n\n\n\n\n\n\nStatistics\n\n\nfrom-scratch\n\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nWork with AMES Data\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nML with Tidymodel\n\n\n\n\n\n\nMachine-Learning\n\n\nR\n\n\nBasics\n\n\n\n\n\n\n\n\n\nApr 21, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nEngineering Features for Time Series\n\n\n\n\n\n\nMachine-Learning\n\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\nYoung Frank\n\n\n\n\n\n\n\n\n\n\n\n\nUse R in a jupyter notebook?\n\n\n\n\n\n\nMachine-Learning\n\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\nYoung Frank\n\n\n\n\n\n\n\n\n\n\n\n\nData Normality\n\n\n\n\n\n\nMachine-Learning\n\n\nPython\n\n\nStatistic\n\n\nExperiment\n\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\nYoung Frank\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Data\n\n\n\n\n\n\nUKRI\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nTrend and Elite Project\n\n\n\n\n\n\nUKRI\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Text Ananlysis - Word Occurance\n\n\n\n\n\n\nUKRI\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nText Analysis - Use N-Gram Graph\n\n\n\n\n\n\nUKRI\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Modeling\n\n\n\n\n\n\nUKRI\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment of Zipf’s Law\n\n\n\n\n\n\nUKRI\n\n\nText-Analysis\n\n\nStatistic\n\n\nExperiment\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nPublic Funding for Artificial Intelligence\n\n\n\n\n\n\nUKRI\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nWhat data look like?\n\n\n\n\n\n\nKaggle\n\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nF.L and Team\n\n\n\n\n\n\n\n\n\n\n\n\nExplore Data in train_static\n\n\n\n\n\n\nKaggle\n\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nF.L and Team\n\n\n\n\n\n\n\n\n\n\n\n\nTry out vega altairs geo spatial visualisation\n\n\n\n\n\n\nPython\n\n\nVisualisation\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nGame of Life Demo Project\n\n\n\n\n\n\nBackend-Demo\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nVisualise Matrix Transformation as Paper Folding\n\n\n\n\n\n\nPython\n\n\nMachine-Learning\n\n\nDeep-Learning\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nF. L\n\n\n\n\n\n\n\n\n\n\n\n\nFirst ML Model\n\n\n\n\n\n\nPython\n\n\nMachine-Learning\n\n\nDeep-Learning\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nF. L\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning with Python Summary Page\n\n\n\n\n\n\nPython\n\n\nMachine-Learning\n\n\nDeep-Learning\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nF. L\n\n\n\n\n\n\n\n\n\n\n\n\nSetup Spark and and delta Lake\n\n\n\n\n\n\nbasic\n\n\nDelta\n\n\nDuckDB\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nSetup Duckdb\n\n\n\n\n\n\nbasic\n\n\nDelta\n\n\nDuckDB\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nF.L\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "04-20 ML with Tidymodel/01.html",
    "href": "04-20 ML with Tidymodel/01.html",
    "title": "ML with Tidymodel",
    "section": "",
    "text": "This notebook summaries key point from Hadley Wickham’s Tidy Model with R. The book only covers basic usage of tidy-model and some other dimension reduction techniques.\nLink to the Book: https://www.tmwr.org/\n\n\n\n\nCode\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(tidymodels)\ndata(\"ames\")\n\n## refer to tmap: https://r-tmap.github.io/tmap-book/visual-variables.html\n## for osm data: https://cran.r-project.org/web/packages/osmdata/vignettes/osmdata.html\n## for query streets: https://wiki.openstreetmap.org/wiki/Key%3ahighway\n\names_sf = sf::st_as_sf(ames,coords = c(\"Longitude\",\"Latitude\"), crs=4326)\names_bbox = sf::st_bbox(ames_sf)\nosm_streets = opq(bbox = ames_bbox) |&gt; \n  add_osm_feature(key=\"highway\",value = c(\n                                          'secondary'\n                                          ,'primary'\n                                          ,'tertiary'\n                                          ,'unclassified'\n                                          ,'residential')) |&gt; \n  # add_osm_feature(key=\"highway\",value = 'motorway') |&gt; \n  osmdata_sf()\n\n## view a intersection\nstreets_sf = sf::st_intersection(sf::st_as_sfc(ames_bbox), osm_streets$osm_lines)\n\ntm_shape(streets_sf) + \n  tm_lines(col='grey') +\ntm_shape(ames_sf) + \n  tm_dots( shape = \"Lot_Shape\"\n          ,col=\"Neighborhood\"\n          ,style = \"cont\"\n          ,size=0.05\n          ,border.col=NA\n          ,border.lwd=0.01) + \n  tm_layout(legend.show=FALSE)\n\n\n\n\n\n\n\n\n\names familar with this data may come handy compare different model output later.\n\n\n\n\nCode\nlibrary(tidymodels)\n# \nggplot2::theme_set(theme_minimal())\ntidymodels_prefer()\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")\n\n\n\n\n\n\n\n\n\nFirst thing they want to tell you is the data is not normal so require you to normalise somehow.\n\n\n\nCode\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n\n\n\names &lt;- ames |&gt; mutate(Sale_Price = log10(Sale_Price))\n\n\n# ames |&gt; \n#   head(1) |&gt; \n#   glimpse()\n\n\n\n\nFollowing code create a linear model. Prediction uses these variables:\n\names |&gt; \n  select(Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type, Latitude, Longitude) |&gt; \n  slice_sample(n=1) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 6\n$ Neighborhood &lt;fct&gt; Sawyer\n$ Gr_Liv_Area  &lt;int&gt; 1059\n$ Year_Built   &lt;int&gt; 1966\n$ Bldg_Type    &lt;fct&gt; OneFam\n$ Latitude     &lt;dbl&gt; 42.03136\n$ Longitude    &lt;dbl&gt; -93.67561\n\n\nTher are their transformation:\n\nNeighborhood: convert low frequency one to “other”, then make dummy varible\nGr_Liv_Area: log10 treatment\nYear_Built: year\nBldg_Type: convert building type into dummy varible\nLatitude: spine function treatment\nLongitude: spine function treatment\n\nlibrary(tidymodels)\ndata(ames)\n\n## Normalise Prediction\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\n## Split Data Sets\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\n## Recipy for Preprocessing Data, Build receipy object\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n  \n## Linear Model\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n## Finaly Evaluate Lazy Object\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(ames_rec)\n\n## Fit a Model\nlm_fit &lt;- fit(lm_wflow, ames_train)"
  },
  {
    "objectID": "04-20 ML with Tidymodel/01.html#introduction-of-data",
    "href": "04-20 ML with Tidymodel/01.html#introduction-of-data",
    "title": "ML with Tidymodel",
    "section": "",
    "text": "Code\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(tidymodels)\ndata(\"ames\")\n\n## refer to tmap: https://r-tmap.github.io/tmap-book/visual-variables.html\n## for osm data: https://cran.r-project.org/web/packages/osmdata/vignettes/osmdata.html\n## for query streets: https://wiki.openstreetmap.org/wiki/Key%3ahighway\n\names_sf = sf::st_as_sf(ames,coords = c(\"Longitude\",\"Latitude\"), crs=4326)\names_bbox = sf::st_bbox(ames_sf)\nosm_streets = opq(bbox = ames_bbox) |&gt; \n  add_osm_feature(key=\"highway\",value = c(\n                                          'secondary'\n                                          ,'primary'\n                                          ,'tertiary'\n                                          ,'unclassified'\n                                          ,'residential')) |&gt; \n  # add_osm_feature(key=\"highway\",value = 'motorway') |&gt; \n  osmdata_sf()\n\n## view a intersection\nstreets_sf = sf::st_intersection(sf::st_as_sfc(ames_bbox), osm_streets$osm_lines)\n\ntm_shape(streets_sf) + \n  tm_lines(col='grey') +\ntm_shape(ames_sf) + \n  tm_dots( shape = \"Lot_Shape\"\n          ,col=\"Neighborhood\"\n          ,style = \"cont\"\n          ,size=0.05\n          ,border.col=NA\n          ,border.lwd=0.01) + \n  tm_layout(legend.show=FALSE)\n\n\n\n\n\n\n\n\n\names familar with this data may come handy compare different model output later.\n\n\n\n\nCode\nlibrary(tidymodels)\n# \nggplot2::theme_set(theme_minimal())\ntidymodels_prefer()\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")\n\n\n\n\n\n\n\n\n\nFirst thing they want to tell you is the data is not normal so require you to normalise somehow.\n\n\n\nCode\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n\n\n\names &lt;- ames |&gt; mutate(Sale_Price = log10(Sale_Price))\n\n\n# ames |&gt; \n#   head(1) |&gt; \n#   glimpse()"
  },
  {
    "objectID": "04-20 ML with Tidymodel/01.html#spoil-alert",
    "href": "04-20 ML with Tidymodel/01.html#spoil-alert",
    "title": "ML with Tidymodel",
    "section": "",
    "text": "Following code create a linear model. Prediction uses these variables:\n\names |&gt; \n  select(Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type, Latitude, Longitude) |&gt; \n  slice_sample(n=1) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 6\n$ Neighborhood &lt;fct&gt; Sawyer\n$ Gr_Liv_Area  &lt;int&gt; 1059\n$ Year_Built   &lt;int&gt; 1966\n$ Bldg_Type    &lt;fct&gt; OneFam\n$ Latitude     &lt;dbl&gt; 42.03136\n$ Longitude    &lt;dbl&gt; -93.67561\n\n\nTher are their transformation:\n\nNeighborhood: convert low frequency one to “other”, then make dummy varible\nGr_Liv_Area: log10 treatment\nYear_Built: year\nBldg_Type: convert building type into dummy varible\nLatitude: spine function treatment\nLongitude: spine function treatment\n\nlibrary(tidymodels)\ndata(ames)\n\n## Normalise Prediction\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\n## Split Data Sets\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\n## Recipy for Preprocessing Data, Build receipy object\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n  \n## Linear Model\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n## Finaly Evaluate Lazy Object\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(ames_rec)\n\n## Fit a Model\nlm_fit &lt;- fit(lm_wflow, ames_train)"
  },
  {
    "objectID": "04-20 ML with Tidymodel/01.html#splittingfeature-selectioncreate-a-data-budget",
    "href": "04-20 ML with Tidymodel/01.html#splittingfeature-selectioncreate-a-data-budget",
    "title": "ML with Tidymodel",
    "section": "Splitting/Feature Selection/Create a “Data Budget”",
    "text": "Splitting/Feature Selection/Create a “Data Budget”\n\nlibrary(tidymodels)\n\n\nSimple 80-20 split\nThe basics is the same, split train test. For this purpose you are splitting the data by 80-20.\n\names_split &lt;- rsample::initial_split(ames, prop = 0.80)\names_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2344/586/2930&gt;\n\n\nRegards to spliting portion here is the advice from the Book:\n\nA test set should be avoided only when the data are pathologically small.\n\n\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\ndim(ames_train)\n\n[1] 2344   74\n\n\n\n\nValidation Split 60-20-20\n\nset.seed(52)\n# To put 60% into training, 20% in validation, and 20% in testing:\names_val_split &lt;- rsample::initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;1758/586/586/2930&gt;\n\n\n\names_train &lt;- training(ames_val_split)\names_test &lt;- testing(ames_val_split)\names_val &lt;- validation(ames_val_split)\n\n\n\nConcepts\n\nindependent experimental unit: (knowing database basic this is just matter of object uid versus alternate uid) for example, measuring one patient\nmulti-level-data/multiple rows per experimental unit:\n\n\nData splitting should occur at the independent experimental unit level of the data!!!\n\n\nSimple resampling across rows would lead to some data within an experimental unit being in the training set and others in the test set.\n\n\n\nPracrtical Implication\n\nthe book admit the practice of train and split at first for a validation of the model but follow up using all the data point possible for a better estimation of data."
  },
  {
    "objectID": "04-20 ML with Tidymodel/01.html#fitting-model-with-parsnip",
    "href": "04-20 ML with Tidymodel/01.html#fitting-model-with-parsnip",
    "title": "ML with Tidymodel",
    "section": "Fitting Model with Parsnip",
    "text": "Fitting Model with Parsnip\n\nlinear_reg\nrand_forest\n\n\nLinear Regression Family\n\nlm\nglmnet: fits generalised linear and model via penalized maximum likelihood.\nstan\n\n\n# switch computational backend for different model\nlinear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n#  regularized regression is the glmnet model \nlinear_reg(penalty=1) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    family = \"gaussian\")\n\n# To estimate with regularization, the second case, a Bayesian model can be fit using the rstanarm package:\nlinear_reg() |&gt; \n  set_engine(\"stan\") |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: stan \n\nModel fit template:\nrstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), \n    weights = missing_arg(), family = stats::gaussian, refresh = 0)\n\n\n\nlm_model = linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  translate()\n\nlm_model |&gt; \n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -313.623       -2.074        2.965  \n\n\n\nlm_xy_fit &lt;- \n  lm_model %&gt;% \n  fit_xy(\n    x = ames_train %&gt;% select(Longitude, Latitude),\n    y = ames_train %&gt;% pull(Sale_Price)\n  )\n\nlm_xy_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -313.623       -2.074        2.965  \n\n\n\n\nTree Model\n\nrand_forest(trees = 1000, min_n = 5) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\") %&gt;% \n  translate()\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 5\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    num.trees = 1000, min.node.size = min_rows(~5, x), num.threads = 1, \n    verbose = FALSE, seed = sample.int(10^5, 1))"
  },
  {
    "objectID": "04-20 ML with Tidymodel/01.html#capture-model-results",
    "href": "04-20 ML with Tidymodel/01.html#capture-model-results",
    "title": "ML with Tidymodel",
    "section": "Capture Model Results",
    "text": "Capture Model Results\n\nRaw original way (useful to check og documentation)\n\nlm_form_fit &lt;- \n  lm_model %&gt;% \n  # Recall that Sale_Price has been pre-logged\n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nlm_form_fit %&gt;% extract_fit_engine() %&gt;% vcov()\n\n            (Intercept)    Longitude     Latitude\n(Intercept)  273.852441  2.052444651 -1.942540743\nLongitude      2.052445  0.021122353 -0.001771692\nLatitude      -1.942541 -0.001771692  0.042265807\n\n\n\nmodel_res &lt;- \n  lm_form_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  summary()\n\n# The model coefficient table is accessible via the `coef` method.\nparam_est &lt;- coef(model_res)\nclass(param_est)\n\n[1] \"matrix\" \"array\" \n\n\n\nparam_est\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -313.622655 16.5484876 -18.95174 5.089063e-73\nLongitude     -2.073783  0.1453353 -14.26896 8.697331e-44\nLatitude       2.965370  0.2055865  14.42395 1.177304e-44\n\n\n\n\nThe Tidy ecosystem for model result\nWhat’s good about tidy is so you can reuse model.\n\ntidy(lm_form_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -314.      16.5       -19.0 5.09e-73\n2 Longitude      -2.07     0.145     -14.3 8.70e-44\n3 Latitude        2.97     0.206      14.4 1.18e-44"
  },
  {
    "objectID": "04-20 ML with Tidymodel/01.html#evaluate-test-set-use-last_fit-method",
    "href": "04-20 ML with Tidymodel/01.html#evaluate-test-set-use-last_fit-method",
    "title": "ML with Tidymodel",
    "section": "Evaluate Test Set use `last_fit()` method",
    "text": "Evaluate Test Set use `last_fit()` method\n\nfinal_lm_res &lt;- last_fit(lm_wflow, ames_split)\nfinal_lm_res\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [2342/588]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\n\n…the modeling process encompasses more than just estimating the parameters of an algorithm that connects predictors to an outcome. This process also includes pre-processing steps and operations taken after a model is fit. We introduced a concept called a model workflow that can capture the important components of the modeling process. Multiple workflows can also be created inside of a workflow set. The last_fit() function is convenient for fitting a final model to the training set and evaluating with the test set.\nFor the Ames data, the related code that we’ll see used again is:\n\n\nlibrary(tidymodels)\ndata(ames)\n\n## normalise y\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\n## split data\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\n## linear models\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n## validating result\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\n\nlm_fit &lt;- fit(lm_wflow, ames_train)"
  },
  {
    "objectID": "04-20 ML with Tidymodel/01.html#yardstick-basic-usage",
    "href": "04-20 ML with Tidymodel/01.html#yardstick-basic-usage",
    "title": "ML with Tidymodel",
    "section": "Yardstick Basic Usage",
    "text": "Yardstick Basic Usage\nYardstick is tool that produce performance matrix with consistent interface.\n## Create a prediction frame\names_test_res &lt;- predict(lm_fit, new_data = ames_test %&gt;% select(-Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 1\n#&gt;   .pred\n#&gt;   &lt;dbl&gt;\n#&gt; 1  5.07\n#&gt; 2  5.31\n\n\n## Bind prediction with Actual value\names_test_res &lt;- bind_cols(ames_test_res, ames_test %&gt;% select(Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 2\n#&gt;   .pred Sale_Price\n#&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1  5.07       5.02\n#&gt; 2  5.31       5.39\n\n## YARDSTICK!! given a dataframe just do these two\nrmse(ames_test_res, truth = Sale_Price, estimate = .pred)\n\n\n## compare multiple matrix\names_metrics &lt;- metric_set(rmse, rsq, mae)\names_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)"
  },
  {
    "objectID": "03-03 Research England Interview Prep/10.html",
    "href": "03-03 Research England Interview Prep/10.html",
    "title": "Experiment of Zipf’s Law",
    "section": "",
    "text": "Zipf’s Law is statistical distribution of word counts in human languages.\n\n\n\nrequire(janeaustenr)\nrequire(tidytext)\nrequire(tidyverse)\n\nnatural = austen_books() |&gt; \n  filter(book == \"Sense & Sensibility\") |&gt; \n  unnest_tokens(word,text) |&gt; \n  count(book,word,sort=T) |&gt; \n  \n  ungroup() |&gt; \n  group_by(book) |&gt; \n  mutate(total = sum(n)) |&gt;\n  ungroup() |&gt; \n  \n  mutate(freq=n/total, rank=row_number()) |&gt; \n  mutate(freq, rank, type=paste(\"natural - \", book)) |&gt; \n  select(freq, rank,type)\n  \n\ngenerate_freq_rank = function( FUN=runif, n = max(natural$rank)) {\n  x = FUN(n)\n  x_nrom = x/sum(x)\n  freq = sort(x_nrom,decreasing=T)\n  rank = seq(length(freq))\n  return(data.frame(freq=freq,rank=rank))\n}\n\n\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Normal Scale\",\n          \"Rank to Frequency of Terms\"\n          )\n\n\n\n\n\n\n\n\n\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Log10 Scale\",\n          \"Rank to Frequency of Terms\")\n\nWarning in transformation$transform(x): NaNs produced\n\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 3244 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\")\n  # cbind(generate_freq_rank(runif), type=\"runif\"),\n  # cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Natual Language Is Mostly Close to Expential but much steaper than Exponetial\")"
  },
  {
    "objectID": "03-03 Research England Interview Prep/10.html#word-frequency-experiment---zipfs-law",
    "href": "03-03 Research England Interview Prep/10.html#word-frequency-experiment---zipfs-law",
    "title": "Experiment of Zipf’s Law",
    "section": "",
    "text": "require(janeaustenr)\nrequire(tidytext)\nrequire(tidyverse)\n\nnatural = austen_books() |&gt; \n  filter(book == \"Sense & Sensibility\") |&gt; \n  unnest_tokens(word,text) |&gt; \n  count(book,word,sort=T) |&gt; \n  \n  ungroup() |&gt; \n  group_by(book) |&gt; \n  mutate(total = sum(n)) |&gt;\n  ungroup() |&gt; \n  \n  mutate(freq=n/total, rank=row_number()) |&gt; \n  mutate(freq, rank, type=paste(\"natural - \", book)) |&gt; \n  select(freq, rank,type)\n  \n\ngenerate_freq_rank = function( FUN=runif, n = max(natural$rank)) {\n  x = FUN(n)\n  x_nrom = x/sum(x)\n  freq = sort(x_nrom,decreasing=T)\n  rank = seq(length(freq))\n  return(data.frame(freq=freq,rank=rank))\n}\n\n\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Normal Scale\",\n          \"Rank to Frequency of Terms\"\n          )\n\n\n\n\n\n\n\n\n\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\"),\n  cbind(generate_freq_rank(runif), type=\"runif\"),\n  cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Compare Normal, Exponential, Univaritive Sample - At Log10 Scale\",\n          \"Rank to Frequency of Terms\")\n\nWarning in transformation$transform(x): NaNs produced\n\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 3244 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\nrbind(\n  natural,\n  cbind(generate_freq_rank(rexp), type=\"rexp\")\n  # cbind(generate_freq_rank(runif), type=\"runif\"),\n  # cbind(generate_freq_rank(rnorm),type=\"normal\")\n  ) |&gt; \n  ggplot(aes(x=rank,y=freq,color=type)) + \n  geom_line() + \n  scale_x_log10() +\n  scale_y_log10() +\n  theme_minimal() +\n  ggtitle(\"Natual Language Is Mostly Close to Expential but much steaper than Exponetial\")"
  },
  {
    "objectID": "03-03 Research England Interview Prep/04.html",
    "href": "03-03 Research England Interview Prep/04.html",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(igraph)\nlibrary(tidytext)\ngtr_desc = read_parquet(\"data/gtr.parquet\") |&gt; \n  select(id, abstractText)\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\"))) |&gt; \n  rename(id=ProjectId)\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  dplyr::relocate(ProjectReference, FundingOrgName, LeadROName, id, \n           is_partial,project_ref,part,occurance)\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is NOT Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain NA!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n# gtr_pj |&gt;\n#   group_by(occurance) |&gt; \n#   summarise(n_projects=n())\n\n## find out about \nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, id) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  ungroup()\n\n## find out with text other than continuous project are repeated\nrepeated_text = gtr_desc |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"id\") |&gt; \n  mutate(year = lubridate::year(StartDate)) |&gt;\n  inner_join(gtr_desc, by=\"id\")"
  },
  {
    "objectID": "03-03 Research England Interview Prep/04.html#explore-n-gram",
    "href": "03-03 Research England Interview Prep/04.html#explore-n-gram",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "explore n-gram",
    "text": "explore n-gram\n\n## break into bi-grame\nabstract_words = analysis_prj |&gt;\n  unnest_tokens(word, abstractText, \"ngrams\",n=2, drop=T) |&gt; \n  count(word,id, sort=T)\n\n## try this `bind_tf_idf` function\nword_distinct = abstract_words |&gt; \n  bind_tf_idf(word,id, n)\n\n## convert bi-graph into network graph\npharases = word_distinct |&gt; \n  arrange(desc(tf_idf)) |&gt; \n  tidyr::separate(word, into=c(\"word1\",\"word2\"), sep=\" \") |&gt; \n  anti_join(stop_words,c(\"word1\"=\"word\")) |&gt; \n  anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n  filter(if_all(c(word1,word2), ~!stringr::str_detect(.x, \"\\\\d+\"))) |&gt; \n  mutate(pharase = paste(word1, word2)) |&gt; \n  filter(!word1 |&gt; str_detect(\"^_\"))\n\nrequire(tidygraph)\n\n## try find graphical centroid of biggest graph\ncatch_pharase = pharases |&gt; \n  group_by(word1, word2) |&gt; \n  summarise(\n    occurance = n()\n  ) |&gt; arrange(-occurance)\n\n`summarise()` has grouped output by 'word1'. You can override using the\n`.groups` argument.\n\ncatch_pharase\n\n# A tibble: 215,740 × 3\n# Groups:   word1 [24,727]\n   word1        word2        occurance\n   &lt;chr&gt;        &lt;chr&gt;            &lt;int&gt;\n 1 artificial   intelligence      1813\n 2 machine      learning           809\n 3 intelligence ai                 400\n 4 real         time               392\n 5 project      aims               322\n 6 deep         learning           249\n 7 real         world              246\n 8 neural       networks           225\n 9 cutting      edge               220\n10 wide         range              217\n# ℹ 215,730 more rows\n\n\nbi-graph truns out to be very useful…. you gets to understand\nEigen Centrality Run every node through to\n\nword_graph = catch_pharase |&gt; \n  as_tbl_graph() |&gt; \n  morph(to_components) |&gt; \n  as_tibble() |&gt; \n  mutate(dim = map_int(graph, ~length(.x))) |&gt; \n  arrange(desc(dim))\n\nbiggest_g=word_graph |&gt; purrr::pluck(\"graph\",1)\n\n## eigen centrality\n\ncentroid_score = biggest_g |&gt; eigen_centrality() |&gt; pluck(\"vector\")\ncentroid = which(centroid_score==max(centroid_score))\n\nbiggest_g |&gt; \n  convert(to_local_neighborhood, centroid, 1) |&gt;  \n  mutate(score = centroid_score[name]) |&gt;\n  convert(to_subgraph, score &gt; quantile(score, 0.99)) |&gt; \n  activate(edges) |&gt; \n  arrange(occurance) |&gt; \n  filter(occurance &gt; 10) |&gt; \n  ggraph(layout=\"gem\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),color=\"cyan4\",trans=\"log\") +\n  geom_node_point(aes(size=score),color='black',trans=\"sqrt\",alpha=0.7) +\n  # geom_node_point(aes( alpha=score) ) +\n  geom_node_text(aes(label = name, alpha=score),repel=T) + \n  scale_edge_alpha(trans=\"sqrt\") +\n  ggtitle(\"By Eigenvector Centrality\") +\n  theme_void()\n\nSubsetting by nodes\n\n\nWarning in geom_edge_link(aes(alpha = occurance, width = occurance), color =\n\"cyan4\", : Ignoring unknown parameters: `trans`\n\n\nWarning in geom_node_point(aes(size = score), color = \"black\", trans = \"sqrt\",\n: Ignoring unknown parameters: `trans`\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\nAnygraphical based algorithmn is interesting here.\n\n## harmonic_centrality\ncache_file = \"cache/04-betweeness_score.RDS\"\nif(interactive()) {\n  betweeness_score = biggest_g |&gt; \n  activate(edges) |&gt;\n  betweenness(weights=E(biggest_g)$\"occurance\")\n  saveRDS(centroid_score, cache_file)\n} else {\n  betweeness_score=readRDS(cache_file)\n}\nbtw_centre = which(centroid_score==max(centroid_score))\n\nbiggest_g |&gt; \n  # convert(to_local_neighborhood, btw_centre) |&gt; \n  mutate(score = betweeness_score[name]) |&gt; \n  convert(to_subgraph, score &gt; quantile(score, 0.95)) |&gt; \n  arrange(desc(score)) |&gt; \n  filter(row_number() &lt; 50) |&gt; \n  activate(edges) |&gt; \n  filter(occurance &gt; 100) |&gt; \n  ggraph(\"kk\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),color=\"cyan4\") +\n  geom_node_point(aes(size=score),color='black',trans=\"sqrt\",alpha=0.7) +\n  # geom_node_point(aes( alpha=score) ) +\n  geom_node_text(aes(label = name, alpha=score),repel=T) + \n  scale_edge_alpha(trans=\"sqrt\") +\n  theme_void() +\n  ggtitle(\"By Edge Betweeness, visualise top 0.01 % \")\n\nSubsetting by nodes\n\n\nWarning in geom_node_point(aes(size = score), color = \"black\", trans = \"sqrt\",\n: Ignoring unknown parameters: `trans`\n\n\n\n\n\n\n\n\n\nThis results actually makes sense if you are looking at quot is a wild card that can get about a lot of things.\nUp to interpretation.\nOkay other then key terms. Seems like the edge betweeness is good for point out adjusant words rather than identify patterns."
  },
  {
    "objectID": "03-03 Research England Interview Prep/04.html#suggestions",
    "href": "03-03 Research England Interview Prep/04.html#suggestions",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "Suggestions",
    "text": "Suggestions\nIt maybe usefull to use n-gram to extract combined terms from title and bind this back into abstract n-graph. Then when we look at graph centrality again we remove any waild card.\nPharases may high high frequency of occurance but this does not means that they have a higher “graphic degree”."
  },
  {
    "objectID": "03-03 Research England Interview Prep/04.html#follow-up",
    "href": "03-03 Research England Interview Prep/04.html#follow-up",
    "title": "Text Analysis - Use N-Gram Graph",
    "section": "Follow up",
    "text": "Follow up\nThe better way is actually filter a few top occuring terms.\n\ncatch_pharase |&gt; \n  filter(occurance &gt; 100) |&gt; \n  as_tbl_graph() |&gt; \n  ggraph(\"kk\") +\n  geom_edge_link(aes(alpha=occurance,width=occurance),trans=\"log\",edge_colour =\"cyan4\") +\n  geom_node_text(aes(label=name),repel = T) + \n  geom_node_point(size=5) +\n  theme_void()\n\nWarning in geom_edge_link(aes(alpha = occurance, width = occurance), trans =\n\"log\", : Ignoring unknown parameters: `trans`"
  },
  {
    "objectID": "03-03 Research England Interview Prep/02.html",
    "href": "03-03 Research England Interview Prep/02.html",
    "title": "Trend and Elite Project",
    "section": "",
    "text": "require(readr)\nrequire(arrow)\nrequire(tidyverse)\nrequire(lubridate)\nrequire(tidyr)\nrequire(scales)\nrequire(plotly)\nrequire(ggtext)\n\nsource(\"set_graphic.R\")\ncsv_export=read_csv('data/projectsearch-1709481069771.csv')\napi_export=arrow::read_parquet('data/gtr.parquet')\n## clean parse date\ngtr = csv_export |&gt; \n  mutate(across(c(StartDate, EndDate), ~as.Date(.x, '%d/%m/%Y')))\n\n\nExplore Trend\n\nAbsolute NumberAward\n\n\n\ngtr |&gt; plot_freq(n()) + \n  ylab(\"Number of Project\")\n\n\n\n\n\n\n\n\n\n\n\ngtr |&gt; \n  plot_freq(sum(AwardPounds,na.rm=T)) + \n  ylab(\"Total Award Sumed\") +\n  labs(caption=\"This plot may be useful if funding were allocated all at the start of the year\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore Elite (Most Expensive) Projects\n\nrequire(htmlwidgets)\n\nLoading required package: htmlwidgets\n\n## parameter\nSNOBYNESS=0.02 ## this is top n most expensive\nPRICE_FORMULAR=quo(\n  AwardPounds/as.numeric(duration, units='days') ## this is how you calculate expensiveness\n) \n\n## calculate award per year \nprice_gtr = gtr |&gt; \n  mutate(duration = EndDate - StartDate) |&gt;\n  mutate(duration_d = as.numeric(duration, units='days')) |&gt; \n  mutate(price = !! PRICE_FORMULAR) |&gt; \n  arrange(desc(price)) |&gt; \n  mutate(expensive_rank = row_number()) |&gt;\n  filter(expensive_rank &lt;= quantile(expensive_rank, SNOBYNESS))\n\ng &lt;- price_gtr |&gt;\n  ggplot(aes(y = price, label = LeadROName, label1= Title, label2 = ProjectReference)) + \n  geom_linerange(aes(xmin = StartDate, xmax = EndDate, y = price, alpha = duration_d), \n                 color = \"darkgrey\") +\n  geom_point(aes(x = StartDate), color = \"blue\") + \n  geom_point(aes(x = EndDate), color = \"navy\") +\n  ggtitle(glue::glue(\"Here are the top {SNOBYNESS * 100}% most expensive AI projects\")) +\n  scale_alpha_continuous(trans='log')\n\nggplotly(g) |&gt;\n  style(traces = 2, text = paste(price_gtr$StartDate,\"\\n\",price_gtr$duration, \"days\")) |&gt;\n  style(traces = 1, text = paste( price_gtr$PIFirstName, price_gtr$PISurname,\"\\n\",price_gtr$duration, \"days\")) |&gt;\n  style(traces = 3, customdata = price_gtr$GTRProjectUrl, text = paste(price_gtr$EndDate,\"\\n\", price_gtr$LeadROName, \"\\n\",price_gtr$Title)) |&gt;\n  onRender(\"\n    function(el) { \n      el.on('plotly_click', function(d) { \n        var url = d.points[0].customdata;\n        window.open(url);\n      });\n    }\n  \") #set up click event that open URL"
  },
  {
    "objectID": "03-03 Research England Interview Prep/01.html",
    "href": "03-03 Research England Interview Prep/01.html",
    "title": "Getting Data",
    "section": "",
    "text": "Overview\ncode in this notebook are written in python\nThere are two data source:\n\ncsv file downloaded directly from GTR website;\nFor project description, from GTR api;\nAdditional coordinate data using open-api;\n\nAlthough from step1 and step2 we are able to get same number of data, only 86.05% can join.\n\nimport requests\nimport pandas as pd\nfrom IPython.display import display\n\n## define a function to check interactivity\n## so when render this document code take too long don't have to re-run everytime \nimport os\ndef is_interactive():\n   return 'SHLVL' not in os.environ\nprint('Interactive?', is_interactive())\n\n## Parameter\nSEARCH_TERM=\"artificial intelligence\"\nMAX_RESULTS=5175 # gathered by direct search \nURL = \"https://gtr.ukri.org/gtr/api/projects.json?sf=pro.sd&so=A\" # sort by project start date Decending (D) Acending(A)\n\nprint(f'Gathering data related to term \"{SEARCH_TERM}\", this has {MAX_RESULTS} results')\n\nInteractive? False\nGathering data related to term \"artificial intelligence\", this has 5175 results\n\n\n\n\nFrom Download\n\n\n\nSearch term “aritificial intelligence” has option to download csv\n\n\n\n\nexample of the donwload\ncsv_export = pd.read_csv('data/projectsearch-1709481069771.csv')\ndisplay(csv_export.sample(3))\n\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n5003\nNERC\nNE/M02086X/1\nUniversity of Essex\nLife Sciences\nResearch Grant\nDumbrell\nAlexander\nNaN\nNaN\nNaN\n...\n31/12/2020\n764323.0\nNaN\nEast of England\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n74C6D687-B820-4A93-8944-E31C3F5CB77E\n8A03ED41-E67D-4F4A-B5DD-AAFB272B6471\n09E8DBA6-A48F-43C3-AB86-C73A2666B10E\n0AD50B14-EDCD-433F-AD3D-A79FB21243A1\n\n\n856\nNERC\nNE/J006629/1\nNERC CEH (Up to 30.11.2019)\nBillett\nResearch Grant\nBillett\nMichael\nNaN\nNaN\nNaN\n...\n11/08/2013\n133273.0\nNaN\nSouth East\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n99E0891B-9CDA-46D5-9735-37D96864DAC8\n8A03ED41-E67D-4F4A-B5DD-AAFB272B6471\n302DE0C3-6809-4EB5-BB59-F89E6BE3EDD9\n669726BC-B188-46AC-9E5B-DF2D348302BD\n\n\n2518\nMRC\nMR/V01157X/1\nUniversity of Glasgow\nMRC Centre for Virus Research\nResearch Grant\nWilson\nSam\nNaN\nNaN\nNaN\n...\n31/08/2023\n612023.0\nNaN\nScotland\nClosed\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n305F8763-9D52-4ECF-83D1-829C17A50287\nC008C651-F5B0-4859-A334-5F574AB6B57C\n595A5FEA-6A63-4445-BD0B-7CA0CC2EE7DF\nDF41B605-8649-4D45-B6E3-592FB3EB3774\n\n\n\n\n3 rows × 25 columns\n\n\n\n\nUnfortunately this csv export don’t contain project description. So we are manually extract this data using the API call.\n\n\nFrom API\n\n# GTR end-point\nif is_interactive():\n  print(\"Interactive context, fetch live data\")\n  ## Initialize a Data Frame\n  df = pd.DataFrame(columns = ['id', \"title\", \"abstractText\", \"techAbstractText\",\"potentialImpact\"])\n  i = int()\n  for i in range(1, MAX_RESULTS // 25 + 1):\n      parameters = {\n          \"q\":SEARCH_TERM,\n          \"s\":25,\n          \"p\":i\n          # \"f\":\"pro.rt\" # search in project topic \n      }\n      \n      response = requests.get(URL, params = parameters)\n      x = response.json()\n      dfx = pd.DataFrame.from_dict(x['project'])[[\"id\",\"title\",\"abstractText\",\"techAbstractText\",'potentialImpact']]\n      df = pd.concat([df,dfx])\n      \n      ## Save data.frame into a parquet \n      df.to_parquet('data/gtr.parquet')\n      api_export = df\nelse:\n  print(\"Skipping intractive\")\n  api_export = pd.read_parquet(\"data/gtr.parquet\")\n\nSkipping intractive\n\n\n\ndisplay(api_export.sample(3))\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n21\nC7866802-704C-4B14-BAB4-B7B97B55C2E5\nMachine Learning for Resource Management in Ne...\nRecent developments in optical networking tech...\nNone\nNone\n\n\n14\n6E70ACBD-7E36-4999-8514-78619CF8E0C1\nA novel circadian output circuit linking senso...\nSleep is essential for normal nervous system f...\nHere we propose experiments to delineate and f...\nSleep is a critical and highly conserved behav...\n\n\n17\n190B610E-C3EB-440A-BF86-96021C64E4C1\nCiViL: Common-sense- and Visually-enhanced nat...\nOne of the most compelling problems in Artific...\nNone\nAutonomous and intelligent systems are becomin...\n\n\n\n\n\n\n\n\n\n\nData Quality\n\n\nBasic Validations\n## check api export\nassert api_export.shape[0] == api_export.id.unique().shape[0], \"Check `field id` is unique\"\n\n## check csv export\nassert csv_export.shape[0] == csv_export.ProjectId.unique().shape[0], \"Check `field id` ProjectID is unique\"\n\n## check relationships\n## check if csv and api call returned same number of rows\nassert api_export.shape[0] - csv_export.shape[0] == 0, \"API and manual search result return the same rows\"\n\n\n\n\njoin key check\nfrom numpy import intersect1d, setdiff1d\n\napi_id = api_export.id.values\ncsv_id = csv_export.ProjectId.values\n\ncommon = intersect1d(api_id,csv_id,assume_unique=True)\napi_extra = setdiff1d(api_id,csv_id,assume_unique=True)\ncsv_extra = setdiff1d(csv_id, api_id,assume_unique=True)\n\nprint(f'''\nWe can join {((common.shape[0] / csv_export.shape[0]) * 100):.02f} % of data\n\nDetails here:\n- {common.shape[0]} can join;\n- {api_extra.shape[0]} extra projects from api;\n- {csv_extra.shape[0]} extra projects from csv download;\n''')\n\n\n\nWe can join 86.05 % of data\n\nDetails here:\n- 4453 can join;\n- 722 extra projects from api;\n- 722 extra projects from csv download;\n\n\n\n\n\nCode\n## Example of extra API export\ndisplay(api_export.query(\"id.isin(@api_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\n\nid\ntitle\nabstractText\ntechAbstractText\npotentialImpact\n\n\n\n\n14\n73C0F6C2-3D17-49DE-856E-0959477F53D2\nTowards More Effective Multi-objective Meta-He...\nThis research project proposes the investigati...\nNone\nNone\n\n\n16\nCCF621C5-1FF1-4601-B873-F5E740369EC7\nTesting Autonomous Vehicle Software using Situ...\nAutonomous vehicles (AVs) must be controlled b...\nNone\nThe short-term result of the project will be a...\n\n\n17\nE3B3EE92-CD36-404F-8A9E-FF63FE58427D\nThales-Bristol Partnership in Hybrid Autonomou...\nHybrid autonomous systems are those where grou...\nNone\nThe business benefits of T-B PHASE are:\\n\\n- L...\n\n\n\n\n\n\n\n\n\n\nCode\n## Example of extra CSV export\ndisplay(csv_export.query(\"ProjectId.isin(@csv_extra)\").sample(3))\n\n\n\n\n\n\n\n\n\n\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nPISurname\nPIFirstName\nPIOtherNames\nPI ORCID iD\nStudentSurname\n...\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nGTRProjectUrl\nProjectId\nFundingOrgId\nLeadROId\nPIId\n\n\n\n\n144\nHorizon Europe Guarantee\n10078198\nSHEFFIELD HALLAM UNIVERSITY\nNaN\nEU-Funded\nAkhgar\nBabak\nNaN\nNaN\nNaN\n...\n31/10/2026\n486568.0\nNaN\nYorkshire and The Humber\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n5F03F3D3-21F9-4D72-873A-7E02BA9FC346\n240CEBFD-1052-4EAC-88DF-D88A163D61C8\n5F8915AC-8108-45EC-817D-E0123EFD9E9E\n877B75B5-46D0-401D-8905-8AE2DE5F1251\n\n\n3149\nHorizon Europe Guarantee\n10065388\nTHRIDIUM LIMITED\nNaN\nEU-Funded\nBunduc\nOana\nNaN\nNaN\nNaN\n...\n31/12/2025\n259403.0\nNaN\nLondon\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\n0FDB0DDA-A546-4E42-840F-3A264B3AF1F9\n240CEBFD-1052-4EAC-88DF-D88A163D61C8\n19EFA79F-49E9-4095-B570-E7BBF27541AD\nBCDED8CE-1A0E-4828-9E7A-1A91E20A818B\n\n\n2381\nBBSRC\nBB/X013243/1\nUniversity of Bristol\nComputer Science\nResearch Grant\nFroudist-Walsh\nSean\nNaN\nhttp://orcid.org/0000-0003-4070-067X\nNaN\n...\n08/08/2024\n199286.0\nNaN\nSouth West\nActive\nhttp://internal-gtr-tomcat-alb-611010599.eu-we...\nE1A19CCC-A2F2-4860-ABC6-6B7827E66503\n2512EF1C-401B-4222-9869-A770D4C5FAC7\n25335D80-2025-4154-989F-37646A6EFBE1\nA1FAB163-268D-4F17-B163-FA7274479238\n\n\n\n\n3 rows × 25 columns"
  },
  {
    "objectID": "03-03 Research England Interview Prep/03.html",
    "href": "03-03 Research England Interview Prep/03.html",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "",
    "text": "require(tidyverse)\nrequire(readr)\nrequire(tidyr)\nrequire(arrow)\nrequire(tidytext)\nrequire(ggplot2)\nrequire(ggrepel)\nrequire(gghighlight)\n\ngtr_desc = read_parquet(\"data/gtr.parquet\")\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\")))"
  },
  {
    "objectID": "03-03 Research England Interview Prep/03.html#overview",
    "href": "03-03 Research England Interview Prep/03.html#overview",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "",
    "text": "require(tidyverse)\nrequire(readr)\nrequire(tidyr)\nrequire(arrow)\nrequire(tidytext)\nrequire(ggplot2)\nrequire(ggrepel)\nrequire(gghighlight)\n\ngtr_desc = read_parquet(\"data/gtr.parquet\")\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\")))"
  },
  {
    "objectID": "03-03 Research England Interview Prep/03.html#cleaning",
    "href": "03-03 Research England Interview Prep/03.html#cleaning",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "Cleaning",
    "text": "Cleaning\noverview\n\nMajority of data and link can join\nDescription and Title contains duplication:\n\nThis is due to project migration when person of interests changes occupation;\nWhen this happens the ProjectReference will ends with a slash.\n\n\n\nHow well does the two data set join\n\nJOIN_META_KEY = c(\"id\"=\"ProjectId\")\n\ndesc_key = gtr_desc[[names(JOIN_META_KEY)]]\nmeta_key = gtr_meta[[JOIN_META_KEY[[1]]]]\n\nstopifnot(\n  !any(duplicated(desc_key)),\n  !any(duplicated(meta_key))\n)\ncan_join = intersect(desc_key, meta_key)\ndesc_cant_join = setdiff(desc_key, meta_key)\nmeta_cant_join= setdiff(meta_key, desc_key)\n\npcg = round(length(can_join) / nrow(gtr_desc) * 100)\n\nmessage(glue::glue(\n  \"{ pcg } % of description can find matching porject id in the csv export;\\n\",\n  \"\\nNumbers breakdown:\\n\",\n  \"\\t - {length(can_join)} can join;\\n\",\n  \"\\t - {length(desc_cant_join)} description will be taken out;\\n\",\n  \"\\t - {length(meta_cant_join)} from csv file will be taken out;\\n\"\n))\n\n86 % of description can find matching porject id in the csv export;\n\nNumbers breakdown:\n     - 4453 can join;\n     - 722 description will be taken out;\n     - 722 from csv file will be taken out;\n\n\n\n\nLinked/Duplicated project\n\n## migrated project consist of this pattern\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  relocate(ProjectReference, FundingOrgName, LeadROName, ProjectId, \n           is_partial,project_ref,part,occurance\n           )\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain null!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n## a lot of these are false duplicate? or have they simply not been included in \n## the project? \ngtr_pj |&gt;\n  group_by(occurance) |&gt; \n  summarise(n_projects=n())\n\n# A tibble: 3 × 2\n  occurance n_projects\n      &lt;int&gt;      &lt;int&gt;\n1         1       4984\n2         2        182\n3         3          9\n\n## actually majority of these project will false alert\n\n\nsmp=sample(1:94,1)# 9/3 + 183/2 \ngtr_pj |&gt; \n  filter(occurance !=1) |&gt; \n  group_by(project_ref) |&gt;\n  filter(cur_group_id() == smp) |&gt; \n  left_join(select(gtr_desc, id,abstractText,title), by = c(\"ProjectId\"=\"id\"))\n\n# A tibble: 2 × 31\n# Groups:   project_ref [1]\n  ProjectReference FundingOrgName LeadROName    ProjectId is_partial project_ref\n  &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;     &lt;lgl&gt;      &lt;chr&gt;      \n1 EP/V008242/2     EPSRC          University o… 510EB79C… TRUE       EP/V008242 \n2 EP/V008242/1     EPSRC          University o… 2A690B5E… TRUE       EP/V008242 \n# ℹ 25 more variables: part &lt;dbl&gt;, occurance &lt;int&gt;, Department &lt;chr&gt;,\n#   ProjectCategory &lt;chr&gt;, PISurname &lt;chr&gt;, PIFirstName &lt;chr&gt;,\n#   PIOtherNames &lt;lgl&gt;, `PI ORCID iD` &lt;chr&gt;, StudentSurname &lt;chr&gt;,\n#   StudentFirstName &lt;chr&gt;, StudentOtherNames &lt;lgl&gt;, `Student ORCID iD` &lt;chr&gt;,\n#   Title &lt;chr&gt;, StartDate &lt;date&gt;, EndDate &lt;date&gt;, AwardPounds &lt;dbl&gt;,\n#   ExpenditurePounds &lt;dbl&gt;, Region &lt;chr&gt;, Status &lt;chr&gt;, GTRProjectUrl &lt;chr&gt;,\n#   FundingOrgId &lt;chr&gt;, LeadROId &lt;chr&gt;, PIId &lt;chr&gt;, abstractText &lt;chr&gt;, …\n\n\nAlthough the project will have different id. The content is actually the same. So it is important these are taken out before input for analysis.\nFor the analysis, we only need to take the first project which end with /1\nWe will need to know which rows to keep which rows to delete\n\nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, ProjectId) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  inner_join(select(gtr_desc, id,abstractText,title), by = c(\"ProjectId\"=\"id\")) |&gt; \n  ungroup()\n\n\nrepeated_text = unique_prj |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\nrepeated_text |&gt; \n  select(ProjectReference,title,abstractText, ProjectId)\n\n# A tibble: 316 × 4\n# Groups:   abstractText [110]\n   ProjectReference title                                 abstractText ProjectId\n   &lt;chr&gt;            &lt;chr&gt;                                 &lt;chr&gt;        &lt;chr&gt;    \n 1 27744            Enhancing Type 2 Diabetes treatment … \"**NEED:** … 6A378D2F…\n 2 10025411         Research and Development support for… \"**NEED:** … B9B20477…\n 3 ST/G003467/1     High speed imaging with diamond dyno… \"1. The pur… 382C54EA…\n 4 ST/G003475/1     High speed imaging with diamond dyno… \"1. The pur… BBD58755…\n 5 NE/V012908/1     Dry deposition processes of volatile… \"A large ra… BD2B6410…\n 6 NE/V01272X/1     Dry deposition processes of volatile… \"A large ra… B7C88680…\n 7 EP/F047770/1     Carbon Dioxide and Alkanes as Electr… \"A major so… 041635B4…\n 8 EP/F047789/1     Carbon Dioxide and Alkanes as Electr… \"A major so… 7FF78CAF…\n 9 EP/F047878/1     Carbon Dioxide and Alkanes as Electr… \"A major so… C595D2BE…\n10 EP/F04772X/1     Carbon Dioxide and Alkanes as Electr… \"A major so… 60F0EBC9…\n# ℹ 306 more rows\n\n\nEven with project dropped, most of these still have little descriptions"
  },
  {
    "objectID": "03-03 Research England Interview Prep/03.html#word-count-and-keyword-summary",
    "href": "03-03 Research England Interview Prep/03.html#word-count-and-keyword-summary",
    "title": "Basic Text Ananlysis - Word Occurance",
    "section": "Word Count and Keyword Summary",
    "text": "Word Count and Keyword Summary\n\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"ProjectId\") |&gt; \n  mutate(year = lubridate::year(StartDate))\n\n\n## function for tokenise target fiel\ntokenize_words_group = function(df, col, group_col) {\n  df |&gt;\n    # preserve chained keywords\n    mutate(text_field = str_replace({{col}}, \n                                    '(A|a)rtificial (I|i)ntelligence', \n                                    'artificialintelligence') |&gt; \n             str_replace(\"machine learning\",\n                         'machinelearning')\n           ) |&gt; \n    unnest_tokens(word,text_field, token=\"words\") |&gt; \n    anti_join(stop_words,\"word\") |&gt; \n    mutate(word=str_replace(word,'artificialintelligence','artificial-intelligence') |&gt; \n             str_replace('machinelearning','machin-learning')\n           ) |&gt; \n    group_by(word, {{group_col}}) |&gt; \n    summarise(n_prj = length(unique(ProjectId)), .groups=\"drop\" ) |&gt; \n    arrange(desc({{group_col}}),desc(n_prj)) |&gt; \n    ungroup()\n}\n\nrank_words = function(word_token, group_col) {\n  word_token |&gt;\n    group_by({{group_col}}) |&gt; \n    dplyr::arrange(desc({{group_col}}),desc(n_prj)) |&gt;\n    dplyr::mutate(rank = row_number()) |&gt;\n    ungroup()\n}\n\nplot_top_n = function(word_token, n, group_col) {\n  top_n = rank_words(word_token, {{group_col}}) |&gt; \n    filter(rank &lt;= n)\n  top_n |&gt; \n    arrange({{group_col}},rank) |&gt;\n    ggplot(aes(x=n_prj, y = reorder_within(word, n_prj, {{group_col}} ))) +\n    geom_col(fill=\"midnightblue\") + \n    scale_y_reordered() +\n    facet_wrap(~ eval(enquo(group_col)), scales=\"free_y\") +\n    xlab(\"number of porject\") + ylab(\"word\")\n}\n\ntitle_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(title, year)\n\nabstrc_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(abstractText, year)\n\n\ntitle_word_by_year |&gt;\n  filter(year &gt;= 2016) |&gt;\n  mutate(year = as.character(year)) |&gt;\n  plot_top_n(15, year) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nabstrc_word_by_year |&gt;\n  filter(year &gt;= 2016) |&gt;\n  mutate(year = as.character(year)) |&gt;\n  plot_top_n(15, year) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n## track progression of top n over years\nplt_style_change = function(text_by_year, \n                            fav_words=c(\"artificial-intelligence\",\"ai\",\"health\", \"machine-learning\",\"learning\"),\n                            limit_rank = 10,\n                            weight = c(\"pcg\", \"absolute\")\n                            ) {\n  \n  if(weight[1] == \"absolute\") {\n    weightQ = quo(n_prj)\n  } else if (weight[1] == \"pcg\") {\n    weightQ = quo(pcg_prj)\n  } else {\n    stop()\n  }\n  \n  word_ranked = text_by_year |&gt; \n    # filter(year &gt; 2010) |&gt; \n    rank_words(year) |&gt; \n    group_by(year) |&gt; \n    mutate(pcg_prj = n_prj/sum(n_prj)) |&gt; \n    ungroup()\n  \n  top_n = word_ranked |&gt;\n    filter(rank &lt;= limit_rank) |&gt; \n    ## join back words that were crop out in top n ranking\n    select(-n_prj, -rank,-pcg_prj)\n  \n  top_n_complete =\n    tidyr::expand(top_n, year, word) |&gt; \n    left_join(word_ranked, c(\"year\",\"word\")) |&gt; \n    mutate(across(c(n_prj, pcg_prj), ~coalesce(.x,0))) |&gt; \n    mutate(rank = coalesce(rank, max(word_ranked$rank + 1)))\n    \n  top_n_complete |&gt; \n    filter(year &gt;= 2010 & year &lt;= 2023) |&gt; \n    ggplot(aes(x=year,y= !! weightQ, color=word)) + \n    geom_line() +\n    geom_point() +\n    theme(legend.position = \"none\", axis.text.y.right = element_text(size = 20)) + \n    # scale_y_reverse() +\n    gghighlight(word %in% fav_words,\n                unhighlighted_params=list(alpha=0.2),\n                line_label_type = \"text_path\"#\"sec_axis\"\n                ) +\n    scale_color_brewer(palette=\"Set2\") +\n    theme_minimal()\n}\n\n\nINTERESTING_WORDS = c(\"artificial-intelligence\",\"ai\",\"health\")\ntitle_word_by_year |&gt; \n  plt_style_change(\n    INTERESTING_WORDS,\n    weight = \"pcg\"\n  ) +\n  ylab(\"propotion of project\") +\n  ggtitle(\"Keywords mention in project title\",\n          stringr::str_wrap(glue::glue(\"The mention of 'health' seen a sharp raise in 2020. \",\n          \"Researchers are more comformatble using term 'ai' rather than 'aritificial-intelligence'\"),80)\n          )\n\n\n\n\n\n\n\n\n\nINTERESTING_WORDS = c(\"artificial-intelligence\",\"ai\",\"health\")\ntitle_word_by_year |&gt; \n  plt_style_change(\n    INTERESTING_WORDS,\n    weight=\"absolute\"\n  ) +\n  ylab(\"absolute number of project\")\n\n\n\n\n\n\n\n\n\nINTERESTING_WORDS=c(\"ai\",\"artificial-intelligence\",\"covid\",\"health\",\"artificial\")\nabstrc_word_by_year |&gt; \n  plt_style_change(\n    fav_words=INTERESTING_WORDS,\n    limit_rank = 30,\n    weight = \"pcg\"\n  )\n\n\n\n\n\n\n\n\n\nINTERESTING_WORDS=c(\"ai\",\"artificial-intelligence\",\"covid\",\"health\",\"artificial\")\nabstrc_word_by_year |&gt; \n  plt_style_change(\n    fav_words=INTERESTING_WORDS,\n    limit_rank = 30,\n    weight=\"absolute\"\n  )"
  },
  {
    "objectID": "03-03 Research England Interview Prep/05.html",
    "href": "03-03 Research England Interview Prep/05.html",
    "title": "Topic Modeling",
    "section": "",
    "text": "library(topicmodels)\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(arrow)\nlibrary(tidytext)"
  },
  {
    "objectID": "03-03 Research England Interview Prep/05.html#overview",
    "href": "03-03 Research England Interview Prep/05.html#overview",
    "title": "Topic Modeling",
    "section": "Overview",
    "text": "Overview\nTopic modeling is usefully for categorize long text into themes without reading full text.\n\nLoad Data\n\ngtr_desc = read_parquet(\"data/gtr.parquet\") |&gt; \n  select(id, abstractText)\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\"))) |&gt; \n  rename(id=ProjectId)\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  dplyr::relocate(ProjectReference, FundingOrgName, LeadROName, id, \n           is_partial,project_ref,part,occurance)\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is NOT Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain NA!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n## find out about \nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, id) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  ungroup()\n\n## find out with text other than continuous project are repeated\nrepeated_text = gtr_desc |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"id\") |&gt; \n  mutate(year = lubridate::year(StartDate)) |&gt;\n  inner_join(gtr_desc, by=\"id\")\n\n\n\nReview Abstract Word Count of Abstract\n\n\nCode\nanalysis_prj |&gt; \n  count(ProjectCategory,sort=T)\n\n\n# A tibble: 28 × 2\n   ProjectCategory                        n\n   &lt;chr&gt;                              &lt;int&gt;\n 1 Studentship                         1338\n 2 Research Grant                      1277\n 3 Collaborative R&D                    402\n 4 Feasibility Studies                  202\n 5 Fellowship                           192\n 6 Knowledge Transfer Partnership       132\n 7 EU-Funded                             77\n 8 Small Business Research Initiative    65\n 9 Study                                 54\n10 Training Grant                        49\n# ℹ 18 more rows\n\n\n\n## word count histogram\nabstract_words = analysis_prj |&gt; \n  unnest_tokens(word,abstractText) |&gt; \n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\nabstract_words |&gt; \n  count(id,ProjectCategory) |&gt; \n  ggplot() +\n  geom_histogram( fill=\"midnightblue\",aes(x=n) ) +\n  theme_minimal() +\n  ggtitle(\"Abstract Length\") +\n  facet_wrap(~ ProjectCategory)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nResearch Grant and Studentship have “rich” abstract we can extract on. You probably what to filter down to Research Grant before modeling"
  },
  {
    "objectID": "03-03 Research England Interview Prep/05.html#topic-model",
    "href": "03-03 Research England Interview Prep/05.html#topic-model",
    "title": "Topic Modeling",
    "section": "Topic Model",
    "text": "Topic Model\n\nTopic Modeling using LDA\nKey matrix from topic models are:\n\nGemma: is proportion of topics for each document\nBeta: is weight of words for each topic.\n\n\n\ncode for creating topic modelings\nabstract_word_count = abstract_words |&gt; \n  count(word,id,sort=T)\n\n## this is very expensive process so cached this result for saving rendering time\nif(interactive()) {\n  gtr_dtm = abstract_word_count |&gt; \n    cast_dtm(id,word,n)\n  topics = gtr_dtm |&gt; topicmodels::LDA(k=10)\n  saveRDS(topics, \"cache/05-topics.rds\")\n} else {\n  topics=readRDS(\"cache/05-topics.rds\")\n}\n\n\n\n\ncode for creating topic modelings\ntopics |&gt; \n  tidy(\"beta\") |&gt; \n  group_by(topic) |&gt; \n  slice_max(beta,n=15) |&gt; \n  ggplot(aes(y=reorder_within(term, beta,topic),x= beta )) +\n  geom_col() + \n  scale_y_reordered() +\n  facet_wrap(~ topic,scales=\"free_y\") +\n  theme_minimal() +\n  xlab(\"beta (higher indicate more relevant to topic)\") +\n  ylab(\"term\") +\n  ggtitle(\"LDA model output result (beta)\")\n\n\n\n\n\n\n\n\n\nThis is a typical graph used to visualize LDA output. beta indicate importance of words to extracted topic.\nHowever, it is still not easy to tell what these topic actually are. To explore topic, n-gram to link most frequently linked words, which will give us a picture of how “these key words” links together.\n\n\nCode\n## examples of top 10 topics.\ntop10_gamma = topics |&gt; \n  tidy(\"gamma\") |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma) |&gt; \n  rename(id=document)\ntop10_gamma |&gt; left_join(gtr_desc,by=\"id\")\n\n\n# A tibble: 10 × 4\n# Groups:   topic [10]\n   id                                   topic gamma abstractText                \n   &lt;chr&gt;                                &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;                       \n 1 7E7A8295-0B52-46C2-A6C8-1956056FE209     1 0.999 \"In partnership with the As…\n 2 4397428D-FBA6-478D-A52B-1C0232F7D4A4     2 0.999 \"The context of the researc…\n 3 A64CF8F6-E8F1-4A67-ACD8-F00A17D55833     3 0.999 \"Hair follicle development …\n 4 E273B321-E931-44F2-9FCC-54A55B2EE320     4 0.999 \"On June 19th, 2017, the Ne…\n 5 BDA3F2AA-0614-4ECE-9766-96C712CC7544     5 0.999 \"Invasive non-native specie…\n 6 7A9FCA6C-194C-4207-9A46-C77BB5FD6D4C     6 0.999 \"This fellowship analyses B…\n 7 1EFB61B0-740C-4754-8E4E-E0AF963D3D20     7 0.999 \"One of the great challenge…\n 8 7E5F0B59-A654-4151-9E82-5A49B0358751     8 0.999 \"In the Calvin-Benson cycle…\n 9 BF012599-0BBD-4702-89F7-5E214599310D     9 0.999 \"Magnetic data storage syst…\n10 931199B4-F6B9-4003-9874-446650A83F4A    10 0.999 \"With Covid-19 causing sign…\n\n\n\n\nTopic against Project Category\n\n## gamma score over the years\n## top gamma's n-gram\ntopic_binded = topics |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.8) |&gt; # 0.8 is a number we can be sure there is only one topic\n                         # for one document\n  rename(id=document) |&gt; \n  left_join(analysis_prj, \"id\")\n\ntopic_binded |&gt;\n  select(topic, ProjectCategory) |&gt;\n  mutate(topic_chr=fct_inorder(as.character(topic)) ) |&gt; \n  complete(topic_chr,ProjectCategory) |&gt; \n  ggplot() + \n  geom_bin2d(aes(x=topic_chr, y = ProjectCategory)) +\n  theme_minimal() +\n  scale_fill_viridis_c(option = \"F\",trans=\"sqrt\") +\n  ggtitle(\"Topic Against Porject Category\") +\n  coord_equal()\n\n\n\n\n\n\n\n\n\n\nuseful function create specific plot\n## For compute bigram ---------------------------------------------\n#' Calculate bigram given document and field\n#' @param doc_df document number\n#' @param field column of text field\n#' @param doc_id id indicating text comes from in fact\ncompute_bi_gram = function(doc_df,field) {\n  .gvars = group_vars(doc_df)\n  if(length(.gvars)==0) {\n    Gvars=quo(NULL)\n  } else {\n    Gvars=quo({any_of(.gvars)})\n  }\n  doc_df |&gt; \n    # select({{field}},!!Gvars ) |&gt; \n    unnest_tokens(pharse,{{field}},'ngrams',n=2) |&gt; \n    separate(pharse, into=c(\"word1\",\"word2\"),sep=\" \") |&gt; \n    ## clear up stopwords\n    anti_join(stop_words, c(\"word1\"=\"word\")) |&gt; \n    anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n    filter(!is.na(word1) & !is.na(word2))\n}\n#' customer counting function that also\ncount_bi_gram = function(bi_gram,...) {\n  bi_gram |&gt; \n    group_by(word1,word2, .add=T) |&gt; \n    group_by(..., .add=T) |&gt; \n    summarise(n=n(),.groups=\"drop\") |&gt; \n    arrange(desc(n))\n}\n## plot functions --------------------------------\nplot_beta=function(lda_model,ki,max_n=15) {\n  lda_model |&gt; \n    tidy(\"beta\") |&gt; \n    filter(topic == ki) |&gt; \n    group_by(topic) |&gt; \n    slice_max(beta,n=max_n) |&gt; \n    ggplot(aes(y=reorder_within(term, beta,topic),x= beta )) +\n    geom_col(fill=\"royalblue3\") + \n    scale_y_reordered() +\n    theme_minimal()\n}\nplot_word_graph=function(bi_gram_tokens,\n                         ki,\n                         top_n=15,\n                         model=NULL,\n                         color= \"cyan4\"\n                         ) {\n  if(!is.null(model)) {\n    beta=tidy(model,\"beta\") |&gt; \n      filter(topic == ki)\n    .add_beta = function(x) {\n      activate(x,nodes) |&gt; \n        left_join(beta, by=c(\"name\"=\"term\"))\n    }\n    .add_node_marker = function() {\n      geom_node_point(aes(size=beta))\n    }\n  } else {\n    .add_beta = \\(x) x\n    .add_node_marker = \\() geom_node_point(size=5)\n  }\n  typical_graph=bi_gram_tokens |&gt; \n    count_bi_gram(topic) |&gt; \n    relocate(word1,word2) |&gt; \n    as_tbl_graph()\n  ## filter bi-gram graph and plot\n  g = typical_graph |&gt; \n    .add_beta() |&gt; \n    activate(edges) |&gt;\n    filter(topic == ki) |&gt; \n    arrange(desc(n)) |&gt; \n    filter(row_number() &lt;= top_n) |&gt; \n    activate(nodes) |&gt; \n    filter(!node_is_isolated())\n  ## plot one single graph\n  g |&gt; \n    ggraph('kk') +\n    geom_edge_link(aes(width=n, alpha=n),color= color) +\n    geom_node_text(aes(label=name),repel = T) +\n    .add_node_marker() +\n    theme_void() +\n    ggtitle(paste(\"topic\",ki))\n}\n\n\n\n\npre compute expensive data\n## data ----------------------\n## bind topic result \ntopic_binded=topics |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.8) |&gt; # 0.8 is a number we can be sure there is only one topic\n                         # for one document\n  rename(id=document) |&gt; \n  left_join(analysis_prj, \"id\")\n\ntypical_docs = topic_binded |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma,n=1) |&gt; \n  select(topic,abstractText)\n\n## compute a graphic exploration\n## combine to bi-gram this is usually the expensive one\nbi_gram_tok = topic_binded |&gt;\n  compute_bi_gram(abstractText)\n## graph -----------------------\nplot_topics = function(model,reps_tk,reps_docs,topic_ki,color= \"cyan4\") {\n  ## topic graph\n  g1 = reps_tk |&gt;\n    plot_word_graph(topic_ki,top_n = 20,model=model,color=color) +\n    ggtitle(paste0(\"Topic\", topic_ki,\"\"))\n  \n  ## beta plot\n  g2 = model |&gt; \n    plot_beta(topic_ki,max_n = 20) +\n    ggtitle(\"Beta, words most represent topics\")\n  \n  ## add example texts\n  foot_notes = reps_docs |&gt; \n    filter(topic == topic_ki) |&gt; \n    pull(abstractText) |&gt; \n    stringr::str_wrap(160) |&gt; \n    stringr::str_trunc(500)\n  g3 = ggplot() + theme_void() + geom_text(aes(x=0,y=0,label = foot_notes)) +\n    ggtitle(\"Example\")\n  ggpubr::ggarrange(g1,g2) |&gt; \n    ggpubr::ggarrange(g3,nrow=2, heights=c(8,5))\n}\n\n\nThe topic modeling are in fact picking up different types of topics. Particularly “Studentship” is extracted as topic 2, “Collaborative R & D” is almost exclusively extracted as topic 10.\n\nG=map(c(2,10),~plot_topics(model=topics,\n                        reps_tk=bi_gram_tok, \n                        reps_docs=typical_docs,\n                        topic_ki=.x))\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\nggpubr::ggarrange(plotlist=G,nrow=2)\n\n\n\n\n\n\n\n\nTopic 3 seems to be describe about “machine learning”. One of the hot words. It makes no surprise that some how related to student ship.\ntopic 8, topic 9 both are very scientific. As we can see the description are very sentific and specific to particular field.\n\nG=map(c(8,9),~plot_topics(model=topics,\n                        reps_tk=bi_gram_tok, \n                        reps_docs=typical_docs,\n                        topic_ki=.x))\nggpubr::ggarrange(plotlist=G,nrow=2)\n\n\n\n\n\n\n\n\ntopic 8 probably sounds like biology. topic 9 sounds like material science.\nIt also interesting to see that although term “artificial intelligence” is frequently linked together, the “beta” score is actually low.\n\n\nTopic Against Institution\n\ntopics |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.75) |&gt; \n  left_join(gtr_desc,c(\"document\"=\"id\"))\n\n# A tibble: 1,762 × 4\n   document                             topic gamma abstractText                \n   &lt;chr&gt;                                &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;                       \n 1 D3519C9D-7595-4C4E-983B-8B1B392C7257     1 0.981 \"PROJECT APPROACH\\nThe C-ST…\n 2 8FD8E9CE-3E40-4FF5-8C5B-F70059C8204B     1 0.801 \"We remember many events fr…\n 3 2DF9A824-3A1B-4660-A686-7084E793B434     1 0.915 \"Digital games have extraor…\n 4 A399E04D-43CF-44D4-A124-31C33772D5C8     1 0.927 \"The project provides a pla…\n 5 B01F4112-691B-4D55-96E5-6FE1A82AFFE2     1 0.913 \"The digital games industry…\n 6 31AD9973-70D5-4514-B07B-1BC2D8FBDFB3     1 0.868 \"This research will explore…\n 7 7E7A8295-0B52-46C2-A6C8-1956056FE209     1 0.999 \"In partnership with the As…\n 8 08CA7336-763E-4976-8849-FA89EA858B44     1 0.943 \"Up to 4% of people who are…\n 9 29CB95C0-B9E4-4918-A58D-5BC8B7D85E4D     1 0.893 \"Around 6,200 per year are …\n10 0FE2D3FE-0C65-42C6-BBBE-2249D2C1AEFD     1 0.888 \"Contemporary art instituti…\n# ℹ 1,752 more rows\n\n\n\n\nFocus on Research Grant\nThe histogram shows us that research grant is the biggest chunk. So let’s apply “research grant” by itself to see if there anything interesting.\n\n## filter to research project only\nres_prj = analysis_prj |&gt; \n  filter(ProjectCategory==\"Research Grant\")\n\n## research document term matrix\nres_dtm = res_prj |&gt; \n  select(id,abstractText) |&gt; \n  unnest_tokens(word,abstractText) |&gt; \n  anti_join(stop_words) |&gt; \n  count(id,word,sort=T) |&gt; \n  cast_dtm(id,word,n)\n\nJoining with `by = join_by(word)`\n\n## research lda models \nif(interactive() ) {\n  res_lda = res_dtm |&gt; topicmodels::LDA(k=10)\n  saveRDS(res_lda,\"cache/05-res_lda.rds\")\n} else {\n  res_lda=readRDS(\"cache/05-res_lda.rds\")\n}\n\n\nres_reps = res_lda |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.75) |&gt; # 0.75 is a number we can be sure there is only one topic\n                         # for one document\n  rename(id=document) |&gt; \n  left_join(analysis_prj, \"id\")\n\n## compute three data sets\nres_betas = res_lda |&gt; tidy(\"beta\")\nres_bi_gram = res_reps |&gt; \n  compute_bi_gram(abstractText)\nres_reps_top = res_reps |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma,n=1)\n\n\n## a simple test of co-herence\nres_reps |&gt; \n  group_by(topic) |&gt; \n  slice_max(gamma,n=1) |&gt; \n  select(topic,LeadROName,Department,Title)\n\n# A tibble: 11 × 4\n# Groups:   topic [10]\n   topic LeadROName                        Department                      Title\n   &lt;int&gt; &lt;chr&gt;                             &lt;chr&gt;                           &lt;chr&gt;\n 1     1 University of the West of England Faculty of Business and Law     Deco…\n 2     1 Coventry University               Ctr for Business in Society     Deco…\n 3     2 University of Southampton         Human Development and Health    Feta…\n 4     3 Loughborough University           Loughborough University in Lon… MIMI…\n 5     4 University of Strathclyde         Inst of Pharmacy and Biomedica… Made…\n 6     5 Oxford Brookes University         Faculty of Health and Life Sci… 21EN…\n 7     6 University of Cambridge           Haematology                     Impr…\n 8     7 University of Cambridge           Computer Science and Technology A Un…\n 9     8 University of York                Computer Science                The …\n10     9 University of Edinburgh           Sch of Engineering              Cent…\n11    10 University of Exeter              Physics                         A Pl…\n\n\nJust by looking at department of research topic, it seems that some of the topic perhaps not as coherent as we hopped. It is possible that we need less topic than 10 perhaps.\nLets do another sample:\n\nres_reps |&gt; \n  group_by(topic) |&gt; \n  slice_sample(n=1) |&gt; \n  select(topic,LeadROName,Department,Title)\n\n# A tibble: 10 × 4\n# Groups:   topic [10]\n   topic LeadROName                        Department                      Title\n   &lt;int&gt; &lt;chr&gt;                             &lt;chr&gt;                           &lt;chr&gt;\n 1     1 University of Surrey              Computing Science               Mode…\n 2     2 Cardiff University                School of Physics and Astronomy Rapi…\n 3     3 University of Southampton         Sch of Chemistry                Arti…\n 4     4 King's College London             Informatics                     PLEA…\n 5     5 University of Bristol             Earth Sciences                  Real…\n 6     6 Royal Veterinary College          Comparative Biomedical Science… Unra…\n 7     7 Edinburgh Napier University       School of Computing             Natu…\n 8     8 University of the West of England Faculty of Environment and Tec… Publ…\n 9     9 University of Liverpool           Engineering (Level 1)           EPSR…\n10    10 University of Cambridge           Physics                         Quan…\n\n\nSo actually this maybe doing alright. Lets analysis department fields.\n\nres_reps |&gt; \n  select(topic,LeadROName,Department,Title,id) |&gt; \n  unnest_tokens(word,Department) |&gt; \n  anti_join(stop_words,\"word\") |&gt; \n  # filter(!word %in% c(\"sch\",\"school\",'science')) |&gt; \n  count(id,word,topic,sort=T) |&gt; \n  bind_tf_idf(word,id,n) |&gt;\n  group_by(topic) |&gt; \n  slice_max(tf_idf,n=10) |&gt; \n  ggplot(aes(x = tf_idf, y = reorder_within(word, tf_idf, topic) )) + \n  geom_col(fill=\"black\") +\n  scale_y_reordered() +\n  facet_wrap(~ topic,scale=\"free_y\") +\n  theme_minimal() +\n  ggtitle(\"Topic Versus Research Insititution Deparatment\")\n\n\n\n\n\n\n\n\nSo 5 and 6 could be similar, 3 and 4 could stand one topic. But we cannot be too sure yet. So\n\n\nCode\n## this process also merge similarish topics\ntopic_guess = c(\n  \"AI & Public Wellfare\",\n  \"Health & Medical\",\n  \"Computer & Mathematic\",\n  \n  \"Applied AI\",\n  \"Environmental Biology\",\n  \"Cellor Biology\",\n  \n  \"AI & Public Wellfare\",\n  \"AI & Public Wellfare\",\n  \"Renewable Energy\",\n  \"Material Science\"\n)\n\n\nInterestingly, climate change is on longer on this.\nFor analysis purposes, we put number 1,7,8 all under one umberalla “AI & Public Well-fare’.\n\n\nTrending Research\n\ntopic_by_year = res_lda |&gt; \n  tidy(\"gamma\") |&gt; \n  left_join(gtr_meta,by=c(\"document\"=\"id\")) |&gt; \n  mutate(year = year(StartDate)) |&gt; \n  mutate(topic_chr = topic_guess[topic]) |&gt; \n  group_by(topic_chr,year) |&gt;\n  summarise(topic_value = sum(gamma * AwardPounds))\n\n`summarise()` has grouped output by 'topic_chr'. You can override using the\n`.groups` argument.\n\nCutoff_Year=2010\n\nfast_growing = topic_by_year |&gt; \n  filter(year &gt;= Cutoff_Year) |&gt; \n  group_by(topic_chr) |&gt; \n  arrange(topic_chr,year) |&gt; \n  mutate(growth = (topic_value - lag(topic_value))/lag(topic_value) ) |&gt; \n  summarise(avg_growth = mean(growth,na.rm=T)) |&gt; \n  slice_max(avg_growth, n=2)\n\nlatest_top = topic_by_year |&gt; \n  slice_max(year,n=1) |&gt; \n  ungroup() |&gt; \n  slice_max(topic_value,n=1)\n  \ntopic_by_year |&gt;\n  ggplot(aes(x=year,y=topic_value)) +\n  geom_line(aes(color=topic_chr,fill=topic_chr)) + \n  theme_minimal() +\n  ylab(\"topic multiply award\") +\n  gghighlight::gghighlight(\n      (  topic_chr %in% fast_growing$topic_chr \n       | topic_chr %in% latest_top$topic_chr )\n    & year &gt;= Cutoff_Year,\n    line_label_type=\"sec_axis\"\n  ) +\n  scale_color_brewer(palette=\"Set2\") +\n  geom_vline(aes(xintercept=Cutoff_Year),linetype=\"dashed\") + \n  ggtitle(sprintf(\"Growing Topics Since %s\", Cutoff_Year))\n\nWarning in geom_line(aes(color = topic_chr, fill = topic_chr)): Ignoring\nunknown aesthetics: fill\n\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: topic_chr"
  },
  {
    "objectID": "03-03 Research England Interview Prep/05.html#appendix",
    "href": "03-03 Research England Interview Prep/05.html#appendix",
    "title": "Topic Modeling",
    "section": "Appendix",
    "text": "Appendix\n\nDeveloping a Visualization for Topic Modeling**\nI found bi-gram graph compensate traditional beta count graph. Instead of “reading tea leafs”, you can try read along the edge.\n\nbi_gram = topic_binded |&gt; \n  select(id,abstractText,topic) |&gt; \n  unnest_tokens(pharse,abstractText,'ngrams',n=2) |&gt; \n  separate(pharse, into=c(\"word1\",\"word2\"),sep=\" \") |&gt; \n  anti_join(stop_words, c(\"word1\"=\"word\")) |&gt; \n  anti_join(stop_words, c(\"word2\"=\"word\")) |&gt; \n  count(topic,word1,word2,sort=T)\n\ntypical_graph = bi_gram |&gt; \n  filter(!is.na(word1) & !is.na(word2)) |&gt;\n  group_by(topic) |&gt;\n  slice_max(n,n=20) |&gt; \n  relocate(word1,word2) |&gt; \n  as_tbl_graph()\n\nG = list()\nfor (i in c(2,10,8,9)) {\n  g = typical_graph |&gt; \n    activate(edges) |&gt;\n    filter(topic == i) |&gt; \n    activate(nodes) |&gt; \n    filter(!node_is_isolated())\n  if(length(g)==0) next()\n  plot_g = g |&gt; \n    ggraph('kk') +\n    geom_edge_link(aes(width=n, alpha=n),trans=\"log\",color=\"royalblue\") +\n    geom_node_text(aes(label=name),repel = T) +\n    geom_node_point(size=5) +\n    theme_void() +\n    ggtitle(paste(\"topic\",i))\n  G = append(G, list(plot_g))\n}\nggpubr::ggarrange(plotlist=G,ncol=2, nrow=2)\n\n\n\nList of all Topics From the Frist Extraction\n\nG=map(1:10,~plot_topics(model=topics,\n                        reps_tk=bi_gram_tok, \n                        reps_docs=typical_docs,\n                        topic_ki=.x))\nggpubr::ggarrange(plotlist=G,nrow=10)\n\n\n\n\n\n\n\n\n\n\nList of All Topic Extraction for Research Grant Only\nThe topic read\n\nG=map(1:10,~plot_topics(model=res_lda,\n                        reps_tk=res_bi_gram, \n                        reps_docs=res_reps_top,\n                        topic_ki=.x,\n                        color= \"coral2\"\n                        ))\nggpubr::ggarrange(plotlist=G,nrow=10)"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#overview",
    "href": "03-03 Research England Interview Prep/the_presentation.html#overview",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Overview",
    "text": "Overview\n\n\n\nWalk you through of exploratory text analysis for research england (UKRI Research and Innovation), which can access this from my GitHub Page\n\n\n\n\nall code avaible on this github page\n\n\n\n\n\nSome text analysis. Include usage of “Topic Model”\nSharing some insights\nDiscuss potential of this analytic for researchers and funding bodies.\n\n\n\n\n\nThank you for your invitation for the interview. As part of the interviewing process, I produced a sample of exploratory text analysis for research england (UKRI Research and Innovation). Hosted as a github page. This analysis use “Topic Modeling” to analysis project description field funded AI researches.\nI’m going to share some of the insight and discuss potential usage for this analysis m"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#data-source-gtr-project-description",
    "href": "03-03 Research England Interview Prep/the_presentation.html#data-source-gtr-project-description",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Data Source: GTR Project Description",
    "text": "Data Source: GTR Project Description\n\n\n\n\n\nSearch term “aritificial intelligence” has option to download csv\n\n\n\nThe Data\n\n5175 project descriptions contains term “artificial-intelligence” can be accessed using GtR api\n\n\n\n\n\n\n\nid\nabstractText\n\n\n\n\nFB05923A-042B-4277-AC29-67C973193C09\nMy PhD project addresses the important issue of how tensors (multidimensional data structures) can be employed for making sense of Big Data recorded across numerous domains in today's connected world. Namely, the proliferation of big, multidimensional data has created an ever-growing demand for innovative techniques to process such data in a computationally efficient and physically meaningful w...\n\n\nE01EBC26-C27E-449B-ABEC-937373F604A9\nProteins are the workhorses of the cell, but despite significant advances in our understanding of the physical and chemical principles underlying their structures and functions, one fundamental property - protein mechanics - remains poorly understood. Mechanical forces are involved in varied biological processes such as force-bearing proteins in the muscle and tension upon chromosomes separatio...\n\n\n\n\n\n\n\n\n\n\n\nFor the analysis, I gathered about 5175 project descriptions from “the Gateway of Research” website. The “Gateway of Research” is a website which let you search details about public founded research projects. In additional to project description, it also allows you to gather other project information such as time, it has been funded, or how much has been awarded.\n(a picture of website and a table)\nDescription of project can be called using GTR website, the rest data I can downloading using the download button"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#data-source-additional-meta-data",
    "href": "03-03 Research England Interview Prep/the_presentation.html#data-source-additional-meta-data",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Data Source: Additional Meta Data",
    "text": "Data Source: Additional Meta Data\n\n\n\n\n\nSearch term “aritificial intelligence” has option to download csv\n\n\n\nThe Data\nBy just search website\n\n\n\n\n\n\nTitle\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nStartDate\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nid\n\n\n\n\nUnravelling Enter...\nBBSRC\nBB/W020491/1\nAnimal and Plant Health Agency\nFood and Environmental Safety\nResearch Grant\n2022-05-17\n2023-07-17\n200860\nNA\nNorth West\nClosed\nA25812AC-5977-4AA5-BE67-63D335DEE63E\n\n\nArtificial Intell...\nInnovate UK\n105048\nCATCHAPP LTD\nNA\nFeasibility Studies\n2019-04-01\n2019-12-31\n78057\nNA\nLondon\nClosed\n1B765215-F6C4-4773-A1B6-8089729A82AE\n\n\nClassifying Advan...\nInnovate UK\n84000\nDIGITAL INTERRUPTION LIMITED\nNA\nCollaborative R&D\n2020-11-01\n2021-04-30\n95138\nNA\nNorth West\nClosed\nF0ACB6F0-01A9-4367-9867-04632E97F6B3\n\n\n\n\n\n\n\n\n\nProject Start Date\nAward amount (£)"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#data-quality",
    "href": "03-03 Research England Interview Prep/the_presentation.html#data-quality",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Data Quality",
    "text": "Data Quality\n\ngtr_desc = read_parquet(\"data/gtr.parquet\") |&gt; \n  select(id, abstractText) ## I've saved downloaded data into a parquet file\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\"))) |&gt; \n  rename(id=ProjectId)\n\n\n\n\n\n\n\n\nTitle\nFundingOrgName\nProjectReference\nLeadROName\nDepartment\nProjectCategory\nStartDate\nEndDate\nAwardPounds\nExpenditurePounds\nRegion\nStatus\nid\n\n\n\n\nUnravelling Enter...\nBBSRC\nBB/W020491/1\nAnimal and Plant Health Agency\nFood and Environmental Safety\nResearch Grant\n2022-05-17\n2023-07-17\n200860\nNA\nNorth West\nClosed\nA25812AC-5977-4AA5-BE67-63D335DEE63E\n\n\nArtificial Intell...\nInnovate UK\n105048\nCATCHAPP LTD\nNA\nFeasibility Studies\n2019-04-01\n2019-12-31\n78057\nNA\nLondon\nClosed\n1B765215-F6C4-4773-A1B6-8089729A82AE\n\n\nClassifying Advan...\nInnovate UK\n84000\nDIGITAL INTERRUPTION LIMITED\nNA\nCollaborative R&D\n2020-11-01\n2021-04-30\n95138\nNA\nNorth West\nClosed\nF0ACB6F0-01A9-4367-9867-04632E97F6B3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nabstractText\n\n\n\n\nFB05923A-042B-4277-AC29-67C973193C09\nMy PhD project addresses the important issue of how tensors (multidimensional data structures) can be employed for making sense of Big Data recorded across numerous domains in today's connected world. Namely, the proliferation of big, multidimensional data has created an ever-growing demand for innovative techniques to process such data in a computationally efficient and physically meaningful w...\n\n\nE01EBC26-C27E-449B-ABEC-937373F604A9\nProteins are the workhorses of the cell, but despite significant advances in our understanding of the physical and chemical principles underlying their structures and functions, one fundamental property - protein mechanics - remains poorly understood. Mechanical forces are involved in varied biological processes such as force-bearing proteins in the muscle and tension upon chromosomes separatio...\n\n\n\n\n\n\n\n\n\n\nOf the two data source\n\nFor 5175 projects, only 4453 can link together that is\nProject description can contain duplicates"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#duplicated-due-to-project-migration",
    "href": "03-03 Research England Interview Prep/the_presentation.html#duplicated-due-to-project-migration",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Duplicated Due to Project Migration",
    "text": "Duplicated Due to Project Migration\nSpecific to this analysis is some projects can migrate from one to another. When this happens the ProjectReference. 97 of these will need taken out.\n\n\n\n\n\n\nProjectReference\nLeadROName\nTitle\n\n\n\n\nES/R00983X/1\nUniversity of the West of England\nThe 'risk of risk': remodelling artificial intelligence algorithms for predicting child abuse.\n\n\nES/R00983X/2\nAston University\nThe 'risk of risk': remodelling artificial intelligence algorithms for predicting child abuse.\n\n\n\n\n\n\n\n\nBelow is the code for taking out the duplicate\ngtr_desc = read_parquet(\"data/gtr.parquet\") |&gt; \n  select(id, abstractText)\ngtr_meta = read_csv(\"data/projectsearch-1709481069771.csv\") |&gt; \n  mutate(across(ends_with(\"Date\"), ~as.Date(.x,\"%d/%m/%Y\"))) |&gt; \n  rename(id=ProjectId)\nPARTIAL_PTTN='(/[1-9])$'\n\n## waggle some columns for analytics\ngtr_pj = gtr_meta |&gt;\n  mutate(\n    is_partial = str_detect(ProjectReference, PARTIAL_PTTN),\n    project_ref = str_replace(ProjectReference,PARTIAL_PTTN,\"\"),\n    part = str_extract(ProjectReference, PARTIAL_PTTN) |&gt; \n      str_extract(\"\\\\d+\") |&gt; \n      as.numeric() |&gt; \n      coalesce(0)\n  ) |&gt; \n  # filter(is_partial) |&gt; \n  group_by(project_ref) |&gt;\n  mutate(occurance = n()) |&gt; \n  ungroup() |&gt; \n  dplyr::relocate(ProjectReference, FundingOrgName, LeadROName, id, \n           is_partial,project_ref,part,occurance)\n\n## early stop if this is no longer true\nstopifnot(\n  \"Project Reference is NOT Unique!\"=length(unique(gtr_pj$ProjectReference)) == nrow(gtr_pj),\n  \"Project Refrence contain NA!\"=!any(is.na(gtr_pj$ProjectReference))\n)\n\n## find out about \nunique_prj = gtr_pj |&gt;\n  relocate(ProjectReference, project_ref, id) |&gt; \n  group_by(project_ref) |&gt; \n  mutate(rn=row_number()) |&gt; \n  filter(rn==1) |&gt; \n  select(-rn) |&gt; \n  ungroup()"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#additional-duplicate",
    "href": "03-03 Research England Interview Prep/the_presentation.html#additional-duplicate",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Additional Duplicate",
    "text": "Additional Duplicate\nAdditional\n669 of duplicates were fund in the descriptions.\n\n\n\n\n\n\nProjectReference\nabstractText\n\n\n\n\n310175\nAwaiting Public Project Summary\n\n\n640016\nAwaiting Public Project Summary\n\n\nEP/X030210/1\nThe European microelectronics (ME) industry has a direct impact on approximately 20% of the European GDP and employs over 250,000 people, with more than 64,000 job vacancies. The main technological...\n\n\n84293\nAbstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensi...\n\n\nEP/X030148/1\nThe European microelectronics (ME) industry has a direct impact on approximately 20% of the European GDP and employs over 250,000 people, with more than 64,000 job vacancies. The main technological...\n\n\n73602\nno public description\n\n\n\n\n\n\n\n\nTreatment for these is taken them out all together\n## find out with text other than continuous project are repeated\nrepeated_text = gtr_desc |&gt; \n  group_by(abstractText) |&gt; \n  mutate(n=n()) |&gt; \n  filter(n!=1) |&gt;\n  arrange(abstractText)\n\n## Take the repeated test one out for now.\nanalysis_prj = unique_prj |&gt; \n  anti_join(repeated_text, by=\"id\") |&gt; \n  mutate(year = lubridate::year(StartDate)) |&gt;\n  inner_join(gtr_desc, by=\"id\")"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#data-reserved-for-analysis",
    "href": "03-03 Research England Interview Prep/the_presentation.html#data-reserved-for-analysis",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Data Reserved for Analysis",
    "text": "Data Reserved for Analysis\nAltogether this give us 3973 of rows for analysis.\n\n\ncode for balance check\nno_match = unique_prj |&gt; \n  anti_join(repeated_text, by=\"id\") |&gt; \n  anti_join(gtr_desc,by=\"id\")\n\ndiscrpency = tibble::tribble(\n  ~\"observation\", ~\"number of row\",\n  \"Total Number of gtr_meta data\", nrow(gtr_meta),\n  \"Less project migration\",-n_takeout,\n  \"Taken out \",- nrow(repeated_text |&gt; semi_join(unique_prj,\"id\")),\n  \"Won't have match to discription\",-nrow(no_match)\n)\n\nstopifnot(\"Discrepency Don't Add Up!\"=nrow(analysis_prj)==sum(discrpency$`number of row`))\n\n\n\n\n\n\n\n\nobservation\nnumber of row\n\n\n\n\nTotal Number of gtr_meta data\n5175\n\n\nLess project migration\n-97\n\n\nTaken out\n-389\n\n\nWon't have match to discription\n-716\n\n\nTotal\n3973"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#project-trend-overview",
    "href": "03-03 Research England Interview Prep/the_presentation.html#project-trend-overview",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Project Trend Overview",
    "text": "Project Trend Overview"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#keyword-mentions",
    "href": "03-03 Research England Interview Prep/the_presentation.html#keyword-mentions",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Keyword Mentions",
    "text": "Keyword Mentions\n\n\n\n## digest paragram into words\ntitle_word_by_year = analysis_prj |&gt; \n  tokenize_words_group(title, year)\n  \n## visualisation \ntitle_word_by_year |&gt; \n  plt_style_change(\n  ...\n\n## function for tokenise target fiel\ntokenize_words_group = function(df, col, group_col) {\n  df |&gt;\n    # preserve chained keywords\n    mutate(text_field = str_replace({{col}}, \n                                    '(A|a)rtificial (I|i)ntelligence', \n                                    'artificialintelligence') |&gt; \n             str_replace(\"machine learning\",\n                         'machinelearning')\n           ) |&gt; \n    unnest_tokens(word,text_field, token=\"words\") |&gt; \n    anti_join(stop_words,\"word\") |&gt; \n    mutate(word=str_replace(word,'artificialintelligence','artificial-intelligence') |&gt; \n             str_replace('machinelearning','machin-learning')\n           ) |&gt; \n    group_by(word, {{group_col}}) |&gt; \n    summarise(n_prj = length(unique(ProjectId)), .groups=\"drop\" ) |&gt; \n    arrange(desc({{group_col}}),desc(n_prj)) |&gt; \n    ungroup()\n}\n\n\n\n\nnext plot to zoom in"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#insights",
    "href": "03-03 Research England Interview Prep/the_presentation.html#insights",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Insights",
    "text": "Insights"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#what-is-topic-modeling",
    "href": "03-03 Research England Interview Prep/the_presentation.html#what-is-topic-modeling",
    "title": "Public Funding for Artificial Intelligence",
    "section": "What is Topic Modeling",
    "text": "What is Topic Modeling\n\nThe process of Extracting Themes from large document\nFrist step in topic modeling is convert text into document-term matrix\nTopic Model Algorithm such as LDA compares similarity between document by comparing propotion word frequencies."
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#on-our-data",
    "href": "03-03 Research England Interview Prep/the_presentation.html#on-our-data",
    "title": "Public Funding for Artificial Intelligence",
    "section": "On Our Data",
    "text": "On Our Data\n\nApply these description data our topic, and take 10 topics out.\n\n## word count histogram\nabstract_words = analysis_prj |&gt; \n  unnest_tokens(word,abstractText) |&gt; \n  anti_join(stop_words)\n  \n## word count\nabstract_word_count = abstract_words |&gt; \n  count(word,id,sort=T)\n  \n## create document term matrix\ngtr_dtm = abstract_word_count |&gt; \n    cast_dtm(id,word,n)\n    \n## create alda modeling\ntopics = gtr_dtm |&gt; topicmodels::LDA(k=10)\n\nbeta\n\nbeta = topics |&gt; tidy(\"beta\")\n\nbeta |&gt; \n  head(3) |&gt; \n  kbl2()\n\n\n\n\n\ntopic\nterm\nbeta\n\n\n\n\n1\namp\n0.0027870\n\n\n2\namp\n0.0004771\n\n\n3\namp\n0.0000829\n\n\n\n\n\n\n\n\ngamma\n\ngamma = topics |&gt; tidy(\"gamma\")\n\ngamma |&gt; \n  head(3) |&gt; \n  kbl2()\n\n\n\n\n\ndocument\ntopic\ngamma\n\n\n\n\n7D030DE3-A4B9-45B9-8C32-04A4C246E933\n1\n0.6638977\n\n\n848C12F5-FE3A-442E-A13C-E796468A3396\n1\n0.0001376\n\n\nC2A76562-29C1-4CD1-BEDE-2AD2E56BDC78\n1\n0.0001244"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#helper-understand-beta-gamma",
    "href": "03-03 Research England Interview Prep/the_presentation.html#helper-understand-beta-gamma",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Helper: Understand beta & gamma",
    "text": "Helper: Understand beta & gamma\n\n\nSum of beta for each topic equal 1\n\nsum_beta=beta |&gt; \n  group_by(topic) |&gt; \n  summarise(n=sum(beta))\n\nstopifnot(all(round(sum_beta$n,2) ==1.00))\nsum_beta |&gt; kbl2()\n\n\n\n\n\ntopic\nn\n\n\n\n\n1\n1\n\n\n2\n1\n\n\n3\n1\n\n\n4\n1\n\n\n5\n1\n\n\n6\n1\n\n\n7\n1\n\n\n8\n1\n\n\n9\n1\n\n\n10\n1\n\n\n\n\n\n\n\n\n\nSum of gamma for each document equal 1\n\nsum_gamma=gamma |&gt; \n  group_by(document) |&gt; \n  summarise(n=sum(gamma))\n\nstopifnot(all(round(sum_gamma$n,2) ==1.00))\nsum_gamma |&gt; \n  head(2) |&gt; \n  kbl2()\n\n\n\n\n\ndocument\nn\n\n\n\n\n000B1EC5-8655-498F-A9E3-AF5A173AD61F\n1\n\n\n000FBDEE-B3A9-43BB-A055-6F6ADF9FC292\n1"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#topic-against-funding-type",
    "href": "03-03 Research England Interview Prep/the_presentation.html#topic-against-funding-type",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Topic Against Funding Type",
    "text": "Topic Against Funding Type\n## finding the most representative gamma\ntopic_binded = topics |&gt; \n  tidy(\"gamma\") |&gt; \n  filter(gamma &gt; 0.8) |&gt; # 0.8 is a number we can be sure there is only one topic\n                         # for one document\n  rename(id=document) |&gt; \n  left_join(analysis_prj, \"id\")\n\n## creating graphics\ntopic_binded |&gt;\n  select(topic, ProjectCategory) |&gt;\n  mutate(topic_chr=fct_inorder(as.character(topic)) ) |&gt; \n  complete(topic_chr,ProjectCategory) |&gt; \n  ggplot() + \n  geom_bin2d(aes(x=topic_chr, y = ProjectCategory)) +\n  theme_minimal() +\n  scale_fill_viridis_c(option = \"F\",trans=\"sqrt\") +\n  ggtitle(\"Topic Against Porject Category\") +\n  coord_equal()\n\nOne thing we can do is for each topic, selecting gamma higher than 0.8"
  },
  {
    "objectID": "03-03 Research England Interview Prep/the_presentation.html#topic-against-funding-type-1",
    "href": "03-03 Research England Interview Prep/the_presentation.html#topic-against-funding-type-1",
    "title": "Public Funding for Artificial Intelligence",
    "section": "Topic Against Funding Type",
    "text": "Topic Against Funding Type\n\n\nThe topic modeling are in fact picking up different types of topics. Particularly “Studentship” is extracted as topic 2, “Collaborative R & D” is almost exclusively extracted as topic 10.\n\nTopic 2 & 10\n\nTopic 2Topic 10\n\n\n\n\n\n\n\n\n\n\ntopic 3 seems to be describe about “machine learning”. One of the hot words. It makes no surprise that some how related to student ship.\ntopic 8, topic 9 both are very scientific. As we can see the description are very scientific and specific to particular field."
  },
  {
    "objectID": "04-20 ML with Tidymodel/02.html",
    "href": "04-20 ML with Tidymodel/02.html",
    "title": "Work with AMES Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\ntheme_set(theme_minimal())\n\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))"
  },
  {
    "objectID": "04-20 ML with Tidymodel/02.html#study-features",
    "href": "04-20 ML with Tidymodel/02.html#study-features",
    "title": "Work with AMES Data",
    "section": "Study Features",
    "text": "Study Features\nMy first intuition is create a factor analysis so I can try understand which dimensions are related which are not.\nExplore data using pca for analysing top performing featuresfviz_pca_ind\n\names.numb = ames |&gt; \n  select(where(is.numeric)) |&gt; \n  select(-starts_with(\"Year_\"), -Longitude, - Latitude, -Sale_Price)\n\n# ames.numb |&gt; glimpse()\n\names.fct = ames.numb |&gt; \n  prcomp(scale=T) # the function\n\n# # fviz_eig from factoextra package\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\names.fct |&gt; fviz_eig() #Extract and visualize the eigenvalues/variances of dimensions\n\n\n\n\n\n\n\n\nFrom this plot the elbow seems to be two or three. This visualization seems to be better at explaining factors (I will continue my fascination with PCA later).\n\nfviz_pca_var(\n  ames.fct\n  ,col.var = \"contrib\"\n  ,gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\n  ,repel = TRUE     # Avoid text overlapping\n  )\n\n\n\n\n\n\n\n\nActually you can use PCA with tidy-model (because you can also train and predict)\n\names_tts = ames |&gt; initial_split(prop = 0.8, strata = Sale_Price)\names_train = ames_tts |&gt; training()\names_test = ames_tts |&gt; testing()\n\n\npca_trans = ames_train |&gt; \n  select(where(is.numeric)) |&gt; \n  recipe(~.) |&gt; \n  step_normalize(all_numeric()) |&gt; \n  step_pca(all_numeric(), num_comp = 2)# still \n\nGetting data out, according to receipt example, instead of train or predict you do so by prep and bake.\nThis results in scoring of the two latent feature for each individual row.\n\n#\npca_estimate = pca_trans |&gt; prep()\npca_data &lt;- bake(pca_estimate,ames_train)\npca_data\n\n# A tibble: 2,342 × 2\n     PC1    PC2\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1 2.54   0.902\n 2 2.13   2.64 \n 3 3.35   0.523\n 4 2.69  -0.442\n 5 2.72  -1.64 \n 6 1.10   1.50 \n 7 3.27   0.386\n 8 2.03   0.253\n 9 3.59  -0.173\n10 0.429 -2.67 \n# ℹ 2,332 more rows\n\n\nExtract coeficient feature for against all score?\n\n## visualise coeficiency results\nrequire(tidytext)\n\nLoading required package: tidytext\n\ntidy(pca_estimate,number=2,type=\"coef\") |&gt; \n  filter(component %in% c(\"PC1\",\"PC2\")) |&gt; \n  ggplot(aes(x = value, y=terms, fill=component)) +\n  geom_col() + \n  scale_fill_brewer(palette = \"Set1\") + \n  ggtitle(\"Extracting Feature from Tidyverse Principle Component\")\n\n\n\n\n\n\n\n\nBy calculating corelation matrix is indeed, you replicate the same value from the latent feature.\n\nX &lt;- ames_train |&gt; select(where(is.numeric), -Sale_Price)\n\ncor(X, pca_data ) |&gt; \n  data.frame() |&gt; \n  rownames_to_column() |&gt; \n  pivot_longer(c(PC1, PC2)) |&gt; \n  ggplot(aes(y=rowname, x=value, fill=name)) + \n  geom_col() + \n  ggtitle(\"Calcuate Corelation Matrix from Scratch\")\n\n\n\n\n\n\n\n\n\ntidy(pca_estimate,number=2,type=\"variance\") |&gt; \n  ggplot(aes(y = value, x = component,fill=terms)) + \n  geom_col() + \n  facet_wrap(~terms,scales = \"free\")\n\n\n\n\n\n\n\n\nExperimenting closest feature.\nHowever I am interested in see which feature should below in which under laying factor?\nSo I created this visualisaion:\n\n\nmagic function for compare features\ncompare_factor = function(tidybaked, pca1, pca2) {\n  \n  pca1Quo = quo(get(pca1))\n  pca2Quo = quo(get(pca2))\n  \n  tidybaked |&gt; \n    filter(component %in% c(pca1,pca2)) |&gt; \n    pivot_wider(\n       names_from=component\n      ,values_from=value\n    ) |&gt; \n    mutate(diff = !!pca2Quo - !!pca1Quo) |&gt; \n    arrange(diff) |&gt; \n    rowwise() |&gt; \n    mutate(\n      mxv=max(!!pca2Quo, !!pca1Quo), miv=min(!!pca2Quo, !!pca1Quo)\n    ) |&gt;\n    mutate(\n       terms = fct_inorder(terms)\n      ,is_pc2=diff &gt;= 0\n      ,abs_diff=abs(diff)) |&gt; \n    ggplot(aes(y= terms,alpha=abs_diff)) + \n    geom_linerange(\n      aes(xmin=miv,xmax=mxv,color=is_pc2)\n      ,linewidth=2\n      ) + \n    geom_point(aes(x=!!pca1Quo),color=\"lightcoral\") + \n    geom_point(aes(x=!!pca2Quo),color=\"cyan3\") +\n    scale_alpha_continuous(trans=\"sqrt\") +\n    theme(legend.position=\"none\")\n}\n\n\nThis compares principle component 1 against 2\n\ntidy(pca_estimate,number=2,type=\"coef\") |&gt; \n  compare_factor(\"PC1\", \"PC2\") +\n  ggtitle(\n    \"Feature Alocation of two principle component\"\n    ,\"Compare coeficency of PCA1 against PCA2\")\n\n\n\n\n\n\n\n\nThis compares principle component from 2 against 3\n\ntidy(pca_estimate,number=2,type=\"coef\") |&gt; \n  compare_factor(\"PC2\", \"PC3\") +\n  ggtitle(\n    \"Feature Aloocation of two principle component\"\n    ,\"Compare PCA2 against PCA3\")\n\n\n\n\n\n\n\n\nHowever I don’t understand this visualisation enough to understand if what this PCA says about data (if they are latend or not). So I am creating below experiment to understand PCA.\n\nExperiment of a rotated 2D surface in a 3D surfaces\nFirst is by creating on latent variable we understand.\nIf project a 2D sheet in a 3D space the surface point on that 2D sheet has underlaying latent space of the 2 dimension.\nFirst we need to sample a surface point and rotate this surface\n\nx &lt;- rnorm(1000)\ny &lt;- rnorm(1000)\n\nd = data.frame(\n   x = x\n  ,y = y \n) |&gt; \n  arrange(x) |&gt; \n  mutate(z = x + y)\nplotly::plot_ly(d\n  ,x=~x\n  ,y=~y\n  ,z=~z\n  ,marker=list(\n    color=\"blue\"\n    ,size=2))\n\nNo trace type specified:\n  Based on info supplied, a 'scatter3d' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d\n\n\nNo scatter3d mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\nrequire(factoextra)\nnormal_fct = d |&gt; \n  prcomp()\n\nnormal_fct |&gt; \n  get_eig()\n\n        eigenvalue variance.percent cumulative.variance.percent\nDim.1 2.910860e+00     7.596114e+01                    75.96114\nDim.2 9.211784e-01     2.403886e+01                   100.00000\nDim.3 1.876141e-30     4.895934e-29                   100.00000\n\n## screen plot is ploting wahts\nnormal_fct |&gt; \n  fviz_eig()\n\n\n\n\n\n\n\n\nIn this case, the true under-laying variance, it depends on how that surface you generated rotate.\n\nnormal_fct |&gt; \n  fviz_pca_var(repel=T)\n\n\n\n\n\n\n\n\n\nfct_d = d |&gt; \n  recipe() |&gt; \n  step_normalize() |&gt; \n  step_pca()\n\nfct_d |&gt; prep() |&gt; tidy()\n\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_dpEKF\n2      2 step      pca       TRUE    FALSE pca_XfBD7"
  },
  {
    "objectID": "04-20 ML with Tidymodel/02.html#actual-model",
    "href": "04-20 ML with Tidymodel/02.html#actual-model",
    "title": "Work with AMES Data",
    "section": "Actual Model",
    "text": "Actual Model\n\n## classic geo-special machine-learning traning\names_rec = ames_train |&gt; \n  recipe(Sale_Price ~ \n           Neighborhood \n         + Gr_Liv_Area \n         + Year_Built \n         + Bldg_Type\n         + Latitude \n         + Longitude, data=ames_train) |&gt; \n  step_log(Gr_Liv_Area, base=10) |&gt; \n  step_other(Neighborhood, threshold=0.01) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) |&gt; \n  step_ns(Latitude,Longitude,deg_free=20)\n  \nlm_model=linear_reg() |&gt; set_engine(\"lm\")\ntree_model=rand_forest() |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"regression\")\n\n\n## base line for use single model\nlm_wflow =\n  workflow() |&gt; \n  add_model(lm_model) |&gt; \n  add_recipe(ames_rec)\n\n## use multiple model requires receipy and model match up\nwflowset = workflow_set(\n  list(ames_rec), \n  models=list(lm_model, tree_model))\n\n\nwflowset |&gt; \n  workflow_map() |&gt; \n  mutate(fitted = map(info, ~ fit(.x$workflow[[1]],ames_train) ))\n\n# A workflow set/tibble: 2 × 5\n  wflow_id           info             option    result         fitted    \n  &lt;chr&gt;              &lt;list&gt;           &lt;list&gt;    &lt;list&gt;         &lt;list&gt;    \n1 recipe_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;try-errr [1]&gt; &lt;workflow&gt;\n2 recipe_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;try-errr [1]&gt; &lt;workflow&gt;\n\n\nYou won’t be able to fit model because you have not learned re-sample yet."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Frank Liang",
    "section": "",
    "text": "Data Analyst/Developer | Exeter, UK"
  },
  {
    "objectID": "01-01 Deep Learning Py/01-12.html",
    "href": "01-01 Deep Learning Py/01-12.html",
    "title": "Visualise Matrix Transformation as Paper Folding",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n%config InlineBackend.figure_format='retina'\n# to change default colormap\nplt.rcParams[\"image.cmap\"] = \"Set3\"\n# to change default color cycle\nmyC= plt.cm.tab20b.colors\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=myC[:1]+myC[2:])\nmatplotlib.rcParams['figure.figsize'] = (20, 10)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nm1 = np.array([[1,2],\n            [3,4],\n            [1,1],\n            [5,4]])#two dimenstional tensor\n\nweight = np.array([0.5,0.5])#what tensor will try to guess\nnp.dot(m1,weight)#if this were a scala problem this the value will be used for relu activation\n\n\narray([1.5, 3.5, 1. , 4.5])\nWhat maybe powerful is matrix will changes shapes. Would it matter what input shape I choose?\nCode\nprint('weight2 is')\nweight2 = np.array([[0.5,0.1,0.3],\n                    [0.5,0.2,0.3]])#you can expand infinitly horisentally\n\nprint(weight2)\nprint('Using dot product we expand the width of the matrix to 3')\nprint(np.dot(m1,weight2))\n\nprint('Use another product to trave the dimension back ')\nweight3 = np.array([[0.1,0.2],\n                    [0.9,0.1],\n                    [0.2,0.5]])\nm3 = np.dot(np.dot(m1,weight2),weight3)\nprint(m3)\nprint(\"notice how the first matrix expand matrix dimention to 3 and the second to 2;\")\n\n\nweight2 is\n[[0.5 0.1 0.3]\n [0.5 0.2 0.3]]\nUsing dot product we expand the width of the matrix to 3\n[[1.5 0.5 0.9]\n [3.5 1.1 2.1]\n [1.  0.3 0.6]\n [4.5 1.3 2.7]]\nUse another product to trave the dimension back \n[[0.78 0.8 ]\n [1.76 1.86]\n [0.49 0.53]\n [2.16 2.38]]\nnotice how the first matrix expand matrix dimention to 3 and the second to 2;\nCode\nweight2 = np.array(\n    [[0.5,0.5,0.5],\n    [0.5,1,0.41]])\nweight3 = np.array(\n    [[1, 0.2],\n    [0.9,0.1],\n    [0.9,0.7]])\n# notice how the th\nweight = np.dot(weight2, weight3)\nprint(weight)\n\ndef my_transformation(m1, w):\n    m3=m1.dot(w)\n    return m3\n\ndef plot_transformation(m1, m3, ax=None):\n    if ax is None:\n        ax=plt.gca()\n    for i in range(m1.shape[0]):\n        x=[m1[i,0],m3[i,0]]\n        y=[m1[i,1],m3[i,1]]\n        ax.plot(x,y, 'k-',alpha=0.8,zorder=5)\n    \n    ax.scatter(m1[:,0],m1[:,1], zorder=6,alpha=1)\n    ax.scatter(m3[:,0],m3[:,1],zorder=7, alpha=0.7)\n\n\n[[1.4   0.5  ]\n [1.769 0.487]]\nCode\n# This perhaps is really usefull to test to see what you neuro network is doing:\nrotator = np.array([\n    [0,1],\n    [1,0]\n])\n\nfigure,ax=plt.subplots(figsize=(5,5))\n\nm1=np.random.random((50,2))\nm2=my_transformation(m1, rotator)\nplot_transformation(m1,m2,ax=ax)"
  },
  {
    "objectID": "01-01 Deep Learning Py/01-12.html#create-a-contour-visualiualisation-of-tendency",
    "href": "01-01 Deep Learning Py/01-12.html#create-a-contour-visualiualisation-of-tendency",
    "title": "Visualise Matrix Transformation as Paper Folding",
    "section": "Create a Contour visualiualisation of tendency",
    "text": "Create a Contour visualiualisation of tendency\nNext I want to visualise the tendency the matrix do for sampled at each points… I found two visual idiom: meshgird and quiver.\n\n\nCode\n\nv1=np.arange(-2, 2, .5)\nv2=np.arange(-2, 2, .5)\nx,y = np.meshgrid(v1, v2)\n\n# to help understand what np.meshgird does. \n# It helps creates cross from two vector\nassert x.shape==y.shape\nassert len(v1)==x.shape[1]\nassert x.size == len(v1) * len(v2)\n\nplt.figure(figsize=(3,3))\nplt.plot(x,y,marker='.',linestyle='none',color='k')\n\nx,y = np.meshgrid(v1, v2)\nz=np.sqrt(x**2 + y**2)\nplt.contourf(x,y,z)\nplt.colorbar()\nplt.axis('scaled')\n\n# z = x*np.exp(-x**2 - y**2)\n# v, u = np.gradient(z, .2, .2)\n# fig, ax = plt.subplots()\n# q = ax.quiver(x,y,u,v)\n# plt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncord=np.vstack((x.reshape(1,x.size),y.reshape(1,y.size))).T\nplt.figure(figsize=(3,3))\nplt.plot(cord[:,0],cord[:,1],marker='.',linestyle=\"none\")"
  },
  {
    "objectID": "01-01 Deep Learning Py/index.html",
    "href": "01-01 Deep Learning Py/index.html",
    "title": "Deep Learning with Python Summary Page",
    "section": "",
    "text": "Ledgend:\n\nFirst Lab Explore linear matrix transformation effect for a two dimension array with two feature (just x and y)\nSecond Follow the Deep Learning Book Chapter 3 use example of IBM sentient analysis."
  },
  {
    "objectID": "01-01 Setup Delta Lake/01.html",
    "href": "01-01 Setup Delta Lake/01.html",
    "title": "Setup Spark and and delta Lake",
    "section": "",
    "text": "image"
  },
  {
    "objectID": "01-01 Setup Delta Lake/01.html#basic-tutorial",
    "href": "01-01 Setup Delta Lake/01.html#basic-tutorial",
    "title": "Setup Spark and and delta Lake",
    "section": "",
    "text": "image"
  },
  {
    "objectID": "01-01 Setup Delta Lake/01.html#code",
    "href": "01-01 Setup Delta Lake/01.html#code",
    "title": "Setup Spark and and delta Lake",
    "section": "Code",
    "text": "Code\n\n# set up a project\nimport pyspark\nfrom delta import *\n\nbuilder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\n\n\n# create a table (run once)\ndata = spark.range(0, 5)\ndata.write.format(\"delta\").save(\"/tmp/delta-table\")\n# run again creates following error: \n#AnalysisException: [DELTA_PATH_EXISTS] Cannot write to already existent path file\n\n\n# Read file from path\ndf = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\ndf.show()\n\n+---+\n| id|\n+---+\n|  2|\n|  3|\n|  4|\n|  0|\n|  1|\n+---+\n\n\n\n\n# upate table data\ndata = spark.range(5, 10)\n# brute force update\ndata.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n\n\n# conidtional update\nfrom delta.tables import *\nfrom pyspark.sql.functions import *\n\ndeltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n\n# Update every even value by adding 100 to it\ndeltaTable.update(\n  condition = expr(\"id % 2 == 0\"),\n  set = { \"id\": expr(\"id + 100\") })\n\n# Delete every even value\ndeltaTable.delete(condition = expr(\"id % 2 == 0\"))\n\n# Upsert (merge) new data\nnewData = spark.range(0, 20)\n\ndeltaTable.alias(\"oldData\") \\\n  .merge(\n    newData.alias(\"newData\"),\n    \"oldData.id = newData.id\") \\\n  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n  .execute()\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n| 15|\n| 16|\n| 17|\n| 18|\n| 19|\n+---+\n\n\n\n\n# time travel\ndf = spark.read.format(\"delta\") \\\n  .option(\"versionAsOf\", 0) \\\n  .load(\"/tmp/delta-table\")\n\ndf.show()\n\n+---+\n| id|\n+---+\n|  2|\n|  3|\n|  4|\n|  0|\n|  1|\n+---+\n\n\n\n\n# perform a live streaming\nstreamingDf = spark.readStream.format(\"rate\").load()\n\nstream = streamingDf \\\n  .selectExpr(\"value as id\") \\\n  .writeStream.format(\"delta\") \\\n  .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\\n  .start(\"/tmp/delta-table\")\n\n23/12/28 19:08:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n23/12/28 19:08:27 ERROR NonFateSharingFuture: Failed to get result from future\nscala.runtime.NonLocalReturnControl\n23/12/28 19:08:37 ERROR NonFateSharingFuture: Failed to get result from future\nscala.runtime.NonLocalReturnControl\n23/12/28 19:08:47 ERROR NonFateSharingFuture: Failed to get result from future\nscala.runtime.NonLocalReturnControl\n23/12/28 19:08:57 ERROR NonFateSharingFuture: Failed to get result from future\nscala.runtime.NonLocalReturnControl\n23/12/28 19:09:07 ERROR NonFateSharingFuture: Failed to get result from future\nscala.runtime.NonLocalReturnControl\n\n\n\nstream.stop()"
  },
  {
    "objectID": "01-01 Setup Delta Lake/01.html#build-in-data",
    "href": "01-01 Setup Delta Lake/01.html#build-in-data",
    "title": "Setup Spark and and delta Lake",
    "section": "Build-in data?",
    "text": "Build-in data?\n\nspark.read.json(\"logs.json\")"
  },
  {
    "objectID": "01-28 Geopandas and Altair Vega/vega-altair.html",
    "href": "01-28 Geopandas and Altair Vega/vega-altair.html",
    "title": "Try out vega altairs geo spatial visualisation",
    "section": "",
    "text": "import altair as alt\nfrom vega_datasets import data\n\nboroughs = alt.topo_feature(data.londonBoroughs.url, 'boroughs')\ntubelines = alt.topo_feature(data.londonTubeLines.url, 'line')\ncentroids = data.londonCentroids.url\nbackground = alt.Chart(boroughs, width=700, height=500).mark_geoshape(\n    stroke='white',\n    strokeWidth=2\n).encode(\n    color=alt.value('#eee'),\n)\nlabels = alt.Chart(centroids).mark_text().encode(\n    longitude='cx:Q',\n    latitude='cy:Q',\n    text='bLabel:N',\n    size=alt.value(8),\n    opacity=alt.value(0.6)\n).transform_calculate(\n    \"bLabel\", \"indexof (datum.name,' ') &gt; 0  ? substring(datum.name,0,indexof(datum.name, ' ')) : datum.name\"\n)\n\nline_scale = alt.Scale(domain=[\"Bakerloo\", \"Central\", \"Circle\", \"District\", \"DLR\",\n                               \"Hammersmith & City\", \"Jubilee\", \"Metropolitan\", \"Northern\",\n                               \"Piccadilly\", \"Victoria\", \"Waterloo & City\"],\n                       range=[\"rgb(137,78,36)\", \"rgb(220,36,30)\", \"rgb(255,206,0)\",\n                              \"rgb(1,114,41)\", \"rgb(0,175,173)\", \"rgb(215,153,175)\",\n                              \"rgb(106,114,120)\", \"rgb(114,17,84)\", \"rgb(0,0,0)\",\n                              \"rgb(0,24,168)\", \"rgb(0,160,226)\", \"rgb(106,187,170)\"])\n\nlines = alt.Chart(tubelines).mark_geoshape(\n    filled=False,\n    strokeWidth=2\n).encode(\n    alt.Color('id:N')\n        .title(None)\n        .legend(orient='bottom-right', offset=0)\n        .scale(line_scale)\n)\n\nbackground + labels + lines\nimport altair as alt\nfrom vega_datasets import data\nimport geopandas as gpd\n\nurl = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\ngdf_ne = gpd.read_file(url)  # zipped shapefile\ngdf_ne = gdf_ne[[\"NAME\", \"CONTINENT\", \"POP_EST\", 'geometry']]\nalt.Chart(gdf_ne).mark_geoshape(\n    fill='lightgrey',stroke='white',strokeWidth=0.5\n)"
  },
  {
    "objectID": "01-28 Geopandas and Altair Vega/vega-altair.html#geo-pandas",
    "href": "01-28 Geopandas and Altair Vega/vega-altair.html#geo-pandas",
    "title": "Try out vega altairs geo spatial visualisation",
    "section": "Geo Pandas",
    "text": "Geo Pandas\n\nimport geopandas\nfrom geodatasets import get_path\n\npath_to_data = get_path(\"nybb\")\ngdf = geopandas.read_file(path_to_data)\n\n\nimport matplotlib.pyplot as plt\n\n%config InlineBackend.figure_format='retina'\n\nax=plt.gca()\ngdf.plot(ax=ax)\ngdf.centroid.plot(ax=ax,color='white')\n\n\n\n\n\n\n\n\n\ngdf.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "02-09 Kaggle Competition/kaggle-2024-credit-risks/fl-02-10.html",
    "href": "02-09 Kaggle Competition/kaggle-2024-credit-risks/fl-02-10.html",
    "title": "Explore Data in train_static",
    "section": "",
    "text": "import pandas as pd\nimport os\nfrom directory_tree import display_tree\nfrom sklearn.metrics import roc_auc_score\nfrom pathlib import Path\nimport duckdb\nimport ibis\nfrom ibis import _\nfrom ibis import selectors as s\nimport re\nfrom importlib import reload \nfrom IPython.display import display, HTML\n# reload(tools)\n\nfrom tools import FeatureDefinition, pretty_display\nimport altair as alt\nfrom functools import reduce\n\n\nDATA_DIR = 'home-credit-credit-risk-model-stability/parquet_files/train'\np = Path(DATA_DIR)\n\nibis.options.interactive=True\n\n# load data into directory\ndata_files=list(p.glob('*.parquet'))#load_all data\nds = {re.sub('.parquet','',f.name):ibis.read_parquet(f) for f in data_files}\n\nWhy train_static? Please this dataset don’t seems to have cardinality problems.\n\nfrom importlib import reload \nfrom IPython.display import display, HTML\n# reload(tools)\n\nfrom tools import FeatureDefinition, pretty_display\n# lookup column definition here if you don't know what \nintersting_cols = [\n    'bankacctype_710L',\n    'credtype_322L','disbursementtype_67L',\n    'inittransactioncode_186L','lastst_736L','paytype1st_925L','paytype_783L',\n    'twobodfilling_608L','typesuite_864L'\n]\npretty_display(\n    FeatureDefinition().lookup_col(intersting_cols)\n)\n\nVariable.str.contains(\"bankacctype_710L\") or Variable.str.contains(\"credtype_322L\") or Variable.str.contains(\"disbursementtype_67L\") or Variable.str.contains(\"inittransactioncode_186L\") or Variable.str.contains(\"lastst_736L\") or Variable.str.contains(\"paytype1st_925L\") or Variable.str.contains(\"paytype_783L\") or Variable.str.contains(\"twobodfilling_608L\") or Variable.str.contains(\"typesuite_864L\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nFile\n\n\n\n\n0\nbankacctype_710L\nType of applicant's bank account.\n[train_static_0_0, train_static_0_1]\n\n\n1\ncredtype_322L\nType of credit.\n[train_static_0_0, train_static_0_1]\n\n\n2\ndisbursementtype_67L\nType of disbursement.\n[train_static_0_0, train_static_0_1]\n\n\n3\ninittransactioncode_186L\nTransaction type of the initial credit\ntransaction.\n[train_static_0_0, train_static_0_1]\n\n\n4\nlastst_736L\nStatus of the client's previous credit\napplication.\n[train_static_0_0, train_static_0_1]\n\n\n5\npaytype1st_925L\nType of first payment of the client.\n[train_static_0_0, train_static_0_1]\n\n\n6\npaytype_783L\nType of payment.\n[train_static_0_0, train_static_0_1]\n\n\n7\ntwobodfilling_608L\nType of application process.\n[train_static_0_0, train_static_0_1]\n\n\n8\ntypesuite_864L\nPersons accompanying the client during the loan\napplication process.\n[train_static_0_0, train_static_0_1]\n\n\n\n\n\n\n\nTime Sensitivity?\nMaybe not to include time-features in as training dataset.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tools\n\nd=(\n    ds.get('train_base')\n    .mutate(date_decision=_.date_decision.to_timestamp('%Y-%m-%d').date())\n    .mutate(dm=_.WEEK_NUM)\n    .group_by([_.dm, _.target])\n    .aggregate(n=_.count())\n    .to_pandas()\n)\n# d\n\nimport altair as alt\nc0=alt.Chart(\n    ds.get('train_base')\n        .mutate(date_decision=_\n                .date_decision\n                .to_timestamp('%Y-%m-%d').date()\n                .truncate('M')\n                )\n        .group_by([_.date_decision])\n        .aggregate(n=_.count())\n        .to_pandas()\n    ,\n    title=['Loan Approval Volumn',\n           'There is a sharp drope during covid'\n        ],height=150, width=670\n    ).mark_line(color='midnightblue').encode(\n        x=alt.X('date_decision',title='Decision Date'),\n        y=alt.Y('n',title='Loan Approved')\n)\nc1=alt.Chart(d,title='Volumn of Approval and Deafult').mark_area().encode(\n    x=alt.X('dm',title='Week Number'),\n    y=alt.Y('n',title='Number of Case'),\n    color='target:N'\n)\nc2=alt.Chart(d,title='Propotion of Default Overtime').mark_area().encode(\n    x=alt.X('dm',title='Week Number'),\n    y=alt.Y('n',title='Propotion of Case').stack('normalize'),\n    color='target:N'\n)\nc0 & (c1  | c2)\n\n\n\n\n\n\n\n\nreload(tools)\nfrom tools import FeatureDefinition\npretty_display(\n    FeatureDefinition().lookup_dsc('change')\n    .query('File==\"train_static_0_0\"')\n)\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nFile\nDescription\n\n\n\n\n415\nequalitydataagreement_891L\ntrain_static_0_0\nFlag indicating sudden changes in client's social-\ndemographic data (e.g. education, family status,\nhousing type).\n\n\n417\nequalityempfrom_62L\ntrain_static_0_0\nFlag indicating a sudden change in the client's\nlength of employment.\n\n\n\n\n\n\n\n\nExplore Categorical Variables\n\nfd=FeatureDefinition()\nintersting_cols = [\n    'equalitydataagreement_891L',\n    'equalityempfrom_62L',\n    'bankacctype_710L',\n    'credtype_322L','disbursementtype_67L',\n    'inittransactioncode_186L','lastst_736L','paytype1st_925L','paytype_783L',\n    'twobodfilling_608L','typesuite_864L',\n    'lastrejectreasonclient_4145040M'\n]\n# pretty_display(\n#     fd.lookup_col(intersting_cols)\n# )\nd2=(\n    ibis.union(\n        ds.get('train_static_0_0'),\n        ds.get('train_static_0_1')\n    )\n    .select(s.contains(intersting_cols + ['case_id']))\n    .join(ds.get('train_base'), ['case_id'])\n)\nC = []\nfor c in intersting_cols:\n    # display(d2.select(c,'target').head(1))\n    description = fd.lookup_col(c).Description.str.wrap(30).loc[0]\n    # print(description)\n    d=(d2\n        .group_by([c,'target'])\n        .aggregate(n=_.count())\n        .to_pandas())\n    ct1=alt.Chart(\n        d\n    ).mark_bar().encode(\n        y=alt.Y(c + ':N', title=description.split('\\n')),\n        x=alt.X('n:Q',title=''),\n        color='target:N'\n    )\n    ct2=alt.Chart(\n        d,width=100\n    ).mark_bar().encode(\n        y=alt.Y(c + ':N', title='', axis=None),\n        x=alt.X('n:Q',title='').stack('normalize'),\n        color='target:N',\n    )\n    ct=ct1 | ct2\n    C += [ct]\nreduce(alt.vconcat, C).configure_axisY(\n    titleAngle=0,\n    titleAlign=\"left\",\n    titleY=10,\n    titleX=-200,\n    titleColor='#404040',\n    titleFontWeight='lighter'\n).properties(\n    title=[\n        \"Categorical Variable Appears in Static Dataset\",\n        \"Nothing distinct by propotion except 'Status of the client's previous application'\"\n        ]\n)\n\n\n\n\n\n\n\n\npretty_display(fd\n               .lookup_tbl('static_0_0')\n               .query('Variable.str.endswith(\"M\")')\n               .reset_index(drop=True))\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nFile\nDescription\n\n\n\n\n0\nlastapprcommoditycat_1041M\ntrain_static_0_0\nCommodity category of the last loan applications\nmade by the applicant.\n\n\n1\nlastapprcommoditytypec_5251766M\ntrain_static_0_0\nCommodity type of the last application.\n\n\n2\nlastcancelreason_561M\ntrain_static_0_0\nCancellation reason of the last application.\n\n\n3\nlastrejectcommoditycat_161M\ntrain_static_0_0\nCategory of commodity in the applicant's last\nrejected application.\n\n\n4\nlastrejectcommodtypec_5251769M\ntrain_static_0_0\nCommodity type of the last rejected application.\n\n\n5\nlastrejectreason_759M\ntrain_static_0_0\nReason for rejection on the most recent rejected\napplication.\n\n\n6\nlastrejectreasonclient_4145040M\ntrain_static_0_0\nReason for the client's last loan rejection.\n\n\n7\npreviouscontdistrict_112M\ntrain_static_0_0\nContact district of the client's previous approved\napplication.\n\n\n\n\n\n\n\nd=(\n    ds.get('train_static_0_0')\n    .join(ds.get('train_base'),'case_id')\n    .select(s.endswith('M'), _.target, _.case_id)\n    .drop('WEEK_NUM')\n    # .pivot_longer(\n    #     s.endswith('M')\n    # )\n    # .group_by([_.name,_.value])\n    # .aggregate(n=_.count())\n)\n\n\n(d\n.select(c, 'target')\n.group_by([s.contains(c),s.contains('target')])\n.aggregate(n=_.count())\n.to_pandas())\n\n\n\n\n\n\n\n\n\npreviouscontdistrict_112M\ntarget\nn\n\n\n\n\n0\nP6_35_77\n0\n2506\n\n\n1\nP41_138_103\n0\n1766\n\n\n2\nP54_133_26\n0\n8831\n\n\n3\nP197_47_166\n0\n34040\n\n\n4\nP111_135_181\n0\n11957\n\n\n...\n...\n...\n...\n\n\n393\nP159_160_144\n1\n27\n\n\n394\nP217_60_135\n1\n16\n\n\n395\nP78_30_175\n1\n26\n\n\n396\nP7_110_89\n1\n18\n\n\n397\nP31_42_128\n1\n7\n\n\n\n\n398 rows × 3 columns\n\n\n\n\n\nfd=FeatureDefinition()\nmcols = [i for i in d.columns if i not in ['target', \"case_id\"]]\nC=[]\nfor c in mcols:\n    data=(d\n        .select(c, 'target')\n        .group_by([s.contains(c),s.contains('target')])\n        .aggregate(n=_.count())\n        .to_pandas())\n    description=fd.lookup_col(c).Description.str.wrap(30).loc[0]\n    cl1=(\n        alt\n        .Chart(data)\n        .mark_bar()\n        .encode(\n            x='n:Q',\n            y=alt.Y(c + ':N',title=description.split('\\n')),\n            color='target:N'\n        )\n    )\n    cl2=(\n        alt\n        .Chart(data,width=100)\n        .mark_bar(\n        )\n        .encode(\n            x=alt.X('n:Q',title='').stack('normalize'),\n            y=alt.Y(c + ':N',title='',axis=None),\n            color='target:N'\n        )\n    )\n    C+=[cl1 | cl2]\nCH=reduce(alt.vconcat,C)\nCH.configure_axisY(\n    titleAngle=0,\n    titleAlign=\"left\",\n    titleY=10,\n    titleX=-250,\n    titleColor='#404040',\n    titleFontWeight='lighter'\n).properties(\n    title='All Category Variable'\n)\n\n\n\n\n\n\n\n\n# alt.data_transformers.enable(\"vegafusion\")\n# alt.Chart(d.to_pandas()).transform_fold(\n#     [i for i in d.columns if i not in ['target', \"case_id\"]]\n# ).mark_bar().encode(\n#     x='count():Q',\n#     y='value:N',\n#     column='key:N'\n# )\nd\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┓\n┃ lastapprcommoditycat_1041M ┃ lastapprcommoditytypec_5251766M ┃ lastcancelreason_561M ┃ lastrejectcommoditycat_161M ┃ lastrejectcommodtypec_5251769M ┃ lastrejectreason_759M ┃ lastrejectreasonclient_4145040M ┃ previouscontdistrict_112M ┃ target ┃ case_id ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━┩\n│ string                     │ string                          │ string                │ string                      │ string                         │ string                │ string                          │ string                    │ int64  │ int64   │\n├────────────────────────────┼─────────────────────────────────┼───────────────────────┼─────────────────────────────┼────────────────────────────────┼───────────────────────┼─────────────────────────────────┼───────────────────────────┼────────┼─────────┤\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │       0 │\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │       2 │\n│ a55475b1                   │ a55475b1                        │ P94_109_143           │ a55475b1                    │ a55475b1                       │ P94_109_143           │ a55475b1                        │ a55475b1                  │      0 │       3 │\n│ a55475b1                   │ a55475b1                        │ P94_109_143           │ a55475b1                    │ a55475b1                       │ P94_109_143           │ a55475b1                        │ a55475b1                  │      0 │       6 │\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │       7 │\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │       8 │\n│ a55475b1                   │ a55475b1                        │ P73_130_169           │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │      10 │\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │      11 │\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │      12 │\n│ a55475b1                   │ a55475b1                        │ P94_109_143           │ a55475b1                    │ a55475b1                       │ P94_109_143           │ a55475b1                        │ a55475b1                  │      0 │      13 │\n│ …                          │ …                               │ …                     │ …                           │ …                              │ …                     │ …                               │ …                         │      … │       … │\n└────────────────────────────┴─────────────────────────────────┴───────────────────────┴─────────────────────────────┴────────────────────────────────┴───────────────────────┴─────────────────────────────────┴───────────────────────────┴────────┴─────────┘"
  },
  {
    "objectID": "03-15-Time-Series/r-book.html",
    "href": "03-15-Time-Series/r-book.html",
    "title": "Use R in a jupyter notebook?",
    "section": "",
    "text": "This notebook is writtten in jupyter!\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.0     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv lubridate 1.9.2     v tibble    3.1.8\nv purrr     1.0.1     v tidyr     1.3.0\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ndf = readr::read_csv('data/store-sales-time-series-forecasting/train.csv')\n\nRows: 3000888 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (1): family\ndbl  (4): id, store_nbr, sales, onpromotion\ndate (1): date\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndf |&gt; head()\n\n\n\nA tibble: 6 x 6\n\n\nid\ndate\nstore_nbr\nfamily\nsales\nonpromotion\n\n\n&lt;dbl&gt;\n&lt;date&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0\n2013-01-01\n1\nAUTOMOTIVE\n0\n0\n\n\n1\n2013-01-01\n1\nBABY CARE\n0\n0\n\n\n2\n2013-01-01\n1\nBEAUTY\n0\n0\n\n\n3\n2013-01-01\n1\nBEVERAGES\n0\n0\n\n\n4\n2013-01-01\n1\nBOOKS\n0\n0\n\n\n5\n2013-01-01\n1\nBREAD/BAKERY\n0\n0"
  },
  {
    "objectID": "05-19 Factor Analysis/lda.html",
    "href": "05-19 Factor Analysis/lda.html",
    "title": "Linear Discriminant Analysis",
    "section": "",
    "text": "Other name: Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher’s linear discriminant.\nFisher is a bigolgist who studies species of birds. (this story is hypothtical) Think in Fisher’s perspective I want to classify bird without measuring ratio…\n\n\nSeems sci-learn’s LDA don’t like this datasets\n\n\ngenerate two linear correlated group\n%config InlineBackend.figure_format='retina'\n# %matplotlib widget\n%matplotlib inline\n# change to widget for interactive\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\n\nsample_size = 100\nx1 = np.random.normal(scale = 1, size = sample_size)\ny1 = x1 * 0.2 + 3 + np.random.normal(scale=0.1, size=sample_size)\nz1 = 0.5 * x1 + y1 + np.random.normal(scale=0.1, size=sample_size)\n\nx2 = np.random.normal(scale = 1, size = sample_size)\ny2 = x2 * 0.6 + 2 + np.random.normal(scale=0.1, size=sample_size)\nz2 = 0.7 * x2 + 0.1 * y2 + np.random.normal(scale=0.1, size=sample_size)\n\nx3 = np.random.normal(scale = 1, size = sample_size)\ny3 = x3 + 1 + np.random.normal(scale=0.1, size=sample_size)\nz3 = 0.5 * x3 + 0.5 * y2 + np.random.normal(scale=0.1, size=sample_size)\n\nx = np.hstack((x1,x2\n               ,x3\n               ))\ny = np.hstack((y1,y2\n               ,y3\n               ))\nz = np.hstack((z1,z2\n               ,z3\n               ))\ng = np.hstack( (np.repeat(1,sample_size)\n              , np.repeat(0,sample_size)\n              , np.repeat(2,sample_size)\n              ))\n\nfigure = plt.figure()\n\nax=figure.add_subplot(111, projection=\"3d\")\nax.scatter(x,y,z, c=g)\nax.set_title(\"Project this Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(n_components=2)\n\nm = np.vstack((x,y,z)).T\nX_2d = lda.fit(m,g).transform(m)\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(X_2d[:,0],X_2d[:,1],c=g)\nax.set_title(\"LDA projection\")\n\nText(0.5, 1.0, 'LDA projection')\n\n\n\n\n\n\n\n\n\nThe effect of LDA or PCA is similar to seeing a 3D object with one eye: projection of higher dimension into one dimension.\n\n\n\n\nLinear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels…\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\n## fit model with principle component\npca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)\n\n## fit model with LDA\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_r2 = lda.fit(X, y).transform(X)\n\n## plog graph\nfig, ax = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\nax1=ax[0]\nax2=ax[1]\n\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\nlw = 2\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    ax1.scatter(\n        X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\n    )\nax1.legend(loc=\"best\", shadow=False, scatterpoints=1)\nax1.set_title(\"PCA of IRIS dataset\")\n\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    ax2.scatter(\n        X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name\n    )\nax2.legend(loc=\"best\", shadow=False, scatterpoints=1)\nax2.set_title(\"LDA of IRIS dataset\")\n\nfig.show()\n\n/var/folders/r5/1cdq52mn21zdnqzl0fvp44zw0000gn/T/ipykernel_98332/2715178903.py:43: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n\n\n\n\nThis following section it maybe useful for popup a few images of iris:\n\nFrom an artist’s perspective the differences between these iris’s are dimensionality of petal and sepal. The latend varible here is probably can view as (how square are these two petal)\n\n\n\nThis blog post thanks to Andy Jones\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate data\nmu1 = [0, 0]\nmu2 = [1, 2]\ncov = [[1, 0], [0, 1]]\nn1 = 500\nn2 = 50\nn = n1 + n2\nx1 = np.random.multivariate_normal(mean=mu1, cov=cov, size=n1)\nx2 = np.random.multivariate_normal(mean=mu2, cov=cov, size=n2)\nx = np.vstack([x1, x2])\ny = np.concatenate([np.repeat(1, n1), np.repeat(2, n2)])\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(x[:, 0], x[:, 1], c=y)\n\n\n\n\n\n\n\n\n\npi_hat_1 = n1 / n ## baysian prior also frequency of target\npi_hat_2 = n2 / n ## baysian prior also frequency of target\n\n## avg,expected_value,or position of normal\nmu_hat_1 = 1 / n1 * np.sum(x1, axis=0) #expected value for group1\nmu_hat_2 = 1 / n2 * np.sum(x2, axis=0) #expected value for group2\n\n# variance, or width of normal function\ncov_hat_1 = 1 / (n1 - 1) * np.matmul((x1 - mu_hat_1).T, (x1 - mu_hat_1)) # with-in group co-variance\ncov_hat_2 = 1 / (n2 - 1) * np.matmul((x2 - mu_hat_2).T, (x2 - mu_hat_2))\ncov_hat = (cov_hat_1 + cov_hat_2) / 2\n\n\n## defined log likelyhood function\ndef LL(x # any point pair\n       , mu # average value\n       , sigma # covariance\n       , prior):\n    dist = x - mu\n    cov_inv = np.linalg.inv(sigma) ## inversion of cov matrix\n    cov_det = np.linalg.det(sigma) ## determinant of cov matrix\n    return -1/2 * np.log(2 * np.pi * cov_det) - 1/2 * dist.T @ cov_inv @ dist + np.log(prior)\n\n## use this to test just so I simplify syntax of the function\n# assert LL_(point_grid[0],mu_hat_1, cov_hat, pi_hat_1) == LL(point_grid[0],mu_hat_1, cov_hat, pi_hat_1)\n## interestingly I also saw v.T * M * v.T in egen value + egen vector... so it reflect a common pattern of something? \n## `cov_inv @ dist` is actually \n\n\n\nIn short LL is “Stacked Normal Distribution”\nGiven probability density function of a draw \\(x_i\\) from a normal distribute function:\n\\[\nf(x_i; \\mu,\\sigma^2) = (2\\pi\\sigma^2)^\\frac{1}{2}\\exp(-\\frac{1}{2}\\frac{(x_i - \\mu)^2}{\\sigma^2})\n\\]\nThis function gives joint probability density function for give sample \\(\\xi=[x_1 ... x_n]\\). This is the Likelihood Function\n\\[\n\\begin{align}\nL(\\xi,\\mu,\\sigma^2) &= \\prod_{i}^{n}{f(x_i; \\mu,\\sigma^2)} \\\\\n&= (2\\pi\\sigma^2)^{-n/2}\\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}{(x_i-\\mu)^2} )\n\\end{align}\n\\]\nBecause the area under density function is one, the joint probability function of varible would be many of these PDF stacked together. The mental image for this is think of any vector (no matter the sequence). The probability of \\(\\xi={x_1 ... x_n}\\) occur follow above function and it doesn’t matter which order.\nThe Log-likelihood Function is just this function logged:\n\\[\n\\theta = (\\mu;\\sigma^2) \\\\\n\\begin{align}\nl(\\theta; \\xi) &= ln[(2\\pi\\sigma^2)^{-n/2}\\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}{(x_i-\\mu)^2} )] \\\\\n&= ln[(2\\pi\\sigma^2) + \\ln[exp(-\\frac{1}{2\\sigma^2} \\sum_i^n{(x_i -\\mu)^2} ))] \\\\\n&= -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n{(x_i - \\mu)^2}\n\\end{align}\n\\] The beauty of Log-Likelihood function is it canceled a lot of \\(\\exp\\) element of normal distribution denity function and instead given you an algibratic expression, which you can then carry on to express with Matrix.\nAn Optimisation Problem\nLog likelihoold function is used when \\(\\^{\\theta}\\) is unknown but a set of sample vector \\(\\xi\\) is. This became a maximisation problem that obtain a \\(\\^{\\theta}\\) that will result that achieve this result. In the language of Math this is written as:\n\\[\n\\^{\\theta} = \\arg \\max_{\\theta}l(\\theta; \\xi)\n\\]\n(source: thanks to statlect)\n\n\n\nLDA approaches the problem by assuming that the condition \\(p(x|y=0)\\) and \\(p(x|y=1)\\) are both normal distribution, with mean and covariance parameter \\((\\mu, \\sigma_0)\\) \\((\\mu. \\sigma_0)\\), under this assumption, the Bayes-optimal solution is to predict points as being from the second class if the log of likelihood ratio is bigger than some threshold \\(T\\)\n\n\nunfload to explain dist.T @ cov_inv @ dist\n## I find it confusing when down to one dimension vector\n## because NP consider it no difference vertical or horizontal\nnp.array([1,2]) @ np.array([[1,2],[3,4]]) ## so this is in fact consider first as horizontal\n# 1 * 1 + 2 * 3 , 1 * 2 + 2 * 4\n## this code below will result in different value\n# np.array([[1,2],[3,4]]) @ np.array([1,2])\n\nnp.array([1,2]) @ np.array([[1,2],[3,4]]) @ np.array([1,2]) \n## 7 * 1  + 10 * 2\n\n\n27\n\n\n\n\n\n\npoint_grid = np.mgrid[-10:10.1:0.5, -10:10.1:0.5].reshape(2, -1).T\nll_vals_1 = [LL(x, mu_hat_1, cov_hat, pi_hat_1) for x in point_grid]\nll_vals_2 = [LL(x, mu_hat_2, cov_hat, pi_hat_2) for x in point_grid]\n\n\npoint_grid\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(point_grid[:, 0], point_grid[:, 1], c=ll_vals_1,marker=\"s\")\nax.plot(mu_hat_1[0], mu_hat_1[1], 'k-^', markersize=14)\n# ax.colorbar()\nax.set_title(\"Log likely hood function\")\n\nText(0.5, 1.0, 'Log likely hood function')\n\n\n\n\n\n\n\n\n\n\ncov_hat\n\narray([[1.19308602, 0.01970993],\n       [0.01970993, 1.0835684 ]])\n\n\n\n\ndef abline(slope, intercept):\n    \"\"\"Plot a line from slope and intercept\"\"\"\n    axes = plt.gca()\n    x_vals = np.array(axes.get_xlim())\n    y_vals = intercept + slope * x_vals\n    plt.plot(x_vals, y_vals, '--')\n    \ncov_inv = np.linalg.inv(cov_hat)\n\n# slope\nslope_vec = cov_inv @ (mu_hat_1 - mu_hat_2)\nslope = -slope_vec[0] / slope_vec[1]\n\n# intercept\nintercept_partial = (\n      np.log(pi_hat_2) - np.log(pi_hat_1) \n    + 0.5 * (mu_hat_1.T @ cov_inv @ mu_hat_1) \n    - 0.5 * (mu_hat_2.T @ cov_inv @ mu_hat_2))\nintercept = intercept_partial / slope_vec[1]\n\n# plotting\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(x[:, 0], x[:, 1], c=y)\nabline(slope, intercept)\nax.plot(mu_hat_1[0], mu_hat_1[1], 'rp', markersize=14)\nax.plot(mu_hat_2[0], mu_hat_2[1], 'rp', markersize=14)\n\n\n\n\n\n\n\n\n\nnp.linalg.inv(cov_hat_1 + cov_hat_2) @ \n\narray([[ 0.41920724, -0.00762531],\n       [-0.00762531,  0.46157704]])\n\n\n\n\n\n\n\n\n\n# tip mash gird does this\nnp.mgrid[1:2.5:0.5, 1:2.5:0.5]# \n\narray([[[1. , 1. , 1. ],\n        [1.5, 1.5, 1.5],\n        [2. , 2. , 2. ]],\n\n       [[1. , 1.5, 2. ],\n        [1. , 1.5, 2. ],\n        [1. , 1.5, 2. ]]])\n\n\n\nnp.mgrid[1:2.5:0.5, 1:2.5:0.5].reshape(2,-1).T\n\narray([[1. , 1. ],\n       [1. , 1.5],\n       [1. , 2. ],\n       [1.5, 1. ],\n       [1.5, 1.5],\n       [1.5, 2. ],\n       [2. , 1. ],\n       [2. , 1.5],\n       [2. , 2. ]])"
  },
  {
    "objectID": "05-19 Factor Analysis/lda.html#background-of-the-analysis---sir-ronald-fisher-1939",
    "href": "05-19 Factor Analysis/lda.html#background-of-the-analysis---sir-ronald-fisher-1939",
    "title": "Linear Discriminant Analysis",
    "section": "",
    "text": "Other name: Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher’s linear discriminant.\nFisher is a bigolgist who studies species of birds. (this story is hypothtical) Think in Fisher’s perspective I want to classify bird without measuring ratio…\n\n\nSeems sci-learn’s LDA don’t like this datasets\n\n\ngenerate two linear correlated group\n%config InlineBackend.figure_format='retina'\n# %matplotlib widget\n%matplotlib inline\n# change to widget for interactive\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\n\nsample_size = 100\nx1 = np.random.normal(scale = 1, size = sample_size)\ny1 = x1 * 0.2 + 3 + np.random.normal(scale=0.1, size=sample_size)\nz1 = 0.5 * x1 + y1 + np.random.normal(scale=0.1, size=sample_size)\n\nx2 = np.random.normal(scale = 1, size = sample_size)\ny2 = x2 * 0.6 + 2 + np.random.normal(scale=0.1, size=sample_size)\nz2 = 0.7 * x2 + 0.1 * y2 + np.random.normal(scale=0.1, size=sample_size)\n\nx3 = np.random.normal(scale = 1, size = sample_size)\ny3 = x3 + 1 + np.random.normal(scale=0.1, size=sample_size)\nz3 = 0.5 * x3 + 0.5 * y2 + np.random.normal(scale=0.1, size=sample_size)\n\nx = np.hstack((x1,x2\n               ,x3\n               ))\ny = np.hstack((y1,y2\n               ,y3\n               ))\nz = np.hstack((z1,z2\n               ,z3\n               ))\ng = np.hstack( (np.repeat(1,sample_size)\n              , np.repeat(0,sample_size)\n              , np.repeat(2,sample_size)\n              ))\n\nfigure = plt.figure()\n\nax=figure.add_subplot(111, projection=\"3d\")\nax.scatter(x,y,z, c=g)\nax.set_title(\"Project this Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(n_components=2)\n\nm = np.vstack((x,y,z)).T\nX_2d = lda.fit(m,g).transform(m)\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(X_2d[:,0],X_2d[:,1],c=g)\nax.set_title(\"LDA projection\")\n\nText(0.5, 1.0, 'LDA projection')\n\n\n\n\n\n\n\n\n\nThe effect of LDA or PCA is similar to seeing a 3D object with one eye: projection of higher dimension into one dimension.\n\n\n\n\nLinear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels…\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\n## fit model with principle component\npca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)\n\n## fit model with LDA\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_r2 = lda.fit(X, y).transform(X)\n\n## plog graph\nfig, ax = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\nax1=ax[0]\nax2=ax[1]\n\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\nlw = 2\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    ax1.scatter(\n        X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\n    )\nax1.legend(loc=\"best\", shadow=False, scatterpoints=1)\nax1.set_title(\"PCA of IRIS dataset\")\n\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    ax2.scatter(\n        X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name\n    )\nax2.legend(loc=\"best\", shadow=False, scatterpoints=1)\nax2.set_title(\"LDA of IRIS dataset\")\n\nfig.show()\n\n/var/folders/r5/1cdq52mn21zdnqzl0fvp44zw0000gn/T/ipykernel_98332/2715178903.py:43: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n\n\n\n\nThis following section it maybe useful for popup a few images of iris:\n\nFrom an artist’s perspective the differences between these iris’s are dimensionality of petal and sepal. The latend varible here is probably can view as (how square are these two petal)\n\n\n\nThis blog post thanks to Andy Jones\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate data\nmu1 = [0, 0]\nmu2 = [1, 2]\ncov = [[1, 0], [0, 1]]\nn1 = 500\nn2 = 50\nn = n1 + n2\nx1 = np.random.multivariate_normal(mean=mu1, cov=cov, size=n1)\nx2 = np.random.multivariate_normal(mean=mu2, cov=cov, size=n2)\nx = np.vstack([x1, x2])\ny = np.concatenate([np.repeat(1, n1), np.repeat(2, n2)])\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(x[:, 0], x[:, 1], c=y)\n\n\n\n\n\n\n\n\n\npi_hat_1 = n1 / n ## baysian prior also frequency of target\npi_hat_2 = n2 / n ## baysian prior also frequency of target\n\n## avg,expected_value,or position of normal\nmu_hat_1 = 1 / n1 * np.sum(x1, axis=0) #expected value for group1\nmu_hat_2 = 1 / n2 * np.sum(x2, axis=0) #expected value for group2\n\n# variance, or width of normal function\ncov_hat_1 = 1 / (n1 - 1) * np.matmul((x1 - mu_hat_1).T, (x1 - mu_hat_1)) # with-in group co-variance\ncov_hat_2 = 1 / (n2 - 1) * np.matmul((x2 - mu_hat_2).T, (x2 - mu_hat_2))\ncov_hat = (cov_hat_1 + cov_hat_2) / 2\n\n\n## defined log likelyhood function\ndef LL(x # any point pair\n       , mu # average value\n       , sigma # covariance\n       , prior):\n    dist = x - mu\n    cov_inv = np.linalg.inv(sigma) ## inversion of cov matrix\n    cov_det = np.linalg.det(sigma) ## determinant of cov matrix\n    return -1/2 * np.log(2 * np.pi * cov_det) - 1/2 * dist.T @ cov_inv @ dist + np.log(prior)\n\n## use this to test just so I simplify syntax of the function\n# assert LL_(point_grid[0],mu_hat_1, cov_hat, pi_hat_1) == LL(point_grid[0],mu_hat_1, cov_hat, pi_hat_1)\n## interestingly I also saw v.T * M * v.T in egen value + egen vector... so it reflect a common pattern of something? \n## `cov_inv @ dist` is actually \n\n\n\nIn short LL is “Stacked Normal Distribution”\nGiven probability density function of a draw \\(x_i\\) from a normal distribute function:\n\\[\nf(x_i; \\mu,\\sigma^2) = (2\\pi\\sigma^2)^\\frac{1}{2}\\exp(-\\frac{1}{2}\\frac{(x_i - \\mu)^2}{\\sigma^2})\n\\]\nThis function gives joint probability density function for give sample \\(\\xi=[x_1 ... x_n]\\). This is the Likelihood Function\n\\[\n\\begin{align}\nL(\\xi,\\mu,\\sigma^2) &= \\prod_{i}^{n}{f(x_i; \\mu,\\sigma^2)} \\\\\n&= (2\\pi\\sigma^2)^{-n/2}\\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}{(x_i-\\mu)^2} )\n\\end{align}\n\\]\nBecause the area under density function is one, the joint probability function of varible would be many of these PDF stacked together. The mental image for this is think of any vector (no matter the sequence). The probability of \\(\\xi={x_1 ... x_n}\\) occur follow above function and it doesn’t matter which order.\nThe Log-likelihood Function is just this function logged:\n\\[\n\\theta = (\\mu;\\sigma^2) \\\\\n\\begin{align}\nl(\\theta; \\xi) &= ln[(2\\pi\\sigma^2)^{-n/2}\\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}{(x_i-\\mu)^2} )] \\\\\n&= ln[(2\\pi\\sigma^2) + \\ln[exp(-\\frac{1}{2\\sigma^2} \\sum_i^n{(x_i -\\mu)^2} ))] \\\\\n&= -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n{(x_i - \\mu)^2}\n\\end{align}\n\\] The beauty of Log-Likelihood function is it canceled a lot of \\(\\exp\\) element of normal distribution denity function and instead given you an algibratic expression, which you can then carry on to express with Matrix.\nAn Optimisation Problem\nLog likelihoold function is used when \\(\\^{\\theta}\\) is unknown but a set of sample vector \\(\\xi\\) is. This became a maximisation problem that obtain a \\(\\^{\\theta}\\) that will result that achieve this result. In the language of Math this is written as:\n\\[\n\\^{\\theta} = \\arg \\max_{\\theta}l(\\theta; \\xi)\n\\]\n(source: thanks to statlect)\n\n\n\nLDA approaches the problem by assuming that the condition \\(p(x|y=0)\\) and \\(p(x|y=1)\\) are both normal distribution, with mean and covariance parameter \\((\\mu, \\sigma_0)\\) \\((\\mu. \\sigma_0)\\), under this assumption, the Bayes-optimal solution is to predict points as being from the second class if the log of likelihood ratio is bigger than some threshold \\(T\\)\n\n\nunfload to explain dist.T @ cov_inv @ dist\n## I find it confusing when down to one dimension vector\n## because NP consider it no difference vertical or horizontal\nnp.array([1,2]) @ np.array([[1,2],[3,4]]) ## so this is in fact consider first as horizontal\n# 1 * 1 + 2 * 3 , 1 * 2 + 2 * 4\n## this code below will result in different value\n# np.array([[1,2],[3,4]]) @ np.array([1,2])\n\nnp.array([1,2]) @ np.array([[1,2],[3,4]]) @ np.array([1,2]) \n## 7 * 1  + 10 * 2\n\n\n27\n\n\n\n\n\n\npoint_grid = np.mgrid[-10:10.1:0.5, -10:10.1:0.5].reshape(2, -1).T\nll_vals_1 = [LL(x, mu_hat_1, cov_hat, pi_hat_1) for x in point_grid]\nll_vals_2 = [LL(x, mu_hat_2, cov_hat, pi_hat_2) for x in point_grid]\n\n\npoint_grid\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(point_grid[:, 0], point_grid[:, 1], c=ll_vals_1,marker=\"s\")\nax.plot(mu_hat_1[0], mu_hat_1[1], 'k-^', markersize=14)\n# ax.colorbar()\nax.set_title(\"Log likely hood function\")\n\nText(0.5, 1.0, 'Log likely hood function')\n\n\n\n\n\n\n\n\n\n\ncov_hat\n\narray([[1.19308602, 0.01970993],\n       [0.01970993, 1.0835684 ]])\n\n\n\n\ndef abline(slope, intercept):\n    \"\"\"Plot a line from slope and intercept\"\"\"\n    axes = plt.gca()\n    x_vals = np.array(axes.get_xlim())\n    y_vals = intercept + slope * x_vals\n    plt.plot(x_vals, y_vals, '--')\n    \ncov_inv = np.linalg.inv(cov_hat)\n\n# slope\nslope_vec = cov_inv @ (mu_hat_1 - mu_hat_2)\nslope = -slope_vec[0] / slope_vec[1]\n\n# intercept\nintercept_partial = (\n      np.log(pi_hat_2) - np.log(pi_hat_1) \n    + 0.5 * (mu_hat_1.T @ cov_inv @ mu_hat_1) \n    - 0.5 * (mu_hat_2.T @ cov_inv @ mu_hat_2))\nintercept = intercept_partial / slope_vec[1]\n\n# plotting\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(x[:, 0], x[:, 1], c=y)\nabline(slope, intercept)\nax.plot(mu_hat_1[0], mu_hat_1[1], 'rp', markersize=14)\nax.plot(mu_hat_2[0], mu_hat_2[1], 'rp', markersize=14)\n\n\n\n\n\n\n\n\n\nnp.linalg.inv(cov_hat_1 + cov_hat_2) @ \n\narray([[ 0.41920724, -0.00762531],\n       [-0.00762531,  0.46157704]])\n\n\n\n\n\n\n\n\n\n# tip mash gird does this\nnp.mgrid[1:2.5:0.5, 1:2.5:0.5]# \n\narray([[[1. , 1. , 1. ],\n        [1.5, 1.5, 1.5],\n        [2. , 2. , 2. ]],\n\n       [[1. , 1.5, 2. ],\n        [1. , 1.5, 2. ],\n        [1. , 1.5, 2. ]]])\n\n\n\nnp.mgrid[1:2.5:0.5, 1:2.5:0.5].reshape(2,-1).T\n\narray([[1. , 1. ],\n       [1. , 1.5],\n       [1. , 2. ],\n       [1.5, 1. ],\n       [1.5, 1.5],\n       [1.5, 2. ],\n       [2. , 1. ],\n       [2. , 1.5],\n       [2. , 2. ]])"
  }
]
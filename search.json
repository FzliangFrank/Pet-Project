[
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html",
    "title": "Frank Vs Iceberg",
    "section": "",
    "text": "Just another Frank trying x database stuff;"
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#introduction",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#introduction",
    "title": "Frank Vs Iceberg",
    "section": "Introduction",
    "text": "Introduction\nApache Iceberg manages mutable data across all different sessions; This blog followed the official documentation to install a docker container and test Apache Iceberg’s capability to version control data.\n\nMore Context\n\nWAP: Write - Audit - Publish, is a data engineering pattern by Netflix Engineer Michelle Winters.\nCDC: Change Data Capture is “a design pattern used in databases and data processing to track and capture data changes—such as insertions, updates, and deletions—in real-time”. A Blog About CDC Using Iceberg"
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#lets-get-started",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#lets-get-started",
    "title": "Frank Vs Iceberg",
    "section": "Let’s Get Started!",
    "text": "Let’s Get Started!\n\nStep-1: Copy 100% tutorial material to spin up a docker container\nI AM …. a new me now. After x amout of deployment failure and thousands hour of frustration, such a vague concept such as “docker” & “server” starts to make sense to me (insert a meme here).\nCopy docker-compose.yml here: https://iceberg.apache.org/spark-quickstart/#docker-compose.\n\n\nStep-2: Wait a long agony of time…\n\nAHHH\nYour patient has finally paid off. Lets actually actually get started.\nFor interacting with this database using jupyter notebook: &gt; You can also use the notebook server available at http://localhost:8888\n(NOTE: YOU cannot actually use the notebook server)"
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#ops",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#ops",
    "title": "Frank Vs Iceberg",
    "section": "Ops",
    "text": "Ops\nOnce this container is build you should use;\ndocker compose start\nTo pause from runing you should use;\ndocker compose pause"
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#read-only-query",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#read-only-query",
    "title": "Frank Vs Iceberg",
    "section": "Read only Query",
    "text": "Read only Query\nSpark will automatically connected to this database.\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"demo\").getOrCreate()\nspark\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/02 20:13:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\n\n            \n                SparkSession - in-memory\n                \n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.5.5\n              Master\n                local[*]\n              AppName\n                demo\n            \n        \n        \n            \n        \n\n\nThis spark session seems to be telling me that session is in memory and will not persist into the database?\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\nspark\n\n\n            \n                SparkSession - in-memory\n                \n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.5.5\n              Master\n                local[*]\n              AppName\n                demo\n            \n        \n        \n            \n        \n\n\nIn one of the browser notebook (interestingly hidden from you).\n\ndf = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2021-04.parquet\")\ndf.write.saveAsTable(\"nyc.taxis\")\n\n                                                                                \n\n\n\nQuery Example Data From Parquet\nA WONDERFUL IN NOTEBOOK SQL\n\n%%sql\nSELECT * FROM demo.nyc.taxis\n--ORDER BY random()\nLIMIT 3\n\n25/04/02 19:20:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \n\n\n\n\n\n\nVendorID\ntpep_pickup_datetime\ntpep_dropoff_datetime\npassenger_count\ntrip_distance\nRatecodeID\nstore_and_fwd_flag\nPULocationID\nDOLocationID\npayment_type\nfare_amount\nextra\nmta_tax\ntip_amount\ntolls_amount\nimprovement_surcharge\ntotal_amount\ncongestion_surcharge\nairport_fee\n\n\n\n\n1\n2021-04-01 00:00:18\n2021-04-01 00:21:54\n1.0\n8.4\n1.0\nN\n79\n116\n1\n25.5\n3.0\n0.5\n5.85\n0.0\n0.3\n35.15\n2.5\n0.0\n\n\n1\n2021-04-01 00:42:37\n2021-04-01 00:46:23\n1.0\n0.9\n1.0\nN\n75\n236\n2\n5.0\n3.0\n0.5\n0.0\n0.0\n0.3\n8.8\n2.5\n0.0\n\n\n1\n2021-04-01 00:57:56\n2021-04-01 01:08:22\n1.0\n3.4\n1.0\nN\n236\n168\n2\n11.5\n3.0\n0.5\n0.0\n0.0\n0.3\n15.3\n2.5\n0.0\n\n\n\n\n\n\nANOTHER SQL\n\nspark.sql(\"SELECT * FROM demo.nyc.taxis LIMIT 3\").show()\n\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|       1| 2021-04-01 00:00:18|  2021-04-01 00:21:54|            1.0|          8.4|       1.0|                 N|          79|         116|           1|       25.5|  3.0|    0.5|      5.85|         0.0|                  0.3|       35.15|                 2.5|        0.0|\n|       1| 2021-04-01 00:42:37|  2021-04-01 00:46:23|            1.0|          0.9|       1.0|                 N|          75|         236|           2|        5.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         8.8|                 2.5|        0.0|\n|       1| 2021-04-01 00:57:56|  2021-04-01 01:08:22|            1.0|          3.4|       1.0|                 N|         236|         168|           2|       11.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        15.3|                 2.5|        0.0|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n\n\n\nNAIVE PYTHON OBJECT\n\nspark.table(\"nyc.taxis\").limit(3).show()\n\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|       1| 2021-04-01 00:00:18|  2021-04-01 00:21:54|            1.0|          8.4|       1.0|                 N|          79|         116|           1|       25.5|  3.0|    0.5|      5.85|         0.0|                  0.3|       35.15|                 2.5|        0.0|\n|       1| 2021-04-01 00:42:37|  2021-04-01 00:46:23|            1.0|          0.9|       1.0|                 N|          75|         236|           2|        5.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         8.8|                 2.5|        0.0|\n|       1| 2021-04-01 00:57:56|  2021-04-01 01:08:22|            1.0|          3.4|       1.0|                 N|         236|         168|           2|       11.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        15.3|                 2.5|        0.0|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n\n\n\n\n%%sql\nWITH d1 as (\n    SELECT VendorID FROM nyc.taxis\n)\nSELECT * FROM nyc.taxis\nLIMIT 2\n\n\n\n\n\nVendorID\ntpep_pickup_datetime\ntpep_dropoff_datetime\npassenger_count\ntrip_distance\nRatecodeID\nstore_and_fwd_flag\nPULocationID\nDOLocationID\npayment_type\nfare_amount\nextra\nmta_tax\ntip_amount\ntolls_amount\nimprovement_surcharge\ntotal_amount\ncongestion_surcharge\nairport_fee\n\n\n\n\n1\n2021-04-01 00:00:18\n2021-04-01 00:21:54\n1.0\n8.4\n1.0\nN\n79\n116\n1\n25.5\n3.0\n0.5\n5.85\n0.0\n0.3\n35.15\n2.5\n0.0\n\n\n1\n2021-04-01 00:42:37\n2021-04-01 00:46:23\n1.0\n0.9\n1.0\nN\n75\n236\n2\n5.0\n3.0\n0.5\n0.0\n0.0\n0.3\n8.8\n2.5\n0.0\n\n\n\n\n\n\n\n\nTry Access Files\n\n\nimport pandas as pd\ntry:\n    pd.read_csv(\"../I'm outside/flue-trends.csv\")\nexcept FileNotFoundError:\n    print(\"You cannot access this file because you've been domed outside container\")\n\n# pd.read_csv(\"warehouse/I'm inside/flue-trends.csv\")\nimport os\nprint(os.getcwd())\n\ntry:\n    flue_trends_csv = pd.read_csv(\"../warehouse/I'm inside/flu-trends.csv\")\nexcept FileNotFoundError:\n    print(\"Still can't find this file\")\n\nflue_trends_csv.columns = flue_trends_csv.columns.str.replace('(?&lt;=[a-z])(?=[A-Z])', '_', regex=True).str.lower()\nflue_trends_csv[['week', 'flu_visits']].head()\n\nYou cannot access this file because you've been domed outside container\n/home/iceberg/notebooks\n\n\n\n\n\n\n\n\n\n\nweek\nflu_visits\n\n\n\n\n0\n2009-06-29/2009-07-05\n180\n\n\n1\n2009-07-06/2009-07-12\n115\n\n\n2\n2009-07-13/2009-07-19\n132\n\n\n3\n2009-07-20/2009-07-26\n109\n\n\n4\n2009-07-27/2009-08-02\n120"
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#time-travel-feature-explore",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#time-travel-feature-explore",
    "title": "Frank Vs Iceberg",
    "section": "Time travel Feature Explore",
    "text": "Time travel Feature Explore\n\n%%sql\nSELECT *\nFROM nyc.taxis.history\n\n\n\n\n\nmade_current_at\nsnapshot_id\nparent_id\nis_current_ancestor\n\n\n\n\n2025-04-02 19:20:50.945000\n7406729593392585287\nNone\nTrue\n\n\n\n\n\n\n\n%%sql\nALTER TABLE nyc.taxis RENAME COLUMN fare_amount TO fare;\n\n\n\n\n\n\n\n\n\n\n%%sql\nALTER TABLE nyc.taxis RENAME COLUMN trip_distance TO distance;\n\n\n\n\n\n\n\n\n\n\n%%sql\nALTER TABLE nyc.taxis ALTER COLUMN distance COMMENT 'The elapsed trip distance in miles reported by the taximeter.';\n\n\n\n\n\n\n\n\n\n\n%%sql\nALTER TABLE nyc.taxis ALTER COLUMN distance TYPE double;\n\n\n\n\n\n\n\n\n\n\n\n%%sql\nALTER TABLE nyc.taxis ALTER COLUMN distance AFTER fare;\n\n\n\n\n\n\n\n\n\n\n%%sql\nALTER TABLE nyc.taxis\nADD COLUMN fare_per_distance_unit float AFTER distance;\n\n\n\n\n\n\n\n\n\n\n%%sql\nUPDATE nyc.taxis\nSET fare_per_distance_unit = fare/distance\n\n                                                                                \n\n\n\n\n\n\n\n\n\n\n\n%%sql\nSELECT *\nFROM nyc.taxis.history\n\n\n\n\n\nmade_current_at\nsnapshot_id\nparent_id\nis_current_ancestor\n\n\n\n\n2025-04-02 19:20:50.945000\n7406729593392585287\nNone\nTrue\n\n\n2025-04-02 19:21:47.891000\n3048435964784048077\n7406729593392585287\nTrue\n\n\n\n\n\n\n\nspark.table(\"nyc.taxis\").write.saveAsTable(\"nyc.taxis_01\")\n\n                                                                                \n\n\nYou can rollback to sepecific checkpoint here:\noriginal_snapshot = df.head().snapshot_id\nspark.sql(f\"CALL system.rollback_to_snapshot('nyc.taxis', {original_snapshot})\")\noriginal_snapshot"
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#pyiceberg-without-docker",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#pyiceberg-without-docker",
    "title": "Frank Vs Iceberg",
    "section": "Pyiceberg without Docker??",
    "text": "Pyiceberg without Docker??\nYou do not need to run iceberg in docker afterall; there is also pyiceberg. Although this seems to mean that the spark are not automatically configured for you anymore, nor the sparkSQL… But the data are hard written on the desk down instead of inside a container.\nHelps understand apache iceberg better.\n\nIceberg leverages the catalog to have one centralized place to organize the tables. This can be a traditional Hive catalog to store your Iceberg tables next to the rest, a vendor solution like the AWS Glue catalog, or an implementation of Icebergs’ own REST protocol. Checkout the configuration page to find all the configuration details.\n\n\nFor the sake of demonstration, we’ll configure the catalog to use the SqlCatalog implementation, which will store information in a local sqlite database. We’ll also configure the catalog to store data files in the local filesystem instead of an object store. This should not be used in production due to the limited scalability.\n\n\nCreate a temporary location for Iceberg\nDownload example parquet file, then “parquet” &gt;&gt; “arrow” &gt;&gt; “iceberg (sqlite)”\n\n\nfrom pyiceberg.catalog import load_catalog\n\nwarehouse_path = \"/tmp/warehouse\"\nimport os \nimport pyarrow.parquet as pq\nos.makedirs(warehouse_path, exist_ok=True) # this make directory in the root \ncatalog = load_catalog(\n    \"default\",\n    **{\n        'type': 'sql',\n        \"uri\": f\"sqlite:///{warehouse_path}/pyiceberg_catalog.db\",\n        \"warehouse\": f\"file://{warehouse_path}\", # path seems to be system path not relative path\n    },\n)\nos.system(f\"ls {warehouse_path}\")\n# os.system(f\"curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet -o /tmp/yellow_tripdata_2023-01.parquet\")\ndf = pq.read_table(\"/tmp/yellow_tripdata_2023-01.parquet\")\n\ndefault.db\npyiceberg_catalog.db\n\n\nCreate a “pyiceberg.Table” in the “catalog”\n\n\ncatalog.create_namespace_if_not_exists(\"default\")\n\ntable = catalog.create_table_if_not_exists(\n    \"default.taxi_dataset\",\n    schema=df.schema,\n)\ntable.overwrite(df)\nlen(table.scan().to_arrow())\n\n3066766\n\n\nWhat you will find is this “catalog” is not just any catalog, but an “SQL” catalog; interestingly this table have an “engine” attribute\n\nprint(type(table.catalog))\nprint(type(table.catalog.engine))\nprint(table.catalog.engine)\n\n&lt;class 'pyiceberg.catalog.sql.SqlCatalog'&gt;\n&lt;class 'sqlalchemy.engine.base.Engine'&gt;\nEngine(sqlite:////tmp/warehouse/pyiceberg_catalog.db)\n\n\nThis engine apears to just connect to a pyiceberg_catalog.db\n\n\n# materalize table into polars\npl_tbl = table.to_polars()\npl_tbl.head(1).collect()\n\n\n\nshape: (1, 20)\n\n\n\nVendorID\ntpep_pickup_datetime\ntpep_dropoff_datetime\npassenger_count\ntrip_distance\nRatecodeID\nstore_and_fwd_flag\nPULocationID\nDOLocationID\npayment_type\nfare_amount\nextra\nmta_tax\ntip_amount\ntolls_amount\nimprovement_surcharge\ntotal_amount\ncongestion_surcharge\nairport_fee\ntip_per_mile\n\n\ni64\ndatetime[μs]\ndatetime[μs]\nf64\nf64\nf64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2\n2023-01-01 00:32:10\n2023-01-01 00:40:36\n1.0\n0.97\n1.0\n\"N\"\n161\n141\n2\n9.3\n1.0\n0.5\n0.0\n0.0\n1.0\n14.3\n2.5\n0.0\nnull\n\n\n\n\n\n\n\nInspect what file hidden underneath:\n\nos.system(f\"find {warehouse_path}\")\n\n/tmp/warehouse\n/tmp/warehouse/pyiceberg_catalog.db\n/tmp/warehouse/default.db\n/tmp/warehouse/default.db/.DS_Store\n/tmp/warehouse/default.db/taxi_dataset\n/tmp/warehouse/default.db/taxi_dataset/.DS_Store\n/tmp/warehouse/default.db/taxi_dataset/data\n/tmp/warehouse/default.db/taxi_dataset/data/00000-0-1325772d-dff1-4f2f-974c-edfd05a605ff.parquet\n/tmp/warehouse/default.db/taxi_dataset/data/00000-0-c31713f9-495d-49d4-bac1-a524055b4e6c.parquet\n/tmp/warehouse/default.db/taxi_dataset/data/00000-0-2ab9f073-afd3-4230-8099-3230b0f2022b.parquet\n/tmp/warehouse/default.db/taxi_dataset/data/00000-0-83bcb64b-c3cb-49f4-9f31-e09a72ad3e78.parquet\n/tmp/warehouse/default.db/taxi_dataset/data/00000-0-6156df4b-c77e-434d-ad3f-570d5e329177.parquet\n/tmp/warehouse/default.db/taxi_dataset/data/00000-0-382f3c63-910d-45ec-aa51-73b6cd1c0606.parquet\n/tmp/warehouse/default.db/taxi_dataset/metadata\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-3891282249768133214-0-80630ce9-5d18-4ddb-a186-18d05771e1bf.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-8332539041388041675-0-1325772d-dff1-4f2f-974c-edfd05a605ff.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/00000-bf5d282c-a785-4462-af96-ccf03569fbb3.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/6156df4b-c77e-434d-ad3f-570d5e329177-m0.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/80630ce9-5d18-4ddb-a186-18d05771e1bf-m0.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/382f3c63-910d-45ec-aa51-73b6cd1c0606-m0.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/00006-58be9c38-c7dd-492f-a702-fa5078b4badc.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/07e37869-2330-426f-8caf-a1a850a46153-m0.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/00000-8e73bc4a-da95-4fc6-b42c-4a0bd25cb46c.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/00000-07fd1c67-1bc2-40db-a135-774921d05708.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/00003-663de7ae-4817-4d31-965f-a760f79503b1.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/3258e171-b32a-4e4b-80a7-2eb9ec943d90-m0.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-1741698885604984058-0-c31713f9-495d-49d4-bac1-a524055b4e6c.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/00002-10ee703a-d74e-45b4-a0f9-c96f34856bd4.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/83bcb64b-c3cb-49f4-9f31-e09a72ad3e78-m0.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-2936152946567510319-0-2ab9f073-afd3-4230-8099-3230b0f2022b.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-1383213220069637644-0-07e37869-2330-426f-8caf-a1a850a46153.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-8836172998645175224-0-28905221-63a2-45e0-9bdd-2214d1f9d67c.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/00004-8159a2ba-b78a-46c1-a9a6-f9678cedc72a.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/00001-b40c3060-eeb0-482e-98e7-f6a27cf66291.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/28905221-63a2-45e0-9bdd-2214d1f9d67c-m0.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/00000-2d906f83-1beb-471f-a8d6-f7420f256af4.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-4175663551801811657-0-382f3c63-910d-45ec-aa51-73b6cd1c0606.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/00000-cd5bb105-aba1-4131-8318-b049a11d3688.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/1325772d-dff1-4f2f-974c-edfd05a605ff-m0.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-5618975270792805759-0-6156df4b-c77e-434d-ad3f-570d5e329177.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-8976772202933927568-0-3258e171-b32a-4e4b-80a7-2eb9ec943d90.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-3834561067045940934-0-83bcb64b-c3cb-49f4-9f31-e09a72ad3e78.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/c31713f9-495d-49d4-bac1-a524055b4e6c-m0.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/snap-9058209337445030832-0-9ee57591-ccfb-4fef-b9d4-9b28f73f8252.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/00007-c9ae85c0-5221-420a-99a8-38e967ebffca.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/00005-4add0790-17bb-4bd0-8782-f17c64319240.metadata.json\n/tmp/warehouse/default.db/taxi_dataset/metadata/2ab9f073-afd3-4230-8099-3230b0f2022b-m0.avro\n/tmp/warehouse/default.db/taxi_dataset/metadata/9ee57591-ccfb-4fef-b9d4-9b28f73f8252-m0.avro\n\n\n0\n\n\nThe actual data are in fact stored within a set of “.parquet” files and the metadata are in set of “json”;\n\nfrom pathlib import Path\nimport pandas as pd\nimport sqlite3\ndb_path = str(Path(warehouse_path) / 'pyiceberg_catalog.db')\nwith sqlite3.connect(db_path) as conn:\n    display(pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn))\n    display(pd.read_sql_query(\"SELECT * FROM iceberg_tables;\", conn))\n\n# db_path = str([*Path(warehouse_path).glob('default.db')][0])\n# with sqlite3.connect(db_path) as conn:\n    # display(pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn))\n\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\niceberg_tables\n\n\n1\niceberg_namespace_properties\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncatalog_name\ntable_namespace\ntable_name\nmetadata_location\nprevious_metadata_location\n\n\n\n\n0\ndefault\ndefault\ntaxi_dataset\nfile:///tmp/warehouse/default.db/taxi_dataset/...\nfile:///tmp/warehouse/default.db/taxi_dataset/...\n\n\n\n\n\n\n\n\nThe “.db” file is a SQLite database; iceberg let you switch this to other\nUpdate a column from pyarrow:\n\nimport pyarrow.compute as pc\n# do not run this cell twice\ndf = df.append_column(\"tip_per_mile\", pc.divide(df[\"tip_amount\"], df[\"trip_distance\"]))\n\nThe schema from pyarrow can seamlessly integrate with pyiceberg by using “union_by_name”\n\nwith table.update_schema() as update_schema:\n    update_schema.union_by_name(df.schema)\ntable.overwrite(df)\nprint(table.scan().to_arrow())\n\npyarrow.Table\nVendorID: int64\ntpep_pickup_datetime: timestamp[us]\ntpep_dropoff_datetime: timestamp[us]\npassenger_count: double\ntrip_distance: double\nRatecodeID: double\nstore_and_fwd_flag: large_string\nPULocationID: int64\nDOLocationID: int64\npayment_type: int64\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\nimprovement_surcharge: double\ntotal_amount: double\ncongestion_surcharge: double\nairport_fee: double\ntip_per_mile: double\n----\nVendorID: [[2,2,2,1,2,...,2,2,1,1,1],[1,2,2,2,2,...,1,1,1,2,2],...,[2,2,2,2,2,...,2,2,2,2,2],[2,2,2,2,2,...,2,2,2,2,2]]\ntpep_pickup_datetime: [[2023-01-01 00:32:10.000000,2023-01-01 00:55:08.000000,2023-01-01 00:25:04.000000,2023-01-01 00:03:48.000000,2023-01-01 00:10:29.000000,...,2023-01-02 21:16:11.000000,2023-01-02 21:56:02.000000,2023-01-02 21:04:31.000000,2023-01-02 21:13:09.000000,2023-01-02 21:45:30.000000],[2023-01-02 21:49:54.000000,2023-01-02 21:17:06.000000,2023-01-02 21:35:06.000000,2023-01-02 21:18:43.000000,2023-01-02 21:24:42.000000,...,2023-01-04 14:04:17.000000,2023-01-04 14:27:49.000000,2023-01-04 14:44:46.000000,2023-01-04 14:35:46.000000,2023-01-04 14:52:44.000000],...,[2023-01-30 20:07:47.000000,2023-01-30 20:28:57.000000,2023-01-30 19:59:53.000000,2023-01-30 20:21:42.000000,2023-01-30 20:09:59.000000,...,2023-01-10 08:10:07.000000,2023-01-10 08:51:52.000000,2023-01-10 08:13:34.000000,2023-01-10 08:29:03.000000,2023-01-10 08:49:00.000000],[2023-01-10 08:30:00.000000,2023-01-10 08:34:07.000000,2023-01-10 08:06:16.000000,2023-01-10 08:47:26.000000,2023-01-10 08:43:51.000000,...,2023-01-31 23:58:34.000000,2023-01-31 23:31:09.000000,2023-01-31 23:01:05.000000,2023-01-31 23:40:00.000000,2023-01-31 23:07:32.000000]]\ntpep_dropoff_datetime: [[2023-01-01 00:40:36.000000,2023-01-01 01:01:27.000000,2023-01-01 00:37:49.000000,2023-01-01 00:13:25.000000,2023-01-01 00:21:19.000000,...,2023-01-02 21:22:04.000000,2023-01-02 22:02:42.000000,2023-01-02 21:08:06.000000,2023-01-02 21:31:43.000000,2023-01-02 21:48:18.000000],[2023-01-02 22:23:48.000000,2023-01-02 21:41:59.000000,2023-01-02 22:00:39.000000,2023-01-02 21:24:23.000000,2023-01-02 21:51:41.000000,...,2023-01-04 14:07:51.000000,2023-01-04 14:40:33.000000,2023-01-04 15:13:24.000000,2023-01-04 14:41:59.000000,2023-01-04 15:18:58.000000],...,[2023-01-30 20:24:09.000000,2023-01-30 20:37:27.000000,2023-01-30 20:16:07.000000,2023-01-30 20:32:01.000000,2023-01-30 20:17:23.000000,...,2023-01-10 08:41:22.000000,2023-01-10 09:12:03.000000,2023-01-10 08:20:49.000000,2023-01-10 08:45:05.000000,2023-01-10 09:42:00.000000],[2023-01-10 08:38:00.000000,2023-01-10 08:41:48.000000,2023-01-10 08:24:17.000000,2023-01-10 09:08:22.000000,2023-01-10 09:18:55.000000,...,2023-02-01 00:12:33.000000,2023-01-31 23:50:36.000000,2023-01-31 23:25:36.000000,2023-01-31 23:53:00.000000,2023-01-31 23:21:56.000000]]\npassenger_count: [[1,1,1,0,1,...,1,1,1,2,1],[1,1,2,1,2,...,0,2,2,1,1],...,[1,1,2,1,1,...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\ntrip_distance: [[0.97,1.1,2.51,1.9,1.43,...,1.59,0.74,0.9,4.3,0.5],[7.9,10.13,17.71,0.61,14.41,...,0.5,1.2,4.7,0.94,3.83],...,[1.95,1.53,3.25,2.23,1.16,...,5.84,2.66,1.22,1.7,20.95],[0.88,1.11,2.03,3.12,5.51,...,3.05,5.8,4.67,3.15,2.85]]\nRatecodeID: [[1,1,1,1,1,...,1,1,1,1,1],[1,1,2,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\nstore_and_fwd_flag: [[\"N\",\"N\",\"N\",\"N\",\"N\",...,\"N\",\"N\",\"Y\",\"N\",\"N\"],[\"N\",\"N\",\"N\",\"N\",\"N\",...,\"N\",\"N\",\"N\",\"N\",\"N\"],...,[\"N\",\"N\",\"N\",\"N\",\"N\",...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\nPULocationID: [[161,43,48,138,107,...,233,79,68,237,234],[164,132,132,140,132,...,107,158,79,246,113],...,[263,239,237,50,237,...,37,113,246,50,141],[262,170,141,4,80,...,107,112,114,230,262]]\nDOLocationID: [[141,237,238,7,79,...,141,211,158,114,164],[173,225,164,237,129,...,234,79,236,234,52],...,[238,237,68,238,229,...,195,161,100,170,132],[236,161,43,161,72,...,48,75,239,79,143]]\npayment_type: [[2,1,1,1,1,...,1,1,1,1,1],[2,1,2,2,1,...,1,1,2,1,1],...,[1,1,1,1,1,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]]\n...\n\n\n\ndf = table.scan(row_filter=\"tip_per_mile &gt; 0\").to_arrow()\nlen(df)\n\n2371784"
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#retrive-a-version-with-pyiceberg",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#retrive-a-version-with-pyiceberg",
    "title": "Frank Vs Iceberg",
    "section": "Retrive a Version with Pyiceberg",
    "text": "Retrive a Version with Pyiceberg\n\n\ntable.history()\n\n[SnapshotLogEntry(snapshot_id=1741698885604984058, timestamp_ms=1743305334480),\n SnapshotLogEntry(snapshot_id=1383213220069637644, timestamp_ms=1743308695020),\n SnapshotLogEntry(snapshot_id=8332539041388041675, timestamp_ms=1743308695985),\n SnapshotLogEntry(snapshot_id=8976772202933927568, timestamp_ms=1743308721822),\n SnapshotLogEntry(snapshot_id=3834561067045940934, timestamp_ms=1743308722873),\n SnapshotLogEntry(snapshot_id=9058209337445030832, timestamp_ms=1743359346432),\n SnapshotLogEntry(snapshot_id=5618975270792805759, timestamp_ms=1743359347616),\n SnapshotLogEntry(snapshot_id=3891282249768133214, timestamp_ms=1743359367299),\n SnapshotLogEntry(snapshot_id=2936152946567510319, timestamp_ms=1743359368365)]\n\n\nJust need to “scan” the correct snapshot id…\nPrevious version\n\ncurrent_snapshot = table.current_snapshot()\nfirst_snapshot = table.snapshots()[0]\nassert first_snapshot.parent_snapshot_id is None\n\n# retrive a snapshot by `scan()` the snapshot_id;\ntable.scan(snapshot_id=first_snapshot.snapshot_id).to_polars().head(1)\n\n\n\nshape: (1, 19)\n\n\n\nVendorID\ntpep_pickup_datetime\ntpep_dropoff_datetime\npassenger_count\ntrip_distance\nRatecodeID\nstore_and_fwd_flag\nPULocationID\nDOLocationID\npayment_type\nfare_amount\nextra\nmta_tax\ntip_amount\ntolls_amount\nimprovement_surcharge\ntotal_amount\ncongestion_surcharge\nairport_fee\n\n\ni64\ndatetime[μs]\ndatetime[μs]\nf64\nf64\nf64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2\n2023-01-01 00:32:10\n2023-01-01 00:40:36\n1.0\n0.97\n1.0\n\"N\"\n161\n141\n2\n9.3\n1.0\n0.5\n0.0\n0.0\n1.0\n14.3\n2.5\n0.0\n\n\n\n\n\n\n\nLater version (have column tip_per_mile)\n\n# current version have snapshort id\ntable.scan().to_polars().head(1)\n\n\n\nshape: (1, 20)\n\n\n\nVendorID\ntpep_pickup_datetime\ntpep_dropoff_datetime\npassenger_count\ntrip_distance\nRatecodeID\nstore_and_fwd_flag\nPULocationID\nDOLocationID\npayment_type\nfare_amount\nextra\nmta_tax\ntip_amount\ntolls_amount\nimprovement_surcharge\ntotal_amount\ncongestion_surcharge\nairport_fee\ntip_per_mile\n\n\ni64\ndatetime[μs]\ndatetime[μs]\nf64\nf64\nf64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n2\n2023-01-01 00:32:10\n2023-01-01 00:40:36\n1.0\n0.97\n1.0\n\"N\"\n161\n141\n2\n9.3\n1.0\n0.5\n0.0\n0.0\n1.0\n14.3\n2.5\n0.0\n0.0\n\n\n\n\n\n\n\n\nimport pyiceberg\nimport pyiceberg.utils\npyiceberg\n\n&lt;module 'pyiceberg' from '/Users/frankliang/miniconda3/lib/python3.12/site-packages/pyiceberg/__init__.py'&gt;"
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#setup-spark-with-apache-iceberg-without-runing-a-docker-container",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#setup-spark-with-apache-iceberg-without-runing-a-docker-container",
    "title": "Frank Vs Iceberg",
    "section": "Setup Spark with Apache Iceberg (Without Runing a Docker Container)",
    "text": "Setup Spark with Apache Iceberg (Without Runing a Docker Container)\n\ntable = catalog.load_table(\"default.taxi_dataset\")\ntable.catalog\n\ndefault (&lt;class 'pyiceberg.catalog.sql.SqlCatalog'&gt;)\n\n\nTo connect to spark locally, you have do configuration.\n\nConfigure the correct extension version:\n\nYou must match the correct spark version with the correct scalar version, otherwise you get this error.\nThis is usually the end before the second : of the package string.\n\nConfigure the same catalog (could not do so here with SQL catalog)\n\nspark-shell\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n      /_/\n         \nUsing Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 11.0.21)\nType in expressions to have them evaluated.\nType :help for more information.\nAbove tells you that the scala version is 2.12.18; there you need to find a package from the MVN repo - Apache Iceberg - iceberg-spark-runtime-3.5_2.12/1.8.1.\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom py4j.protocol import Py4JJavaError\nfrom colorama import Fore, Style\n# the tutorial come from [https://www.dremio.com/blog/3-ways-to-use-python-with-apache-iceberg/]\nwarehouse_path = \"/tmp/warehouse\"\nconf = (\n    pyspark.SparkConf()\n        .setAppName('app_name')\n        #packages\n        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1')\n        #SQL Extensions\n        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n        #Configuring Catalog\n        .set('spark.sql.catalog.deafult', 'org.apache.iceberg.spark.SparkCatalog')\n        .set('spark.sql.catalog.deafult.type', 'hadoop')\n        .set('spark.sql.catalog.deafult.warehouse', 'file://' + warehouse_path)\n        \n)\n\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\n\n:: loading settings :: url = jar:file:/Users/frankliang/miniconda3/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n\n\nIvy Default Cache set to: /Users/frankliang/.ivy2/cache\nThe jars for the packages stored in: /Users/frankliang/.ivy2/jars\norg.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-6fc2f773-1f2f-4db8-b7c3-fad102d35662;1.0\n    confs: [default]\n    found org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.8.1 in central\n:: resolution report :: resolve 68ms :: artifacts dl 2ms\n    :: modules in use:\n    org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.8.1 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n    ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-6fc2f773-1f2f-4db8-b7c3-fad102d35662\n    confs: [default]\n    0 artifacts copied, 1 already retrieved (0kB/3ms)\n25/04/02 20:25:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel)."
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#start-spark-from-scratch",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#start-spark-from-scratch",
    "title": "Frank Vs Iceberg",
    "section": "Start Spark from Scratch:",
    "text": "Start Spark from Scratch:\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom py4j.protocol import Py4JJavaError\nfrom colorama import Fore, Style\n# the tutorial come from [https://www.dremio.com/blog/3-ways-to-use-python-with-apache-iceberg/]\nwarehouse_path = \"warehouse\"\nconf = (\n    pyspark.SparkConf()\n        .setAppName('try-iceberg')\n        #packages\n        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1')\n        #SQL Extensions\n        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n        #Configuring Catalog\n        .set('spark.sql.catalog.local', 'org.apache.iceberg.spark.SparkCatalog')\n        .set('spark.sql.catalog.local.type', 'hadoop')\n        .set('spark.sql.catalog.local.warehouse', '../' + warehouse_path)\n        .set('spark.sql.defaultCatalog', 'local')\n)\n\nspark = SparkSession\\\n   .builder\\\n   .config(conf=conf)\\\n   .enableHiveSupport()\\\n   .getOrCreate()\n\n25/04/02 20:27:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n\n\n\nspark.catalog.listCatalogs()\n\n[CatalogMetadata(name='local', description=None),\n CatalogMetadata(name='spark_catalog', description=None)]\n\n\n25/04/02 21:20:40 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1053090 ms exceeds timeout 120000 ms\n25/04/02 21:20:40 WARN SparkContext: Killing executors is not supported by current scheduler.\n25/04/02 21:23:23 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 21:23:23 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 21:40:26 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 21:40:26 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 21:54:11 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/02 21:54:11 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/02 21:54:21 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 21:54:21 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 21:54:31 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 21:54:31 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 21:54:41 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 21:54:41 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 22:00:27 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 22:00:27 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 22:00:37 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 22:00:37 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 22:30:05 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 22:30:05 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 22:30:15 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 22:30:15 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 22:45:24 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 22:45:24 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 23:02:14 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/02 23:02:14 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/02 23:02:24 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/02 23:02:24 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/02 23:02:34 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 23:02:34 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 23:20:07 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 23:20:07 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 23:45:01 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/02 23:45:01 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:00:49 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:00:49 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:17:18 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:17:18 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:17:28 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:17:28 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:17:38 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:17:38 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:32:09 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:32:09 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:32:19 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:32:19 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:47:48 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 00:47:48 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 01:22:49 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 01:22:49 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 01:33:12 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 01:33:12 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 01:49:32 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 01:49:32 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 02:22:18 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 02:22:18 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 02:38:26 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 02:38:26 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 02:55:36 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 02:55:36 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 03:28:36 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 03:28:36 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 03:28:46 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 03:28:46 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 03:44:17 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 03:44:17 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 03:44:27 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 03:44:27 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 03:44:37 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 03:44:37 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 04:19:56 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 04:19:56 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 04:53:19 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 04:53:19 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 05:24:28 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 05:24:28 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 05:40:30 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 05:40:30 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 06:01:03 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 06:01:03 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 06:18:34 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 06:18:34 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 06:51:09 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 06:51:09 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 07:10:32 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 07:10:32 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 07:19:23 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:19:23 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:19:40 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:19:40 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:28:39 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:28:39 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:29:02 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 07:29:02 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 07:29:17 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:29:17 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:38:04 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:38:04 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:47:13 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:47:13 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:47:34 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:47:34 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:58:00 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 07:58:00 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:02:01 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 08:02:01 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 08:05:47 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 08:05:47 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 08:14:54 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:14:54 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:24:14 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:24:14 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:24:24 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:24:24 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:24:34 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:24:34 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:33:12 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 08:33:12 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n    at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n    at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n    at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n    at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n    at scala.concurrent.Future.flatMap(Future.scala:306)\n    at scala.concurrent.Future.flatMap$(Future.scala:306)\n    at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n    ... 17 more\n25/04/03 08:42:17 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:42:17 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:42:27 ERROR Inbox: Ignoring error\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:42:27 WARN Executor: Issue communicating with driver in heartbeater\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n    at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n    at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n    at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n    at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n    at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n    at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n    at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n    at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n    at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n    ... 3 more\nCaused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@macbookpro.localdomain:54570\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n    at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n    at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.trySuccess(Promise.scala:94)\n    at scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n    at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n    at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n    at scala.concurrent.Promise.complete(Promise.scala:53)\n    at scala.concurrent.Promise.complete$(Promise.scala:52)\n    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n    at scala.concurrent.Promise.success(Promise.scala:86)\n    at scala.concurrent.Promise.success$(Promise.scala:86)\n    at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n    at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n    at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n    at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n    ... 8 more\n25/04/03 08:42:27 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n\n\n\nfrom pyiceberg.catalog import load_catalog\nimport pyarrow.parquet as pq\n\n# annoytingly you can only load \ndf = pq.read_table('/tmp/yellow_tripdata_2023-01.parquet')\ncatalog = load_catalog('default', **{\n    \"type\": 'sql',\n    \"uri\": f\"sqlite:///../warehouse/pyiceberg_catalog.db\",\n    \"warehouse\": f\"file://../warehouse\",\n})\ncatalog.create_namespace_if_not_exists('sparkly')\ntable = catalog.create_table_if_not_exists('sparkly.taxi_dataset', schema=df.schema)\ntable.overwrite(df)\nassert len(table.scan().to_arrow()) == 3066766"
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#feature-checklist",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#feature-checklist",
    "title": "Frank Vs Iceberg",
    "section": "Feature Checklist",
    "text": "Feature Checklist\nIntegretion & Geospatial & Basic\n\nSupport geometry type: https://github.com/apache/iceberg/pull/10981\nSupport IBIS framework: Nope!\nSupport DUCKDB: Yes!\nAbility to interact with file system, as predicted, through docker volume.\n\nSQL Feature Completeness\n\nWith statement or CTE\nMost of SQL feature comes from SparkSQL&gt;\nUnique Key & Foreign Key constraint? Nope\nPolymorphism? or Table Inheritance? Nope\n\nTime Travel\n\nIf a column is renamed, when query a previous version, do the name fallback to previous version too? Or just the data?"
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#final-thoughts",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#final-thoughts",
    "title": "Frank Vs Iceberg",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThe time travel feature is fluid. Despite so this is an analytic data warehousing, not application database, so not as comprehensive as postgres database; however, the ability to reverse versioning is very demure.\nAnnoyingly this warehouse don’t support “foreign key”, which means you have to implement your own logic for securing foreign key for dimensional table."
  },
  {
    "objectID": "2025/03-26 Try Apache Iceberg/notebooks/01.html#reference",
    "href": "2025/03-26 Try Apache Iceberg/notebooks/01.html#reference",
    "title": "Frank Vs Iceberg",
    "section": "Reference",
    "text": "Reference\n\nConfigure Catelog to use different database backend\nPyiceberg - Python implementation of Apacheiceberg\nAdd a parquet file directly to Iceberg\n\nthis is useful in the case if you already have the parquet file yourself and expect .parquet file are the same schema;\niceberg will consider them as snapshot will expire them as usual.\n\nSetup Iceberg with Spark\n\nthe blog is based on the official documentation - spark configuration\nthis blog introduce you to pyspark config + amazon S3 but really confusing if you want a simple experience\nA different blog introduce limitation with local spark"
  },
  {
    "objectID": "2024/10-22 Mars Js/blog/02.html#what-about-detailed-3d-models-in-esri",
    "href": "2024/10-22 Mars Js/blog/02.html#what-about-detailed-3d-models-in-esri",
    "title": "3D Buildings on Mars - Part II Explore ESRI",
    "section": "What about Detailed 3d Models in ESRI?",
    "text": "What about Detailed 3d Models in ESRI?\nHere are my research how to do this. ARCGIS seems to gives you three options\n\n\npublish a scene service,\n\n\nintegrated mesh layer and 3D object scene layer\n\n\nas footprint (just polygons with height)\n\n\nAccroding to ESRI, publish a scene service is the orthodoxical way:\n\nSome visualizations can be made more compelling when they display the detailed 3D model of a building. 3D object scene layers store large city models that can be displayed with textures or colors. Such data can be modeled in software such as ArcGIS Pro or CityEngine, can be extracted from LiDAR data, and some 3D data has been made available on open data portals. A first step to visualize this type of data is to publish a scene service, add it to a map as a scene layer, and then visualize it in a scene view. You can display the data either with the original textures or set a renderer to display the buildings with a different color or with a data-driven styling.\n\n\nFeature Layer\nFeature layer seems to be just a underlaying 3D scene. You have to publish it to a server for it to work on ESRI, for example This Swizarland 3D Map\nThis seems to be layers you call from a url?\n\n\nA Scene layer which let you drop a 3D model on map!\nGold! I come accross this example that let you drag and drop a 3D model.\nconst mesh = await sceneLayer.convertMesh([file]);\n  sketchVM.place(mesh, {\n    graphicProperties: {\n      layer: sketchLayer,\n    }\n  });\n  await sceneLayer.applyEdits({\n    addFeatures: graphics\n  });\n\n  await sceneLayer.applyEdits({\n    updateFeatures: graphics\n  });\n\n  await sceneLayer.applyEdits({\n    deleteFeatures: ids\n  });\n\n\nAdd a custom feature with FeatureLayer\nAccroding to the documentation you can create feature layer with either URL, arcgis id, or an array (feature layer table).\n\ngeometryType: If working with spatial layers, the geometry type of the features must be indicated (since only one geometry type is allowed per layer)\nBoth spatial and non-spatial feature collections require an objectId field, this must be indicated along with an array of field objects\n\nYou have to specify schema by use this structure:\n{\n    fields: [\n        {name: \"column\", alias: \"\", type: \"oid/string/date\"},\n        ...\n    ],\n    objectIdField: \"column\",\n    geometryType:\"point/null\",\n    spatialReference: { wkid: 4326 },\n}\n\nA client-side non-spatial table can be created by setting the layer’s geometryType property to null, then the table must be loaded by calling the load() method.\n\nImplement in javascript\n// Create an empty non-spatial feature layer\n// Set geometryType property to null when creating non-spatial feature layer\nconst layer = new FeatureLayer({\n  source: [],\n  objectIdField: \"OBJECTID\",\n  fields: [{\n    name: \"OBJECTID\",\n    type: \"oid\"\n  }],\n  geometryType: null\n});\nlayer.load().then(() =&gt; console.log(layer.isTable))\n\n\nFinally\n\n// This is the working example \n// Access ESRI module and place a custom event into a folder\n\nimport Map from \"@arcgis/core/Map.js\";\nimport SceneView from \"@arcgis/core/views/SceneView\";\nimport ElevationLayer from \"@arcgis/core/layers/ElevationLayer\";\nimport TileLayer from \"@arcgis/core/layers/TileLayer\";\nimport Graphic from \"@arcgis/core/Graphic\";\nimport ObjectSymbol3DLayer from \"@arcgis/core/symbols/ObjectSymbol3DLayer\";\nimport GraphicsLayer from \"@arcgis/core/layers/GraphicsLayer\"\n\nimport PointSymbol3D from \"@arcgis/core/symbols/PointSymbol3D\";\n\nimport Point from \"@arcgis/core/geometry/Point.js\"\nimport SimpleMarkerSymbol from \"@arcgis/core/symbols/SimpleMarkerSymbol.js\"\n\nconst marsElevation = new ElevationLayer({\n    url: \"https://astro.arcgis.com/arcgis/rest/services/OnMars/MDEM200M/ImageServer\",\n    copyright: \"NASA, ESA, HRSC, Goddard Space Flight Center, USGS Astrogeology Science Center, Esri\"\n});\n\nconst marsImagery = new TileLayer({\n    url: \"https://astro.arcgis.com/arcgis/rest/services/OnMars/MDIM/MapServer\",\n    title: \"Imagery\",\n    copyright: \"USGS Astrogeology Science Center, NASA, JPL, Esri\"\n});\n\n\nconst map = new Map({\n    ground: {\n      layers: [marsElevation]\n    },\n    layers: [marsImagery]\n  });\n\nconst view = new SceneView({\n    map: map,\n    container: \"viewDiv\",\n    // setting the spatial reference for Mars_2000 coordinate system\n    spatialReference: {\n        wkid: 104971\n    },\n    camera: {\n        position: {\n        x: 27.63423,\n        y: -6.34466,\n        // z: 1281525.766,\n        z: 128152.5766 * 6,\n        spatialReference: 104971\n        },\n        heading: 332.28,\n        tilt: 37.12\n    }\n});\n\n// To import any 3D geometry, you have to create a new object called\n// ObjectSymbol3DLayer\nconst buildingGeometry = new ObjectSymbol3DLayer({\n    anchor: \"relative\",\n    resource: {\n      href: \"/model 2/building-parts/individuals.glb\" // Replace with the actual path to your .glb file\n    },\n    heading: 180, // Optional: Adjust rotation\n    tilt: 0,\n    roll: 0,\n    depth: 750000,   // Adjust scale as needed\n    width: 750000,\n    height: 10000\n  });\n\n// Essentailly We Are Using This ObjectSymbol as one singular Symbol;\nconst buildingSymbol = new PointSymbol3D({\n    symbolLayers: [buildingGeometry]\n});\n\n// Define the graphic with the symbol and add to the layer\n// Point Graphical Network;\nconst pointGraphic = new Graphic({\n    geometry: {\n        type: \"point\",\n        x: 27.63423,\n        y: -6.34466,\n        spatialReference: {\n            wkid: 104971 // this is Mars wkid\n        }\n    },\n    symbol: buildingSymbol\n});\n\nconst graphicsLayer = new GraphicsLayer();\ngraphicsLayer.add(pointGraphic);\nmap.add(graphicsLayer);\nin INDEX.html:\n&lt;link rel=\"stylesheet\" href=\"https://js.arcgis.com/4.32/esri/themes/light/main.css\"&gt;\n&lt;style&gt;\n    html,\n    body,\n    #viewDiv {\n        padding: 0;\n        margin: 0;\n        height: 100%;\n        width: 100%;\n    }\n&lt;/style&gt;\n&lt;body&gt;\n    &lt;script type=\"module\" src=\"a1-esri-mars.js\"&gt;&lt;/script&gt;\n    &lt;style&gt;\n        /* html,\n        body,    */\n        #viewDiv {\n          padding: 0;\n          margin: 0;\n          height: 100%;\n          width: 100%;\n        }\n    &lt;/style&gt;\n    &lt;div id=\"viewDiv\"&gt;&lt;/div&gt;\n&lt;/body&gt;"
  },
  {
    "objectID": "2024/10-13 D3/notebook.html",
    "href": "2024/10-13 D3/notebook.html",
    "title": "Creating a Force Network Using D3",
    "section": "",
    "text": "Lets try use the fancy force graph diagram in python!"
  },
  {
    "objectID": "2024/10-13 D3/notebook.html#intro",
    "href": "2024/10-13 D3/notebook.html#intro",
    "title": "Creating a Force Network Using D3",
    "section": "",
    "text": "Lets try use the fancy force graph diagram in python!"
  },
  {
    "objectID": "2024/10-13 D3/notebook.html#relevant-development",
    "href": "2024/10-13 D3/notebook.html#relevant-development",
    "title": "Creating a Force Network Using D3",
    "section": "Relevant Development",
    "text": "Relevant Development\n\nobservable js: Generalized Observable Framework - pyobsplot allows to use Observable Plot to create charts in Jupyter notebooks, VSCode notebooks, Google Colab and Quarto documents. Plots are created from Python code with a syntax as close as possible to the JavaScript one.\nd3py: A thin Python Wrapper for usage with graph network."
  },
  {
    "objectID": "2024/10-13 D3/notebook.html#place-data-into-observables",
    "href": "2024/10-13 D3/notebook.html#place-data-into-observables",
    "title": "Creating a Force Network Using D3",
    "section": "Place Data into Observables",
    "text": "Place Data into Observables\n\nUse Obserable\n\nfrom IPython.display import display, HTML\nimport json\n\n# Data to pass (example: nodes and links for the force-directed graph)\ndata = {\n    'nodes': [{'id': 0}, {'id': 1}, {'id': 2}],\n    'links': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]\n}\n\n# HTML/JS code to embed Observable in Jupyter\nobservable_code = f\"\"\"\n&lt;div id='chart'&gt;&lt;/div&gt;\n&lt;script type=\"module\"&gt;\n  import {{ Runtime, Inspector }} from \"https://cdn.jsdelivr.net/npm/@observablehq/runtime@4/dist/runtime.js\";\n  import define from \"https://api.observablehq.com/@d3/force-directed-graph.js?v=3\"; // Use Observable's force-directed graph\n  \n  const runtime = new Runtime();\n  \n  runtime.module(define, name =&gt; {{\n    if (name === \"chart\") return new Inspector(document.querySelector(\"#chart\"));\n  }});\n  \n  // Pass the Python data to Observable\n  const data = {json.dumps(data)};\n  \n  // This assumes that Observable can receive external data like this, or you'd modify the Observable notebook to accept it.\n&lt;/script&gt;\n\"\"\"\n\n# Display the graph in Jupyter\ndisplay(HTML(observable_code))\n\n\n\n\n\n\nThis is called an embedment. However the data won’t quite update with this. I’m gussing if you use observable direction you don’t need the “https://api.observablehq.com/”, you just need everything after @.\nThis is also a bit misleading because the force digram is just grabing element from observable instead of customise creating what we whated.\n\n\nUse D3 By itself\nStart from scratch, warp some python data in a jupyter notebook. Here is a script given by AI\n\nfrom IPython.display import display, HTML\nimport json\n\n# Data for nodes and links (Python dictionary)\ndata = {\n    'nodes': [{'id': 0, 'name': 'A'}, {'id': 1, 'name': 'B'}, {'id': 2, 'name': 'C'}],\n    'links': [{'source': 0, 'target': 1}, {'source': 1, 'target': 2}]\n}\n\n# Convert the Python data to JSON (so it can be used in JavaScript)\njson_data = json.dumps(data)\n\n# HTML and JS to render the force-directed graph with D3.js\nhtml_code = f\"\"\"\n&lt;div id=\"d3-graph\"&gt;&lt;/div&gt;\n&lt;script src=\"https://d3js.org/d3.v7.min.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n\n\n// Parse the Python data passed to the script\nconst graphData = {json_data};\n\n// Set the dimensions of the graph\nconst width = 600;\nconst height = 400;\n\n// Append an SVG element to the div with id 'd3-graph'\nconst svg = d3.select(\"#d3-graph\")\n    .append(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height);\n\n// Define the force simulation\nconst simulation = d3.forceSimulation(graphData.nodes)\n    .force(\"link\", d3.forceLink(graphData.links).id(d =&gt; d.id).distance(100))\n    .force(\"charge\", d3.forceManyBody().strength(-400))\n    .force(\"center\", d3.forceCenter(width / 2, height / 2));\n\n// Create the link (line) elements\nconst link = svg.append(\"g\")\n    .selectAll(\"line\")\n    .data(graphData.links)\n    .enter()\n    .append(\"line\")\n    .attr(\"stroke\", \"#999\")\n    .attr(\"stroke-width\", 2);\n\n// Create the node (circle) elements\nconst node = svg.append(\"g\")\n    .selectAll(\"circle\")\n    .data(graphData.nodes)\n    .enter()\n    .append(\"circle\")\n    .attr(\"r\", 10)\n    .attr(\"fill\", \"steelblue\")\n    .call(drag(simulation));\n\n// Dragging functionality for nodes\nfunction drag(simulation) {{\n    function dragstarted(event, d) {{\n        if (!event.active) simulation.alphaTarget(0.3).restart();\n        d.fx = d.x;\n        d.fy = d.y;\n    }}\n    \n    function dragged(event, d) {{\n        d.fx = event.x;\n        d.fy = event.y;\n    }}\n    \n    function dragended(event, d) {{\n        if (!event.active) simulation.alphaTarget(0);\n        d.fx = null;\n        d.fy = null;\n    }}\n    \n    return d3.drag()\n        .on(\"start\", dragstarted)\n        .on(\"drag\", dragged)\n        .on(\"end\", dragended);\n}}\n\n// Update positions of nodes and links on each tick\nsimulation.on(\"tick\", () =&gt; {{\n    link\n        .attr(\"x1\", d =&gt; d.source.x)\n        .attr(\"y1\", d =&gt; d.source.y)\n        .attr(\"x2\", d =&gt; d.target.x)\n        .attr(\"y2\", d =&gt; d.target.y);\n\n    node\n        .attr(\"cx\", d =&gt; d.x)\n        .attr(\"cy\", d =&gt; d.y);\n}});\n\n&lt;/script&gt;\n\"\"\"\n\nThis AI code will not display anything in jupyter notebook. Because d3 module is not imported correctly. However it does if we written done in html_code and display in browser\n\nif False: \n    with open('d3force.html', \"w\") as f:\n        f.write(html_code)\n\nWork-around is probably use vanilla javascript. Based on official documentation: https://d3js.org/getting-started. To use this in a package\n\n# Data for nodes and links (Python dictionary)\njson_data = {\n    'nodes': [{'id': 0, 'name': 'A'}\n              , {'id': 1, 'name': 'B'}\n              , {'id': 2, 'name': 'C'}\n              ,{'id': 3, 'name': 'D'} ],\n    'links': [{'source': 0, 'target': 1}\n              , {'source': 1, 'target': 2}\n              , {'source': 1, 'target': 3}\n              , {'source':2, 'target': 3}]\n}\n\nhtml_code_2 = f\"\"\"\n&lt;p id=\"dev\"&gt;&lt;/p&gt;\n&lt;div id=\"d3-graph\"&gt; &lt;/div&gt;\n&lt;script type=\"module\"&gt;\n\nimport * as d3 from \"https://cdn.jsdelivr.net/npm/d3@7/+esm\";\n\n// Parse the Python data passed to the script\nconst graphData = {json_data};\n\n// Set the dimensions of the graph\nconst width = 600;\nconst height = 400;\n\n// Append an SVG element to the div with id 'd3-graph'\nconst svg = d3.select(\"#d3-graph\")\n    .append(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height);\n\n// Define the force simulation\nconst simulation = d3.forceSimulation(graphData.nodes)\n    .force(\"link\", d3.forceLink(graphData.links).id(d =&gt; d.id).distance(100))\n    .force(\"charge\", d3.forceManyBody().strength(-400))\n    .force(\"center\", d3.forceCenter(width / 2, height / 2));\n\n// Create the link (line) elements\nconst link = svg.append(\"g\")\n    .selectAll(\"line\")\n    .data(graphData.links)\n    .enter().append(\"line\")\n    .attr(\"stroke\", \"#999\")\n    .attr(\"stroke-width\", 2);\n\n\n// Create the node (circle) elements\nconst node = svg.append(\"g\")\n    .selectAll(\"circle\")\n    .data(graphData.nodes)\n    .enter().append(\"circle\")\n    .attr(\"r\", 10)\n    .attr(\"fill\", \"#a3e4d7\")\n    .attr(\"stroke-linecap\", \"round\")\n    .attr(\"stroke-linejoin\", \"round\")\n    .call(drag(simulation));\n\n// Add Label To Node: This technique is adding element instead of text on top of label\nconst text1 = svg.append(\"g\")\n    .attr(\"class\", \"labels\")\n    .selectAll(\"text\")\n    .data(graphData.nodes)\n    .enter().append(\"text\")\n    .attr(\"dx\", 12)\n    .attr(\"dy\", \".35em\")\n    .text(function(d) {{ return d.name }})\n\n// Obnoxious as it is, you are essentially doubling the text to create a white background\n// now you will have to add it to tick to simulate movement\nconst text2 = text1\n    .clone(true).lower()\n    .attr(\"fill\", \"none\")\n    .attr(\"stroke\", \"white\")\n    .attr(\"stroke-width\", 3);\n\n\n\n//document.getElementById(\"dev\").innerHTML = labels\n//document.getElementById(\"dev\").innerHTML += typeof graphData.nodes.map(d =&gt; d.name)\n\n\n// Dragging functionality for nodes\nfunction drag(simulation) {{\n    function dragstarted(event, d) {{\n        if (!event.active) simulation.alphaTarget(0.3).restart();\n        d.fx = d.x;\n        d.fy = d.y;\n    }}\n    \n    function dragged(event, d) {{\n        d.fx = event.x;\n        d.fy = event.y;\n    }}\n    \n    function dragended(event, d) {{\n        if (!event.active) simulation.alphaTarget(0);\n        d.fx = null;\n        d.fy = null;\n    }}\n    \n    return d3.drag()\n        .on(\"start\", dragstarted)\n        .on(\"drag\", dragged)\n        .on(\"end\", dragended);\n}}\n\n\n// This is Very Important; If you don't add this function there will be no coordinate\n// Update positions of nodes and links on each tick\nsimulation.on(\"tick\", () =&gt; {{\n    link\n        .attr(\"x1\", d =&gt; d.source.x)\n        .attr(\"y1\", d =&gt; d.source.y)\n        .attr(\"x2\", d =&gt; d.target.x)\n        .attr(\"y2\", d =&gt; d.target.y);\n\n    node.attr(\"transform\", d =&gt; `translate(${{d.x}},${{d.y}})`);\n    text1.attr(\"transform\", d =&gt; `translate(${{d.x}},${{d.y}})`);\n    text2.attr(\"transform\", d =&gt; `translate(${{d.x}},${{d.y}})`);\n}});\n\n&lt;/script&gt;\n\"\"\"\n\ndisplay(HTML(html_code_2))\n\n\n\n \n\n\n\n\nthe common pattern is:\n\nselectAll(), data(), entre(), append()\n\nto add a label, you have to create a new element\n\nThis maybe useful to read: thinking with join\n\n\nMore on D3.Js method\n\nd3_script = '''\nconst matrix = [\n  [11975,  5871, 8916, 2868],\n  [ 1951, 10048, 2060, 6171],\n  [ 8010, 16145, 8090, 8045],\n  [ 1013,   990,  940, 6907]\n];\n\nd3.select(\"body\")\n  .append(\"table\")\n  .selectAll(\"tr\")\n  .data(matrix)\n  .join(\"tr\")\n  .selectAll(\"td\")\n  .data(d =&gt; d)\n  .join(\"td\")\n  .text(d =&gt; d)\n  .attr(\"stroke\": \"white\");\n'''\ntable_html = f'''\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-US\"&gt;\n  &lt;h1&gt;Example Of Join&lt;/h1&gt;   \n  &lt;body&gt;&lt;/body&gt;\n  &lt;/html&gt;\n&lt;script type=\"module\"&gt;\n    import * as d3 from \"https://cdn.jsdelivr.net/npm/d3@7/+esm\";\n    {d3_script}\n&lt;/script&gt;\n'''\n\ndisplay(HTML(table_html))\n\n\n\n\n  Example Of Join   \n  \n  \n\n\n\n\n\nUse a Prebuild Observable Module to add Label\n\nfrom IPython.display import display, HTML\nimport json\n\njson_data = {\n    'nodes': [{'id': 0, 'name': 'A'}\n              , {'id': 1, 'name': 'B'}\n              , {'id': 2, 'name': 'C'}\n              ,{'id': 3, 'name': 'D'} ],\n    'links': [{'source': 0, 'target': 1}\n              , {'source': 1, 'target': 2}\n              , {'source': 1, 'target': 3}]\n}\n\ndef read_file(filename):\n    with open(filename, 'r') as f:\n        return f.read()\n\n\njs_function = read_file(\"ForceGraph.js\")\nassert js_function,'File Not Found'\n\n\nhtml_code_3 = f'''\n&lt;div id=\"d3-graph-3\"&gt;&lt;/div&gt;\n&lt;script type=\"module\" src=\"https://cdn.jsdelivr.net/npm/d3@7/+esm\"&gt;\n\n{js_function}\n\n// Set the dimensions of the graph\nconst width = 600;\nconst height = 400;\n\n// Append an SVG element to the div with id 'd3-graph'\n\nmy_svg = ForceGraph({json_data},{{\n    nodeId: d =&gt; d.id,\n    //nodeGroup: d =&gt; d.id,\n    nodeTitle: d =&gt; d.name,\n    width,\n    height: 600\n}});\n\nd3.select(\"#d3-graph-3).append(my_svg)\n&lt;/script&gt;\n'''\n\ndisplay(HTML(html_code_3))\n\n\n\n\n\n\n\n# def write_file(x, filename):\n#     with open(filename,'w') as f:\n#         f.write(x)\n\n# write_file(html_code_3, \"label-force.html\")"
  },
  {
    "objectID": "2024/10-13 D3/notebook.html#resources",
    "href": "2024/10-13 D3/notebook.html#resources",
    "title": "Creating a Force Network Using D3",
    "section": "Resources",
    "text": "Resources\n\nObserable update method for: Adding Zoom or Drag\n\nin the new version of observable you use:\n\nd3.drag()\n.on(“start”, (event, d) =&gt; circle.filter(p =&gt; p === d).raise().attr(“stroke”, “black”))\n.on(“drag”, (event, d) =&gt; (d.x = event.x, d.y = event.y))\n.on(“end”, (event, d) =&gt; circle.filter(p =&gt; p === d).attr(“stroke”, null))\n\n\nZoom and Drag Implementation in the old observable resources\nDifference betweeen d3.join and d3.append\n\nd3.append is adding child element."
  },
  {
    "objectID": "2024/05-19 Factor Analysis/pca.html",
    "href": "2024/05-19 Factor Analysis/pca.html",
    "title": "Implementation of Principle Component Analyis",
    "section": "",
    "text": "One of the most common dimension reduction techniques"
  },
  {
    "objectID": "2024/05-19 Factor Analysis/pca.html#in-scikit-learn-you-use-this-package",
    "href": "2024/05-19 Factor Analysis/pca.html#in-scikit-learn-you-use-this-package",
    "title": "Implementation of Principle Component Analyis",
    "section": "In Scikit-Learn you use this package",
    "text": "In Scikit-Learn you use this package\n\nfrom sklearn.preprocessing import StandardScaler\n\niris = load_iris()\nX = iris['data']\ny = iris['target']\n\nX_std = StandardScaler().fit_transform(X)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2).fit(X_std)\n\nprint('Components:\\n', pca.components_)\nprint('Explained variance ratio:\\n', pca.explained_variance_ratio_)\n\ncum_explained_variance = np.cumsum(pca.explained_variance_ratio_)\nprint('Cumulative explained variance:\\n', cum_explained_variance)\n\nX_pca = pca.transform(X_std) # Apply dimensionality reduction to X.\nprint('Transformed data shape:', X_pca.shape)\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c = y)\nplt.xlabel('PC1'); plt.xticks([])\nplt.ylabel('PC2'); plt.yticks([])\nplt.title('2 components, captures {}% of total variation'.format(cum_explained_variance[1].round(4)*100))\nplt.show()\n\nComponents:\n [[ 0.52106591 -0.26934744  0.5804131   0.56485654]\n [ 0.37741762  0.92329566  0.02449161  0.06694199]]\nExplained variance ratio:\n [0.72962445 0.22850762]\nCumulative explained variance:\n [0.72962445 0.95813207]\nTransformed data shape: (150, 2)"
  },
  {
    "objectID": "2024/05-19 Factor Analysis/pca.html#conclusion",
    "href": "2024/05-19 Factor Analysis/pca.html#conclusion",
    "title": "Implementation of Principle Component Analyis",
    "section": "Conclusion",
    "text": "Conclusion\n\nPCA is based covariance a measurement of correlation of two product\nPCA is linear based algebra.\nDimension reduction is in fact finding top “k” highest eigen value and their corresponding “eigen-vector”, they corresponding sets of co-efficiency (or scores) that each dimension contributes to (the latent factor).\nThe resulting “k” dimension results in most explained variance been kept.\n\n\nReference: Implementation of Principle Component\n\nAlireza Bagheri Python Implementation of PCA\nSergen Cansiz exmpalin co-variance with eigenvalues\nGrant Sanderson’s Visually Expalin Linear Algebra\nDuke University’s Guide and Other Cool Staff\nStack Exchange: Intuition of Eigenvalue with Covariance Matrix\n\n\n\nOther Dimension Reduction Techniques\n\nMote Carlo Generator\nLinear Discreminant Analysis & Comparsion here"
  },
  {
    "objectID": "2024/03-15-Time-Series/statlab - Normality of Data.html",
    "href": "2024/03-15-Time-Series/statlab - Normality of Data.html",
    "title": "Data Normality",
    "section": "",
    "text": "This notebook explore: - The R square against normal data - Norm data against lag - qq plot against normal data\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nplot_params = {'color': '0.75',\n 'style': '.-',\n 'markeredgecolor': '0.25',\n 'markerfacecolor': '0.25',\n 'legend': False}\n\nplt.style.use('seaborn-whitegrid')\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\n%config InlineBackend.figure_format = 'retina'\n\n/var/folders/r5/1cdq52mn21zdnqzl0fvp44zw0000gn/T/ipykernel_23399/1354677373.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n\n\n\n\n\n# explore linear correlation\ndef foo(x):\n    y = x * 3 + 1\n    return y\n\nx = np.arange(1, 4)\ny = np.apply_along_axis(foo, axis = 0, arr = x)\n\nif ((x - x.mean())**2).sum()/x.shape[0] == x.var():\n    print('var is not sqrt-ed variance')\nelse: \n    print('var is standard deviation')\n\ndef corrianda(X, Y):\n    c = ((X - X.mean())*(Y - Y.mean())).sum()\n    return(c)\ndef ssd(X):\n    c = X.var() * X.shape[0]\n    return(c)\ndef covar(X, Y):\n    c = corrianda(x, y) / np.sqrt(ssd(x) * ssd(y))\n    return(c)\n\nvar is not sqrt-ed variance\n\n\n\nx = np.arange(1, 10000)\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a perfect line has coefficency of {covar(x, y)}')\n\na perfect line has coefficency of 1.0\n\n\n\nx = np.random.normal(50, 10, 10000)\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a standard normal line has coefficency of {covar(x, y)}')\n\na standard normal line has coefficency of 1.0\n\n\n\nx = np.arange(1, 10000)**2\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a skewed data(power) has coefficency of {covar(x, y)}')\n\na skewed data(power) has coefficency of 1.0000000000000002\n\n\n\nx = np.log(np.arange(1, 10000))\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a skewed data(log) has coefficency of {covar(x, y)}')\n\na skewed data(log) has coefficency of 0.9999999999999999\n\n\n\n\n\n\ndf = pd.read_csv(\n    'data/output/Calenrier_output.csv',\n    parse_dates = ['date']\n)\ndf = df.set_index('date').to_period('D')\n\n\ndf.filter(regex='s\\(\\d+,\\d+\\)').iloc[0:15,:].plot(style = 'o')\ndf.filter(regex='sin|cos').iloc[0:15,:].plot(style = 'o:')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe s something seems to be modeling week\n\n\n\n\nx = pd.Series(np.random.normal(50, 10, 200))\n\n\nfig, (_) = plt.subplots(2, 3, figsize = (15,10) )\nlags = np.append(np.arange(5), 50)\nfor i in np.arange(len(lags)):\n    sns.scatterplot(\n        data = pd.DataFrame({\"x\":x, \"lag\" :x.shift(i)}),\n        x = 'x',\n        y = 'lag',\n        ax = _[i%2, i//2]\n    )\n    _[i%2, i//2].set_title(f'Lag {lags[i]} steps')\n\n\n\n\n\n\n\n\nWhat I realise about this a normal distributed plot (taking out in random orders) will not have auto-correlatio no matter what. This is because after taking out a random value, the distribution is still normal.\nAlternatively If I order x and let it leg this results might be very different:\n\nfig, (_) = plt.subplots(2, 3, figsize = (15,10) )\nlags = np.append(np.arange(5), 50)\nfor i in np.arange(len(lags)):\n    sns.scatterplot(\n        data = pd.DataFrame({\"x\":x.sort_values(), \"lag\" :x.sort_values().shift(i)}),\n        x = 'x',\n        y = 'lag',\n        ax = _[i%2, i//2]\n    )\n    _[i%2, i//2].set_title(f'Lag {lags[i]} steps')\n\n\n\n\n\n\n\n\nThink of this problem in very extrem ways. At very extrem. The order of a normally distributed variables are extremely random. There will be no pattern to lag at all.\nAt the other end, our variable are extremely ordered. There will be near perfect correcation between lag and time. (This correlation will be near one)\nFor a gross sample that is normal. The slop should only be between 0 to 1. Trimed tail should not affect result here.\nWhat does it means when any of your is extremely random\nI guess what by looking at distribution of something you really know what is going on underneath.\n\n\nUse qqplot\n\nimport statsmodels.api as sm\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (14, 7))\nsm.qqplot(x.sort_values().head(100), ax=ax1);\nax1.set_title('trail tails results in upward bending')\n# trail tails results in upward bending\nsm.qqplot(x.sort_values().tail(100), ax=ax2);\nax2.set_title('trim heads restuls in downward bending')\n# trim heads restuls in downward bending\n\nText(0.5, 1.0, 'trim heads restuls in downward bending')\n\n\n\n\n\n\n\n\n\nNormal data can be abnormal by missing some part.\n\n\n\n\nx = np.random.normal(50, 4, 10000)\nsx = pd.Series(x).sort_values()\ndfx = pd.DataFrame(sx, columns=['x'])\ndfx['rank'] = dfx.rank()\n\ndfx_capture = dfx.query('x &lt; x.quantile(0.9)')\ndfx_escaped= dfx.query('x &gt;= x.quantile(0.9)')\ng = sns.scatterplot(data = dfx_capture, x = 'x', y = 'rank', edgecolor = None, alpha = 0.05)\ng = sns.scatterplot(data = dfx_escaped, x = 'x', y = 'rank', edgecolor= None, color = 'red', alpha = 0.01)\ng.set_title('value x to rank plot is a symatric sigmoid curve')\n\nText(0.5, 1.0, 'value x to rank plot is a symatric sigmoid curve')\n\n\n\n\n\n\n\n\n\nPloting distribution as sigmoid. Should in theory against tail trim. This is useful. In biology experiments, some capture device is not particular good at capturing top 10% escape expert animals. As a result, sample will give us"
  },
  {
    "objectID": "2024/03-15-Time-Series/statlab - Normality of Data.html#intro",
    "href": "2024/03-15-Time-Series/statlab - Normality of Data.html#intro",
    "title": "Data Normality",
    "section": "",
    "text": "This notebook explore: - The R square against normal data - Norm data against lag - qq plot against normal data\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nplot_params = {'color': '0.75',\n 'style': '.-',\n 'markeredgecolor': '0.25',\n 'markerfacecolor': '0.25',\n 'legend': False}\n\nplt.style.use('seaborn-whitegrid')\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\n%config InlineBackend.figure_format = 'retina'\n\n/var/folders/r5/1cdq52mn21zdnqzl0fvp44zw0000gn/T/ipykernel_23399/1354677373.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n\n\n\n\n\n# explore linear correlation\ndef foo(x):\n    y = x * 3 + 1\n    return y\n\nx = np.arange(1, 4)\ny = np.apply_along_axis(foo, axis = 0, arr = x)\n\nif ((x - x.mean())**2).sum()/x.shape[0] == x.var():\n    print('var is not sqrt-ed variance')\nelse: \n    print('var is standard deviation')\n\ndef corrianda(X, Y):\n    c = ((X - X.mean())*(Y - Y.mean())).sum()\n    return(c)\ndef ssd(X):\n    c = X.var() * X.shape[0]\n    return(c)\ndef covar(X, Y):\n    c = corrianda(x, y) / np.sqrt(ssd(x) * ssd(y))\n    return(c)\n\nvar is not sqrt-ed variance\n\n\n\nx = np.arange(1, 10000)\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a perfect line has coefficency of {covar(x, y)}')\n\na perfect line has coefficency of 1.0\n\n\n\nx = np.random.normal(50, 10, 10000)\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a standard normal line has coefficency of {covar(x, y)}')\n\na standard normal line has coefficency of 1.0\n\n\n\nx = np.arange(1, 10000)**2\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a skewed data(power) has coefficency of {covar(x, y)}')\n\na skewed data(power) has coefficency of 1.0000000000000002\n\n\n\nx = np.log(np.arange(1, 10000))\ny = np.apply_along_axis(foo, axis=0, arr = x)\nprint(f'a skewed data(log) has coefficency of {covar(x, y)}')\n\na skewed data(log) has coefficency of 0.9999999999999999\n\n\n\n\n\n\ndf = pd.read_csv(\n    'data/output/Calenrier_output.csv',\n    parse_dates = ['date']\n)\ndf = df.set_index('date').to_period('D')\n\n\ndf.filter(regex='s\\(\\d+,\\d+\\)').iloc[0:15,:].plot(style = 'o')\ndf.filter(regex='sin|cos').iloc[0:15,:].plot(style = 'o:')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe s something seems to be modeling week\n\n\n\n\nx = pd.Series(np.random.normal(50, 10, 200))\n\n\nfig, (_) = plt.subplots(2, 3, figsize = (15,10) )\nlags = np.append(np.arange(5), 50)\nfor i in np.arange(len(lags)):\n    sns.scatterplot(\n        data = pd.DataFrame({\"x\":x, \"lag\" :x.shift(i)}),\n        x = 'x',\n        y = 'lag',\n        ax = _[i%2, i//2]\n    )\n    _[i%2, i//2].set_title(f'Lag {lags[i]} steps')\n\n\n\n\n\n\n\n\nWhat I realise about this a normal distributed plot (taking out in random orders) will not have auto-correlatio no matter what. This is because after taking out a random value, the distribution is still normal.\nAlternatively If I order x and let it leg this results might be very different:\n\nfig, (_) = plt.subplots(2, 3, figsize = (15,10) )\nlags = np.append(np.arange(5), 50)\nfor i in np.arange(len(lags)):\n    sns.scatterplot(\n        data = pd.DataFrame({\"x\":x.sort_values(), \"lag\" :x.sort_values().shift(i)}),\n        x = 'x',\n        y = 'lag',\n        ax = _[i%2, i//2]\n    )\n    _[i%2, i//2].set_title(f'Lag {lags[i]} steps')\n\n\n\n\n\n\n\n\nThink of this problem in very extrem ways. At very extrem. The order of a normally distributed variables are extremely random. There will be no pattern to lag at all.\nAt the other end, our variable are extremely ordered. There will be near perfect correcation between lag and time. (This correlation will be near one)\nFor a gross sample that is normal. The slop should only be between 0 to 1. Trimed tail should not affect result here.\nWhat does it means when any of your is extremely random\nI guess what by looking at distribution of something you really know what is going on underneath.\n\n\nUse qqplot\n\nimport statsmodels.api as sm\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (14, 7))\nsm.qqplot(x.sort_values().head(100), ax=ax1);\nax1.set_title('trail tails results in upward bending')\n# trail tails results in upward bending\nsm.qqplot(x.sort_values().tail(100), ax=ax2);\nax2.set_title('trim heads restuls in downward bending')\n# trim heads restuls in downward bending\n\nText(0.5, 1.0, 'trim heads restuls in downward bending')\n\n\n\n\n\n\n\n\n\nNormal data can be abnormal by missing some part.\n\n\n\n\nx = np.random.normal(50, 4, 10000)\nsx = pd.Series(x).sort_values()\ndfx = pd.DataFrame(sx, columns=['x'])\ndfx['rank'] = dfx.rank()\n\ndfx_capture = dfx.query('x &lt; x.quantile(0.9)')\ndfx_escaped= dfx.query('x &gt;= x.quantile(0.9)')\ng = sns.scatterplot(data = dfx_capture, x = 'x', y = 'rank', edgecolor = None, alpha = 0.05)\ng = sns.scatterplot(data = dfx_escaped, x = 'x', y = 'rank', edgecolor= None, color = 'red', alpha = 0.01)\ng.set_title('value x to rank plot is a symatric sigmoid curve')\n\nText(0.5, 1.0, 'value x to rank plot is a symatric sigmoid curve')\n\n\n\n\n\n\n\n\n\nPloting distribution as sigmoid. Should in theory against tail trim. This is useful. In biology experiments, some capture device is not particular good at capturing top 10% escape expert animals. As a result, sample will give us"
  },
  {
    "objectID": "2024/03-15-Time-Series/Learn -Time Series.html",
    "href": "2024/03-15-Time-Series/Learn -Time Series.html",
    "title": "Engineering Features for Time Series",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#pip install -U scikit-learn scipy\nfrom sklearn.linear_model import LinearRegression\n\n\nData\nkaggle timeserires tutorial\nstore sale data\n\n\nPython Enviroment Version\nI’ve created conda enviroment py10 for this running the python 3.10.4. Always use python --version to check if you are on py10. This should have package pyeath installed. Uninstalled conda. use Python 3.11.\n\n\nSetting Figures\n\nplot_params = {'color': '0.75',\n 'style': '.-',\n 'markeredgecolor': '0.25',\n 'markerfacecolor': '0.25',\n 'legend': False}\n\nplt.style.use('seaborn-whitegrid')\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\n%config InlineBackend.figure_format = 'retina'\n\n\n#pip install kaggle\n#!kaggle kernels output ryanholbrook/linear-regression-with-time-series -p data\ndf = pd.read_csv('data/store-sales-time-series-forecasting/train.csv',\n                 index_col='date',\n                 parse_dates=['date']\n                 )\ndf.info()\ntype(df.index)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 3000888 entries, 2013-01-01 to 2017-08-15\nData columns (total 5 columns):\n #   Column       Dtype  \n---  ------       -----  \n 0   id           int64  \n 1   store_nbr    int64  \n 2   family       object \n 3   sales        float64\n 4   onpromotion  int64  \ndtypes: float64(1), int64(3), object(1)\nmemory usage: 137.4+ MB\n\n\npandas.core.indexes.datetimes.DatetimeIndex\n\n\n\n\nTime Step Feature\nSimply index time as x. Day 1, Day 2 ect\n\n# Engineer a time step feature\nimport numpy as np\ngross_sale = df.groupby('date')[['sales']].sum()\ngross_sale['Time']=np.arange(len(gross_sale.index))\ngross_sale.head(3)\n\n\n\n\n\n\n\n\n\nsales\nTime\n\n\ndate\n\n\n\n\n\n\n2013-01-01\n2511.618999\n0\n\n\n2013-01-02\n496092.417944\n1\n\n\n2013-01-03\n361461.231124\n2\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n# scatter plot\nax.plot('Time', 'sales', data=gross_sale, color='0.75')\n# regression plot\nax = sns.regplot(x='Time', y='sales', data=gross_sale, ci=None, scatter_kws=dict(color='0.25'))\nax.set_title('Time Plot of Gross Sales');\n\n\n\n\n\n\n\n\nAdvantage of a timestep feature instead of just time is it scales verywell. So you don’t have to worry too much about does it matter we measure in date units or in month units.\n\n\nLag Features\nThis one is about use previous value to predict future recent value\n\ngross_sale['Lag_1'] = gross_sale['sales'].shift(1)\nfig, ax = plt.subplots()\nax = sns.regplot(x='Lag_1', y='sales', data=gross_sale, ci=None, scatter_kws=dict(color='0.25'))\nax.set_aspect('equal')\nax.set_title('Lag Plot of Hardcover Sales')\n\nText(0.5, 1.0, 'Lag Plot of Hardcover Sales')\n\n\n\n\n\n\n\n\n\nserial dependence high sales on one day usually means hig sales on the next day\n\nX = gross_sale.loc[:, ['Time']]\ny = gross_sale.loc[:, 'sales']\nprint('X looks like this: '), print(X.head(3)), print('...'), print(f\"X is {type(X)}\")\nprint('y looks like this: '), print(y.head(3)), print('...'), print(f\"y is {type(y)}\")\n# HINT: do you know now why when fit X they prefer use the higher X\n\nX looks like this: \n            Time\ndate            \n2013-01-01     0\n2013-01-02     1\n2013-01-03     2\n...\nX is &lt;class 'pandas.core.frame.DataFrame'&gt;\ny looks like this: \ndate\n2013-01-01      2511.618999\n2013-01-02    496092.417944\n2013-01-03    361461.231124\nName: sales, dtype: float64\n...\ny is &lt;class 'pandas.core.series.Series'&gt;\n\n\n(None, None, None, None)\n\n\n\n# fit a model be like:\nX = gross_sale.loc[:, ['Time']]\ny = gross_sale.loc[:, 'sales']\nmodel = LinearRegression()\nmodel.fit(X, y)\npred_y = pd.Series(model.predict(X), index = X.index)\n\n\n\nPlus: Multi-assign and Plot several plot\n\n# python hack about multi-assign\na, b = [1,2]\na, (b, c) = [1, (2, 3)]\n\n\n\nTrend\n\nData\n\ndf = pd.read_csv('data/store-sales-time-series-forecasting/train.csv',\n                        index_col = 'date',\n                        parse_dates = ['date']\n                        ).to_period('D')\naverage_sales = df.groupby('date')['sales'].mean()\n\n\naverage_sales.head()\n\ndate\n2013-01-01      1.409438\n2013-01-02    278.390807\n2013-01-03    202.840197\n2013-01-04    198.911154\n2013-01-05    267.873244\nFreq: D, Name: sales, dtype: float64\n\n\n\n\nMoving Average\nMoving average is the idea of observe overage value within a window of time frame. But instead of those windows being mutually exclusive, those windows roll on.\n\nlen(average_sales.loc['2013'])\n\n364\n\n\nIn the gross_sale data, one enclose cycle is one years. So window should be set as 360. minimum periods is typically half this window (not sure why)\n\nmoving_average = average_sales.rolling(\n    window = 364,\n    center = True,\n    min_periods=183\n).mean()\nax = average_sales.plot(style = '.', color = '0.5')\nmoving_average.plot(\n    ax = ax,\n    linewidth = 3,\n    title = 'Ploting Moving Average'\n)\n\n\n\n\n\n\n\n\n\nmoving_average.sample(3), gross_sale.sample(3)\n\n(date\n 2015-12-20    437.490098\n 2014-04-11    288.010483\n 2015-12-17    436.879765\n Freq: D, Name: sales, dtype: float64,\n                    sales  Time          Lag_1\n date                                         \n 2016-05-25  6.375120e+05  1237  606377.205216\n 2015-03-24  4.073697e+05   810  462664.237004\n 2016-04-02  1.150825e+06  1184  872467.320075)\n\n\n\n\nDeterministic Model\nwhich is a fancy term for linear regression with time series. This is also a linear model. To make this model work requries you to converge time index to_period index.\nThis process is similar to linear model. Instead fitting x, y, z three independ varaibles, you are fitting three x of different ‘orders’.\n\\[\ny = a + b\\,x + c\\,x^2 + d\\,x^3 + ... + n\\,x^{n}\n\\]\n(Somewhat reminds me of Taylor’s series. Maybe that’s what order mans) What’s interesting about this function is odd quatric functions can vary ups and downs, so to fit into any shape you like.\n\n#pip install statsmodels\nfrom statsmodels.tsa.deterministic import DeterministicProcess\n\ny = average_sales.copy()\ndp = DeterministicProcess(\n    index=y.index,\n    constant = True, # dummy features\n    order = 3,       # time dummy trend, 1 is linear, 2 is quadratic, 3 cubic\n    drop = True\n)\nX = dp.in_sample()\nX.tail()\n\n\n\n\n\n\n\n\n\nconst\ntrend\ntrend_squared\ntrend_cubed\n\n\ndate\n\n\n\n\n\n\n\n\n2017-08-11\n1.0\n1680.0\n2822400.0\n4.741632e+09\n\n\n2017-08-12\n1.0\n1681.0\n2825761.0\n4.750104e+09\n\n\n2017-08-13\n1.0\n1682.0\n2829124.0\n4.758587e+09\n\n\n2017-08-14\n1.0\n1683.0\n2832489.0\n4.767079e+09\n\n\n2017-08-15\n1.0\n1684.0\n2835856.0\n4.775582e+09\n\n\n\n\n\n\n\n\n\nX_fore = dp.out_of_sample(steps = 90)\nX_fore.head()\n\n\n\n\n\n\n\n\n\nconst\ntrend\ntrend_squared\ntrend_cubed\n\n\n\n\n2017-08-16\n1.0\n1685.0\n2839225.0\n4.784094e+09\n\n\n2017-08-17\n1.0\n1686.0\n2842596.0\n4.792617e+09\n\n\n2017-08-18\n1.0\n1687.0\n2845969.0\n4.801150e+09\n\n\n2017-08-19\n1.0\n1688.0\n2849344.0\n4.809693e+09\n\n\n2017-08-20\n1.0\n1689.0\n2852721.0\n4.818246e+09\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\ny = average_sales\nmodel = LinearRegression(fit_intercept = False)\nmodel.fit(X, y)\ny_pred = pd.Series(model.predict(X), index = X.index)\ny_fore = pd.Series(model.predict(X_fore), index = X_fore.index)\n\n\nax = average_sales.plot(\n    style = '.', color = '0.5', title = 'average sales'\n)\nax = y_pred.plot(ax= ax, linewidth=3, label='Trend') # underscore is for temporary variable\nax = y_fore.plot(ax = ax, linewidth=3, label=\"Trend Forcast\", color = 'C3')\n\n\n\n\n\n\n\n\n\n\nRisks of Highorder Ploynomials\nDue to the property of function &gt; An order 11 polynomial will include terms like t ** 11. Terms like these tend to diverge rapidly outside of the training period making forecasts very unreliable.\n\ndp = DeterministicProcess(\n    index=y.index,\n    order = 11,       # time dummy trend, 1 is linear, 2 is quadratic, 3 cubic\n)\nX = dp.in_sample()\n\nmodel = LinearRegression(fit_intercept = True)\nmodel.fit(X, y)\n\nX_fore = dp.out_of_sample(steps=90)\ny_pred = pd.Series(model.predict(X), index=X.index)\ny_fore = pd.Series(model.predict(X_fore), index = X_fore.index)\n\nax = y.plot(style = '.', alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\")\nax = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\", color='C0')\nax = y_fore.plot(ax=ax, linewidth=3, label=\"Trend Forecast\", color='C3')\nax.legend();\n\n\n\n\n\n\n\n\n\n\nFit Trend with Spines\nMultivariate Adaptive Regression Splines (MARS) &gt; Splines are a nice alternative to polynomials when you want to fit a trend. The Multivariate Adaptive Regression Splines (MARS) algorithm in the pyearth library is powerful and easy to use. There are a lot of hyperparameters you may want to investigate.\nThis use Earth() model in pyearth package by Stephen Milborrow, the originsl is R version. API here You will need to install via conda. Use the one under scikit-learn.\nconda install pip install sklearn-contrib-py-earth\n\n#from pyearth import Earth\ntry: \n    y = average_sales.copy()\n    dp = DeterministicProcess(index=y.index, order=1)\n    X = dp.in_sample()\n\n    # Fit a MARS model with `Earth`\n\n    model = Earth()\n    model.fit(X, y)\n\n    y_pred = pd.Series(model.predict(X), index=X.index)\n\n    ax = y.plot(#**plot_params, \n                title=\"Average Sales\", ylabel=\"items sold\")\n    ax = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\")\nexcept: \n    print('This code will no execute until pyearth is installed')\n\nThis code will no execute until pyearth is installed\n\n\n\n\n\nSeasonality\nYou already know sine and cosine functions are used to model these. Terms are called Fourtier features.\n\nFourier Features\n1 pair of fourier features are: \\[\nk =  2 \\pi \\frac{t}{f}\n\\\\\nf(j) = \\beta_1 \\sin(j * k) + \\beta_2 \\cos(j * k)\n\\]\n\\(n\\) order(s) of fourier features: \\[\nF(n) = \\sum_{j=1}^{n} f(j)\n\\]\n\n\\(k\\) is time scaled to frequency\n\\(i\\) is order of features\n\\(\\beta_1\\) and \\(\\beta_2\\) is what you throw into linear regression\n\nThe advantage of a fourier pair is so that two parameters are at the same scale.\n\n# create fourier feature for linear regression to figure out\ndef fourier_features(index, freq, order):\n    time = np.arange(len(index), dtype=np.float32)\n    k = 2 * np.pi * (1 / freq) * time\n    features = {}\n    for i in range(1, order + 1):\n        features.update({\n            f\"sin_{freq}_{i}\": np.sin(i * k),\n            f\"cos_{freq}_{i}\": np.cos(i * k),\n        })\n    return pd.DataFrame(features, index=index)\n# this transform it into something you are easy to fits into linear regression\n\n\n\nPeriodogram\nGiven frequency y = $ {2}$ \\(\\beta_1\\), \\(\\beta_2\\) is the coefficients of sine and cosine.\nA useful trigonometric identity is is: \\[\nA \\cos(2 \\pi \\omega t + \\phi) = \\beta_1 \\cos(2 \\pi \\omega t) + \\beta_2 \\sin(2 \\pi \\omega t) \\\\\n\\beta_1 = A \\cos(\\phi) \\\\\n\\beta_2 = - A \\sin(\\phi) \\\\\n2A^2 = \\beta_1^2 + \\beta_2^2\n\\] The whole time series is represented as: \\[\nx_t = \\sum_{j = 1}^{n/2}\n      [\n        \\beta_1(\\frac{j}{n}) cos(2 \\pi \\omega_j t)+\n        \\beta_2(\\frac{j}{n}) sin(2 \\pi \\omega_j t)\n      ]\n\\] In periodogram given \\(\\frac{j} {n}\\) frequency: \\[\nP(\\frac{j} {n}) = \\beta_1^2 (\\frac{j} {n}) + \\beta_2^2(\\frac{j}{n})\n\\]\n\nA relatively large value of P(j/n) indicates relatively more importance for the frequency j/n (or near j/n) in explaining the oscillation in the observed series. P(j/n) is proportional to the squared correlation between the observed series and a cosine wave with frequency j/n. The dominant frequencies might be used to fit cosine (or sine) waves to the data, or might be used simply to describe the important periodicities in the series.\n\nsource: PennState Eberly College of Science\n\nEstimate \\(\\beta_1\\) and \\(\\beta_2\\) is two of n parameters\nThey are not neccessary estimated by regression but this math device called Fast Fourier Transformation (FFT)\n\n\n\n(Fast) Fourier Transformation\n\n\n\nimage\n\n\nI think of it this way. Think you can fit a n sum of fontier pairs together. Your frequency is then coposed of n possible pair of fourier pairs. The higher order, the higher the freqency (lower the wave length). Some fourier will be more dominant than the other. When a fourier pair is not dominant, their sum of \\(\\beta\\) square may as well be 0. This means it cancels them out. So if you slice the equation by fourier pairs. Each pair will represent strenght of that wave function. With 0 indicate very low effect. Higher value indicate more dominate effect.\ndig further: what is fourier transform\n\nCustom Functions\n\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\ndef seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax\n\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram( # this code do not generate graph it creates two vectors\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\n\n\nExample\n\naverage_sales_2017 = average_sales.squeeze().loc['2017']\nX = average_sales_2017.to_frame()\nX['week'] = X.index.week\nX['day'] = X.index.dayofweek\nseasonal_plot(X, \n              'sales',\n              period = 'week',\n              freq = 'day')\nplot_periodogram(average_sales_2017)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTE: This set of features happens to have four spikes and the four spkes happens to be consecutive\nNote x is completely different scale to what’s introduce in PenState’s text book (where frequency is represented as a fraction). Here frequency is represented as the inverse whole period with low frequency left and high frequency right.\nThis graph tells you strong weekly seasonality. Because you want to reduce chances of over fitting. You want to try reduce the fourier sets. You can model these ways:\n\n12 month frequency (30/365) with 4 fourier pairs\n1 anual frequency (364/365) with 26 fourier paris (there will be a lot of frequency here)\n\nSet frequency as how you want period to return. As order increase it captures the intrinsic orders of the frequency.\nSo in the one the first fourier feature is observed monthly hense the set frequency to month. There are about four additionaly waves (they all happens to be twice as frequent as the other one). Hense we set four fourier pairs.\n\n\n\n\nCompute Fourier Feature (statmodels)\n\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfourier = CalendarFourier(freq=\"M\", order=4) # here parameter derived from periodogram\ndp = DeterministicProcess(\n    index=average_sales_2017.index,\n    constant=True,               # dummy feature for bias (y-intercept)\n    order=1,                     # trend (order 1 means linear)\n    seasonal=True,               # weekly seasonality (indicators)\n    additional_terms=[fourier],  # annual seasonality (fourier)\n    drop=True,                   # drop terms to avoid collinearity\n)\nX = dp.in_sample()  \n# note fourier needs to be made as a list for it to be literatable\ny = average_sales_2017\nmodel = LinearRegression(fit_intercept=False)\n_ = model.fit(X, y)\ny_pred = pd.Series(model.predict(X), index=y.index)\nX_fore = dp.out_of_sample(steps=90)\ny_fore = pd.Series(model.predict(X_fore), index=X_fore.index)\nax = y.plot(color='0.25', style='.-', alpha = 0.25,title=\"Store Average Sales\")\nax = y_pred.plot(ax=ax, label=\"Seasonal\")\nax = y_fore.plot(ax=ax, label=\"Seasonal Forecast\", color='C3')\n_ = ax.legend();\n\n\n\n\n\n\n\n\n\n#X.to_csv('data/output/Calenrier_output.csv')\n\n\n\nDetrend or deseasonalising\nVerify that we are not modeling random variance\n\ny_deseason = y - y_pred\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(10, 7))\nax1 = plot_periodogram(y, ax=ax1)\nax1.set_title(\"Product Sales Frequency Components\")\nax2 = plot_periodogram(y_deseason, ax=ax2)\nax2.set_title(\"Deseasonalized\")\nax3 = plot_periodogram(y, ax=ax3)\n#ax.axes.set_facecolor('blue')\nax3 = plot_periodogram(y_deseason, ax=ax3)\nax3.axes.lines[0].set_color('blue')\nax3.set_title(\"Product Sales Frequency Components\"); # take this out you would be creating a new plot\n\n\n\n\n\n\n\n\nThis plot shows that our model ahs surverred very well in explaining seasonality variance.\n\nHoliday (Special Events)\nYou can fit spacial events by creating dummy variables (here it is convinent because we are only useing one years to train)\n\nholidays_events = pd.read_csv(\n    'data/store-sales-time-series-forecasting/holidays_events.csv',\n    index_col = 'date',\n    dtype={\n        'type': 'category',\n        'locale': 'category',\n        'locale_name': 'category',\n        'description': 'category',\n        'transferred': 'bool',\n    },\n    parse_dates = ['date'],\n    infer_datetime_format=True\n    ).to_period('D')\n#type(holidays_events.index)\nholidays = (\n    holidays_events\n    .query(\"locale in ['National', 'Regional']\")\n    .loc['2017':'2017-08-15', ['description']]\n    .assign(description=lambda x: x.description.cat.remove_unused_categories())\n)\ndisplay(holidays)\n\n\n\n\n\n\n\n\n\ndescription\n\n\ndate\n\n\n\n\n\n2017-01-01\nPrimer dia del ano\n\n\n2017-01-02\nTraslado Primer dia del ano\n\n\n2017-02-27\nCarnaval\n\n\n2017-02-28\nCarnaval\n\n\n2017-04-01\nProvincializacion de Cotopaxi\n\n\n2017-04-14\nViernes Santo\n\n\n2017-05-01\nDia del Trabajo\n\n\n2017-05-13\nDia de la Madre-1\n\n\n2017-05-14\nDia de la Madre\n\n\n2017-05-24\nBatalla de Pichincha\n\n\n2017-05-26\nTraslado Batalla de Pichincha\n\n\n2017-06-25\nProvincializacion de Imbabura\n\n\n2017-08-10\nPrimer Grito de Independencia\n\n\n2017-08-11\nTraslado Primer Grito de Independencia\n\n\n\n\n\n\n\n\n\nax = y_deseason.plot(**plot_params)\nplt.plot_date(holidays.index, y_deseason[holidays.index], color='C3')\nax.set_title('National and Regional Holidays');\n\n\n\n\n\n\n\n\n\nX_holidays = pd.get_dummies(\n    holidays,\n    columns = ['description']\n)\nX2 = X.join(X_holidays, on='date').fillna(0.0)\nmodel = LinearRegression().fit(X2, y)\ny_pred = pd.Series(\n    model.predict(X2),\n    index = X2.index,\n    name = 'Fitted',\n)\n\n\nX_holidays.sample(3)\n\n\n\n\n\n\n\n\n\ndescription_Batalla de Pichincha\ndescription_Carnaval\ndescription_Dia de la Madre\ndescription_Dia de la Madre-1\ndescription_Dia del Trabajo\ndescription_Primer Grito de Independencia\ndescription_Primer dia del ano\ndescription_Provincializacion de Cotopaxi\ndescription_Provincializacion de Imbabura\ndescription_Traslado Batalla de Pichincha\ndescription_Traslado Primer Grito de Independencia\ndescription_Traslado Primer dia del ano\ndescription_Viernes Santo\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-01\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n2017-08-10\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2017-08-11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\n\n\nax = y.plot(**plot_params, alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\")\nax = y_pred.plot(ax=ax, label=\"Seasonal\")\nax.legend();\n\n\n\n\n\n\n\n\n\nax = y.plot(color='0.25', style='.-', alpha = 0.25,title=\"Store Average Sales\")\nax = y_pred.plot(ax=ax, label=\"Seasonal\")\nax = y_fore.plot(ax=ax, label=\"Seasonal Forecast\", color='C3')\nax = plt.plot_date(holidays.index, y[holidays.index], color = 'C3');\n\n\n\n\n\n\n\n\n\n\n\nTime Series as Features\n\nPartial Autocorrelcation\n\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\nplot_pacf(average_sales_2017);\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/graphics/tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\nCycle\n\n\nHybrid Models\nEssentially all above combined:\nseries = trend + seasons + cycles + error\nRegression algorithmn: * transform target: * for example decision tree. * group target value in training and make prediction of feature by averaging values in a group * transform features: * for example polynormial function. Use mathmatical function. * featues as input combines and transform\nFeature Transformer Extrapolate Target Value (think of this as a point inbetween two discrete value amongst a function) beyound bondary of training set. Same cannot be said for Decision Tree. Random Forest and Gradient boosted decision tree takes the last step.\n#kaggle recommend: 1. linear regression for extrapolate trend 2. transform target to remove trend 3. apply XGBoost to detrended residual\n\nlinbear regression + XGBosst\n\nfrom pathlib import Path\nfrom warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom xgboost import XGBRegressor\n\n\nsimplefilter(\"ignore\")\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n)\n\ndata_dir = Path(\"data/ts-course/\")\nindustries = [\"BuildingMaterials\", \"FoodAndBeverage\"]\nretail = pd.read_csv(\n    data_dir / \"us-retail-sales.csv\",\n    usecols=['Month'] + industries,\n    parse_dates=['Month'],\n    index_col='Month',\n).to_period('D').reindex(columns=industries)\nretail = pd.concat({'Sales': retail}, names=[None, 'Industries'], axis=1) # this is a hayer of hierarachical index\n\nretail.head()\n\n\n\n\n\n\n\n\n\nSales\n\n\nIndustries\nBuildingMaterials\nFoodAndBeverage\n\n\nMonth\n\n\n\n\n\n\n1992-01-01\n8964\n29589\n\n\n1992-02-01\n9023\n28570\n\n\n1992-03-01\n10608\n29682\n\n\n1992-04-01\n11630\n30228\n\n\n1992-05-01\n12327\n31677\n\n\n\n\n\n\n\n\nTips: now retail is hierarchically indexed you can only have to access a top layer column before you can access a bottom layer.\n\ny = retail.copy()\ndp = DeterministicProcess(\n    index=y.index,\n    constant=True,\n    order=2,\n    drop=True\n)\nX = dp.in_sample() # features for training data\n\nidx_train, idx_test = train_test_split(\n    y.index, test_size=12 * 4, shuffle=False,\n)\nX_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\ny_train, y_test = y.loc[idx_train], y.loc[idx_test]\n\n# Fit trend model\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_fit = pd.DataFrame(\n    model.predict(X_train),\n    index=y_train.index, # index are index\n    columns=y_train.columns, # columns are column labels\n)\ny_pred = pd.DataFrame(\n    model.predict(X_test),\n    index=y_test.index,\n    columns=y_test.columns,\n)\n\n\n# Plot\naxs = y_train.plot(color='0.25', subplots=True, sharex=True)\naxs = y_test.plot(color='0.25', subplots=True, sharex=True, ax=axs)\naxs = y_fit.plot(color='C0', subplots=True, sharex=True, ax=axs)\naxs = y_pred.plot(color='C3', subplots=True, sharex=True, ax=axs)\nfor ax in axs: ax.legend([])\n_ = plt.suptitle(\"Trends\")\n\n\n\n\n\n\n\n\nData Transformation before add XGBOOST\n\nWhile the linear regression algorithm is capable of multi-output regression, the XGBoost algorithm is not. To predict multiple series at once with XGBoost, we’ll instead convert these series from wide format, with one time series per column, to long format, with series indexed by categories along rows.\n\n\nX = retail.stack()  # pivot dataset wide to long\ndisplay(X.head())\ny = X.pop('Sales')\n\n\n\n\n\n\n\n\n\n\nSales\n\n\nMonth\nIndustries\n\n\n\n\n\n1992-01-01\nBuildingMaterials\n8964\n\n\nFoodAndBeverage\n29589\n\n\n1992-02-01\nBuildingMaterials\n9023\n\n\nFoodAndBeverage\n28570\n\n\n1992-03-01\nBuildingMaterials\n10608\n\n\n\n\n\n\n\n\n\nprint(\"stack transform default retaill index \\n from {var1} \\n to {var2} \\\n      ( y index)\".format(\nvar1 =type(retail.head().index),\nvar2 =type(y.head().index)\n))\n\nstack transform default retaill index \n from &lt;class 'pandas.core.indexes.period.PeriodIndex'&gt; \n to &lt;class 'pandas.core.indexes.multi.MultiIndex'&gt;       ( y index)\n\n\n\n# Turn row labels into categorical feature columns with a label encoding\nX = X.reset_index('Industries')\nprint(\"X.index is now {var1}\".format(var1=type(X.index)))\n# Label encoding for 'Industries' feature\nfor colname in X.select_dtypes([\"object\", \"category\"]):\n    X[colname], _ = X[colname].factorize()\n\n# Label encoding for annual seasonality\nX[\"Month\"] = X.index.month  # values are 1, 2, ..., 12\n\n# Create splits\nX_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\ny_train, y_test = y.loc[idx_train], y.loc[idx_test]\n\nX.index is now &lt;class 'pandas.core.indexes.period.PeriodIndex'&gt;\n\n\n\n# Pivot wide to long (stack) and convert DataFrame to Series (squeeze)\ny_fit = y_fit.stack().squeeze()    # trend from training set\ny_pred = y_pred.stack().squeeze()  # trend from test set\n\nprint(\"y is now {y_index}\".format(\n    y_index = type(y_fit.index)\n))\n\n# Create residuals (the collection of detrended series) from the training set\ny_resid = y_train - y_fit\n\n# Train XGBoost on the residuals\nxgb = XGBRegressor()\nxgb.fit(X_train, y_resid)\n\n# Add the predicted residuals onto the predicted trends\ny_fit_boosted = xgb.predict(X_train) + y_fit\ny_pred_boosted = xgb.predict(X_test) + y_pred\n\ny is now &lt;class 'pandas.core.indexes.multi.MultiIndex'&gt;\n\n\n\naxs = y_train.unstack(['Industries']).plot(\n    color='0.25', figsize=(11, 5), subplots=True, sharex=True,\n    title=['BuildingMaterials', 'FoodAndBeverage'],\n)\naxs = y_test.unstack(['Industries']).plot(\n    color='0.25', subplots=True, sharex=True, ax=axs,\n)\naxs = y_fit_boosted.unstack(['Industries']).plot(\n    color='C0', subplots=True, sharex=True, ax=axs,\n)\naxs = y_pred_boosted.unstack(['Industries']).plot(\n    color='C3', subplots=True, sharex=True, ax=axs,\n)\nfor ax in axs: ax.legend([])\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\nfrom pathlib import Path\nfrom warnings import simplefilter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\n\ndef plot_multistep(y, every=1, ax=None, palette_kwargs=None):\n    palette_kwargs_ = dict(palette='husl', n_colors=16, desat=None)\n    if palette_kwargs is not None:\n        palette_kwargs_.update(palette_kwargs)\n    palette = sns.color_palette(**palette_kwargs_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.set_prop_cycle(plt.cycler('color', palette))\n    for date, preds in y[::every].iterrows():\n        preds.index = pd.period_range(start=date, periods=len(preds))\n        preds.plot(ax=ax)\n    return ax\n\nflu_trends = pd.read_csv(\"data/ts-course/flu-trends.csv\")\nflu_trends.set_index(\n    pd.PeriodIndex(flu_trends.Week, freq = 'W'),\n    inplace = True\n)\nflu_trends.drop(\"Week\", axis=1, inplace=True)\n\n\ndef make_lags(ts, lags, lead_time=1):\n    return pd.concat(\n        {\n            f'y_lag_{i}': ts.shift(i)\n            for i in range(lead_time, lags + lead_time)\n        },\n        axis=1)\n\ny = flu_trends.FluVisits.copy() #setting this up any change to flu_trend will be copied\nX = make_lags(y, lags=4).fillna(0.0)\n\n\ndef make_multistep_target(ts, steps):\n    return pd.concat(\n        {f'y_step_{i + 1}': ts.shift(-i)\n         for i in range(steps)},\n        axis=1)\ny = make_multistep_target(y, steps=8).dropna()\ny, X = y.align(X, join= \"inner\", axis = 0) # align make index of x and y homogenious\n\nto read more about align here\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns)\ny_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\n\n\ntrain_rmse = mean_squared_error(y_train, y_fit, squared=False)\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))\n\npalette = dict(palette='husl', n_colors=64)\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6))\nax1 = flu_trends.FluVisits[y_fit.index].plot(**plot_params, ax=ax1)\nax1 = plot_multistep(y_fit, ax=ax1, palette_kwargs=palette)\n_ = ax1.legend(['FluVisits (train)', 'Forecast'])\nax2 = flu_trends.FluVisits[y_pred.index].plot(**plot_params, ax=ax2)\nax2 = plot_multistep(y_pred, ax=ax2, palette_kwargs=palette)\n_ = ax2.legend(['FluVisits (test)', 'Forecast'])\n\nTrain RMSE: 389.12\nTest RMSE: 582.33\n\n\n\n\n\n\n\n\n\n\nDirect Strategy - Use XGBoost regreesor\nHere is short cut recursively produce mutli output regression model. &gt; XGBoost can’t produce multiple outputs for regression tasks. But by applying the Direct reduction strategy, we can still use it to produce multi-step forecasts. This is as easy as wrapping it with scikit-learn’s MultiOutputRegressor.\n\nfrom sklearn.multioutput import MultiOutputRegressor\n\nmodel = MultiOutputRegressor(XGBRegressor()) #cheat code\nmodel.fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns)\ny_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\n\n\n#X_train.sample(3)\ny_train.sample(3)\n\n\n\n\n\n\n\n\n\ny_step_1\ny_step_2\ny_step_3\ny_step_4\ny_step_5\ny_step_6\ny_step_7\ny_step_8\n\n\nWeek\n\n\n\n\n\n\n\n\n\n\n\n\n2013-07-08/2013-07-14\n11\n13.0\n14.0\n10.0\n13.0\n13.0\n13.0\n22.0\n\n\n2014-05-19/2014-05-25\n80\n61.0\n47.0\n33.0\n27.0\n19.0\n22.0\n19.0\n\n\n2012-12-24/2012-12-30\n2961\n2303.0\n2291.0\n2258.0\n2012.0\n1510.0\n1134.0\n930.0\n\n\n\n\n\n\n\n\n\ntrain_rmse = mean_squared_error(y_train, y_fit, squared=False)\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))\n\npalette = dict(palette='husl', n_colors=64)\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6))\nax1 = flu_trends.FluVisits[y_fit.index].plot(**plot_params, ax=ax1)\nax1 = plot_multistep(y_fit, ax=ax1, palette_kwargs=palette)\n_ = ax1.legend(['FluVisits (train)', 'Forecast'])\nax2 = flu_trends.FluVisits[y_pred.index].plot(**plot_params, ax=ax2)\nax2 = plot_multistep(y_pred, ax=ax2, palette_kwargs=palette)\n_ = ax2.legend(['FluVisits (test)', 'Forecast'])\n\nTrain RMSE: 1.22\nTest RMSE: 526.45\n\n\n\n\n\n\n\n\n\nRegressorChain() versus MultiOutputRegressor"
  },
  {
    "objectID": "2024/02-09 Kaggle Competition/kaggle-2024-credit-risks/fl-02-08.html",
    "href": "2024/02-09 Kaggle Competition/kaggle-2024-credit-risks/fl-02-08.html",
    "title": "What data look like?",
    "section": "",
    "text": "import pandas as pd\nimport os\nfrom directory_tree import display_tree\nfrom sklearn.metrics import roc_auc_score\nfrom pathlib import Path\nimport duckdb\n\nDATA_FOLDER=\"home-credit-credit-risk-model-stability\"\nassert Path(DATA_FOLDER).exists()\ndisplay_tree(DATA_FOLDER)\n\nhome-credit-credit-risk-model-stability/\n├── csv_files/\n│   ├── test/\n│   │   ├── test_applprev_1_0.csv\n│   │   ├── test_applprev_1_1.csv\n│   │   ├── test_applprev_1_2.csv\n│   │   ├── test_applprev_2.csv\n│   │   ├── test_base.csv\n│   │   ├── test_credit_bureau_a_1_0.csv\n│   │   ├── test_credit_bureau_a_1_1.csv\n│   │   ├── test_credit_bureau_a_1_2.csv\n│   │   ├── test_credit_bureau_a_1_3.csv\n│   │   ├── test_credit_bureau_a_1_4.csv\n│   │   ├── test_credit_bureau_a_2_0.csv\n│   │   ├── test_credit_bureau_a_2_1.csv\n│   │   ├── test_credit_bureau_a_2_10.csv\n│   │   ├── test_credit_bureau_a_2_11.csv\n│   │   ├── test_credit_bureau_a_2_2.csv\n│   │   ├── test_credit_bureau_a_2_3.csv\n│   │   ├── test_credit_bureau_a_2_4.csv\n│   │   ├── test_credit_bureau_a_2_5.csv\n│   │   ├── test_credit_bureau_a_2_6.csv\n│   │   ├── test_credit_bureau_a_2_7.csv\n│   │   ├── test_credit_bureau_a_2_8.csv\n│   │   ├── test_credit_bureau_a_2_9.csv\n│   │   ├── test_credit_bureau_b_1.csv\n│   │   ├── test_credit_bureau_b_2.csv\n│   │   ├── test_debitcard_1.csv\n│   │   ├── test_deposit_1.csv\n│   │   ├── test_other_1.csv\n│   │   ├── test_person_1.csv\n│   │   ├── test_person_2.csv\n│   │   ├── test_static_0_0.csv\n│   │   ├── test_static_0_1.csv\n│   │   ├── test_static_0_2.csv\n│   │   ├── test_static_cb_0.csv\n│   │   ├── test_tax_registry_a_1.csv\n│   │   ├── test_tax_registry_b_1.csv\n│   │   └── test_tax_registry_c_1.csv\n│   └── train/\n│       ├── train_applprev_1_0.csv\n│       ├── train_applprev_1_1.csv\n│       ├── train_applprev_2.csv\n│       ├── train_base.csv\n│       ├── train_credit_bureau_a_1_0.csv\n│       ├── train_credit_bureau_a_1_1.csv\n│       ├── train_credit_bureau_a_1_2.csv\n│       ├── train_credit_bureau_a_1_3.csv\n│       ├── train_credit_bureau_a_2_0.csv\n│       ├── train_credit_bureau_a_2_1.csv\n│       ├── train_credit_bureau_a_2_10.csv\n│       ├── train_credit_bureau_a_2_2.csv\n│       ├── train_credit_bureau_a_2_3.csv\n│       ├── train_credit_bureau_a_2_4.csv\n│       ├── train_credit_bureau_a_2_5.csv\n│       ├── train_credit_bureau_a_2_6.csv\n│       ├── train_credit_bureau_a_2_7.csv\n│       ├── train_credit_bureau_a_2_8.csv\n│       ├── train_credit_bureau_a_2_9.csv\n│       ├── train_credit_bureau_b_1.csv\n│       ├── train_credit_bureau_b_2.csv\n│       ├── train_debitcard_1.csv\n│       ├── train_deposit_1.csv\n│       ├── train_other_1.csv\n│       ├── train_person_1.csv\n│       ├── train_person_2.csv\n│       ├── train_static_0_0.csv\n│       ├── train_static_0_1.csv\n│       ├── train_static_cb_0.csv\n│       ├── train_tax_registry_a_1.csv\n│       ├── train_tax_registry_b_1.csv\n│       └── train_tax_registry_c_1.csv\n├── feature_definitions.csv\n├── parquet_files/\n│   ├── test/\n│   │   ├── test_applprev_1_0.parquet\n│   │   ├── test_applprev_1_1.parquet\n│   │   ├── test_applprev_1_2.parquet\n│   │   ├── test_applprev_2.parquet\n│   │   ├── test_base.parquet\n│   │   ├── test_credit_bureau_a_1_0.parquet\n│   │   ├── test_credit_bureau_a_1_1.parquet\n│   │   ├── test_credit_bureau_a_1_2.parquet\n│   │   ├── test_credit_bureau_a_1_3.parquet\n│   │   ├── test_credit_bureau_a_1_4.parquet\n│   │   ├── test_credit_bureau_a_2_0.parquet\n│   │   ├── test_credit_bureau_a_2_1.parquet\n│   │   ├── test_credit_bureau_a_2_10.parquet\n│   │   ├── test_credit_bureau_a_2_11.parquet\n│   │   ├── test_credit_bureau_a_2_2.parquet\n│   │   ├── test_credit_bureau_a_2_3.parquet\n│   │   ├── test_credit_bureau_a_2_4.parquet\n│   │   ├── test_credit_bureau_a_2_5.parquet\n│   │   ├── test_credit_bureau_a_2_6.parquet\n│   │   ├── test_credit_bureau_a_2_7.parquet\n│   │   ├── test_credit_bureau_a_2_8.parquet\n│   │   ├── test_credit_bureau_a_2_9.parquet\n│   │   ├── test_credit_bureau_b_1.parquet\n│   │   ├── test_credit_bureau_b_2.parquet\n│   │   ├── test_debitcard_1.parquet\n│   │   ├── test_deposit_1.parquet\n│   │   ├── test_other_1.parquet\n│   │   ├── test_person_1.parquet\n│   │   ├── test_person_2.parquet\n│   │   ├── test_static_0_0.parquet\n│   │   ├── test_static_0_1.parquet\n│   │   ├── test_static_0_2.parquet\n│   │   ├── test_static_cb_0.parquet\n│   │   ├── test_tax_registry_a_1.parquet\n│   │   ├── test_tax_registry_b_1.parquet\n│   │   └── test_tax_registry_c_1.parquet\n│   └── train/\n│       ├── train_applprev_1_0.parquet\n│       ├── train_applprev_1_1.parquet\n│       ├── train_applprev_2.parquet\n│       ├── train_base.parquet\n│       ├── train_credit_bureau_a_1_0.parquet\n│       ├── train_credit_bureau_a_1_1.parquet\n│       ├── train_credit_bureau_a_1_2.parquet\n│       ├── train_credit_bureau_a_1_3.parquet\n│       ├── train_credit_bureau_a_2_0.parquet\n│       ├── train_credit_bureau_a_2_1.parquet\n│       ├── train_credit_bureau_a_2_10.parquet\n│       ├── train_credit_bureau_a_2_2.parquet\n│       ├── train_credit_bureau_a_2_3.parquet\n│       ├── train_credit_bureau_a_2_4.parquet\n│       ├── train_credit_bureau_a_2_5.parquet\n│       ├── train_credit_bureau_a_2_6.parquet\n│       ├── train_credit_bureau_a_2_7.parquet\n│       ├── train_credit_bureau_a_2_8.parquet\n│       ├── train_credit_bureau_a_2_9.parquet\n│       ├── train_credit_bureau_b_1.parquet\n│       ├── train_credit_bureau_b_2.parquet\n│       ├── train_debitcard_1.parquet\n│       ├── train_deposit_1.parquet\n│       ├── train_other_1.parquet\n│       ├── train_person_1.parquet\n│       ├── train_person_2.parquet\n│       ├── train_static_0_0.parquet\n│       ├── train_static_0_1.parquet\n│       ├── train_static_cb_0.parquet\n│       ├── train_tax_registry_a_1.parquet\n│       ├── train_tax_registry_b_1.parquet\n│       └── train_tax_registry_c_1.parquet\n└── sample_submission.csv\n\n\n\nDATA_DIR='home-credit-credit-risk-model-stability/parquet_files/train'\np = Path(DATA_DIR)\n\n\nimport ibis\nimport ibis\nimport ibis.selectors as s\n\n# Set up ibis\nibis.options.interactive = True\n\n\n\nassert (p / 'train_base.parquet').exists\nprint('Your is training target is column `target`')\nibis.read_parquet((p / 'train_base.parquet')).head(3)\n\nYour is training target is column `target`\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┓\n┃ case_id ┃ date_decision ┃ MONTH  ┃ WEEK_NUM ┃ target ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━┩\n│ int64   │ string        │ int64  │ int64    │ int64  │\n├─────────┼───────────────┼────────┼──────────┼────────┤\n│       0 │ 2019-01-03    │ 201901 │        0 │      0 │\n│       1 │ 2019-01-03    │ 201901 │        0 │      0 │\n│       2 │ 2019-01-04    │ 201901 │        0 │      0 │\n└─────────┴───────────────┴────────┴──────────┴────────┘\n\n\n\nFrom reading this notebook ‘https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook’\n\nLast letter of column name is ‘P’ or ‘A’ is float64\nStrings are categorical variables\n\nObserver data below:\n\nthere are cardinality amont the tables.\n\n\n# load data into directory\ndata_files=list(p.glob('*_1.parquet'))#load_all data\nds = {f.name:ibis.read_parquet(f) for f in data_files}\n# preview of data\nfor i, j in ds.items():\n    print(i)\n    print(display(j.head(3)))\n\ntrain_other_1.parquet\nNone\ntrain_credit_bureau_a_1_1.parquet\nNone\ntrain_static_0_1.parquet\nNone\ntrain_tax_registry_c_1.parquet\nNone\ntrain_person_1.parquet\nNone\ntrain_credit_bureau_b_1.parquet\nNone\ntrain_tax_registry_b_1.parquet\nNone\ntrain_debitcard_1.parquet\nNone\ntrain_applprev_1_1.parquet\nNone\ntrain_tax_registry_a_1.parquet\nNone\ntrain_deposit_1.parquet\nNone\ntrain_credit_bureau_a_2_1.parquet\nNone\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ case_id ┃ amtdebitincoming_4809443A ┃ amtdebitoutgoing_4809440A ┃ amtdepositbalance_4809441A ┃ amtdepositincoming_4809444A ┃ amtdepositoutgoing_4809442A ┃ num_group1 ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ int64   │ float64                   │ float64                   │ float64                    │ float64                     │ float64                     │ int64      │\n├─────────┼───────────────────────────┼───────────────────────────┼────────────────────────────┼─────────────────────────────┼─────────────────────────────┼────────────┤\n│   43801 │                12466.6010 │                12291.2000 │                      914.2 │                         0.0 │                   304.80002 │          0 │\n│   43991 │                 3333.4001 │                 3273.4001 │                        0.0 │                         0.0 │                     0.00000 │          0 │\n│   44001 │                10000.0000 │                10000.0000 │                        0.0 │                         0.0 │                     0.00000 │          0 │\n└─────────┴───────────────────────────┴───────────────────────────┴────────────────────────────┴─────────────────────────────┴─────────────────────────────┴────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ annualeffectiverate_199L ┃ annualeffectiverate_63L ┃ classificationofcontr_13M ┃ classificationofcontr_400M ┃ contractst_545M ┃ contractst_964M ┃ contractsum_5085717L ┃ credlmt_230A ┃ credlmt_935A ┃ dateofcredend_289D ┃ dateofcredend_353D ┃ dateofcredstart_181D ┃ dateofcredstart_739D ┃ dateofrealrepmt_138D ┃ debtoutstand_525A ┃ debtoverdue_47A ┃ description_351M ┃ dpdmax_139P ┃ dpdmax_757P ┃ dpdmaxdatemonth_442T ┃ dpdmaxdatemonth_89T ┃ dpdmaxdateyear_596T ┃ dpdmaxdateyear_896T ┃ financialinstitution_382M ┃ financialinstitution_591M ┃ instlamount_768A ┃ instlamount_852A ┃ interestrate_508L ┃ lastupdate_1112D ┃ lastupdate_388D ┃ monthlyinstlamount_332A ┃ monthlyinstlamount_674A ┃ nominalrate_281L ┃ nominalrate_498L ┃ num_group1 ┃ numberofcontrsvalue_258L ┃ numberofcontrsvalue_358L ┃ numberofinstls_229L ┃ numberofinstls_320L ┃ numberofoutstandinstls_520L ┃ numberofoutstandinstls_59L ┃ numberofoverdueinstlmax_1039L ┃ numberofoverdueinstlmax_1151L ┃ numberofoverdueinstlmaxdat_148D ┃ numberofoverdueinstlmaxdat_641D ┃ numberofoverdueinstls_725L ┃ numberofoverdueinstls_834L ┃ outstandingamount_354A ┃ outstandingamount_362A ┃ overdueamount_31A ┃ overdueamount_659A ┃ overdueamountmax2_14A ┃ overdueamountmax2_398A ┃ overdueamountmax2date_1002D ┃ overdueamountmax2date_1142D ┃ overdueamountmax_155A ┃ overdueamountmax_35A ┃ overdueamountmaxdatemonth_284T ┃ overdueamountmaxdatemonth_365T ┃ overdueamountmaxdateyear_2T ┃ overdueamountmaxdateyear_994T ┃ periodicityofpmts_1102L ┃ periodicityofpmts_837L ┃ prolongationcount_1120L ┃ prolongationcount_599L ┃ purposeofcred_426M ┃ purposeofcred_874M ┃ refreshdate_3813885D ┃ residualamount_488A ┃ residualamount_856A ┃ subjectrole_182M ┃ subjectrole_93M ┃ totalamount_6A ┃ totalamount_996A ┃ totaldebtoverduevalue_178A ┃ totaldebtoverduevalue_718A ┃ totaloutstanddebtvalue_39A ┃ totaloutstanddebtvalue_668A ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64                  │ float64                 │ string                    │ string                     │ string          │ string          │ float64              │ float64      │ float64      │ string             │ string             │ string               │ string               │ string               │ float64           │ float64         │ string           │ float64     │ float64     │ float64              │ float64             │ float64             │ float64             │ string                    │ string                    │ float64          │ float64          │ float64           │ string           │ string          │ float64                 │ float64                 │ float64          │ float64          │ int64      │ float64                  │ float64                  │ float64             │ float64             │ float64                     │ float64                    │ float64                       │ float64                       │ string                          │ string                          │ float64                    │ float64                    │ float64                │ float64                │ float64           │ float64            │ float64               │ float64                │ string                      │ string                      │ float64               │ float64              │ float64                        │ float64                        │ float64                     │ float64                       │ float64                 │ float64                │ float64                 │ float64                │ string             │ string             │ string               │ float64             │ float64             │ string           │ string          │ float64        │ float64          │ float64                    │ float64                    │ float64                    │ float64                     │\n├─────────┼──────────────────────────┼─────────────────────────┼───────────────────────────┼────────────────────────────┼─────────────────┼─────────────────┼──────────────────────┼──────────────┼──────────────┼────────────────────┼────────────────────┼──────────────────────┼──────────────────────┼──────────────────────┼───────────────────┼─────────────────┼──────────────────┼─────────────┼─────────────┼──────────────────────┼─────────────────────┼─────────────────────┼─────────────────────┼───────────────────────────┼───────────────────────────┼──────────────────┼──────────────────┼───────────────────┼──────────────────┼─────────────────┼─────────────────────────┼─────────────────────────┼──────────────────┼──────────────────┼────────────┼──────────────────────────┼──────────────────────────┼─────────────────────┼─────────────────────┼─────────────────────────────┼────────────────────────────┼───────────────────────────────┼───────────────────────────────┼─────────────────────────────────┼─────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────┼────────────────────────┼───────────────────┼────────────────────┼───────────────────────┼────────────────────────┼─────────────────────────────┼─────────────────────────────┼───────────────────────┼──────────────────────┼────────────────────────────────┼────────────────────────────────┼─────────────────────────────┼───────────────────────────────┼─────────────────────────┼────────────────────────┼─────────────────────────┼────────────────────────┼────────────────────┼────────────────────┼──────────────────────┼─────────────────────┼─────────────────────┼──────────────────┼─────────────────┼────────────────┼──────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼─────────────────────────────┤\n│   19694 │                     NULL │                    NULL │ ea6782cc                  │ a55475b1                   │ 7241344e        │ a55475b1        │                 NULL │         NULL │          0.0 │ 2020-09-29         │ NULL               │ NULL                 │ 2014-09-29           │ NULL                 │          43315.26 │             0.0 │ a55475b1         │        20.0 │        NULL │                 NULL │                12.0 │              2017.0 │                NULL │ a55475b1                  │ P204_66_73                │              0.0 │             NULL │              NULL │ 2019-06-27       │ NULL            │                  0.0000 │                    NULL │             NULL │             NULL │          0 │                      2.0 │                      8.0 │                NULL │                NULL │                        NULL │                       NULL │                          22.0 │                          NULL │ NULL                            │ 2017-11-21                      │                        0.0 │                       NULL │                   NULL │                   NULL │              NULL │                0.0 │              2967.666 │                   NULL │ NULL                        │ 2018-03-31                  │              2967.666 │                 NULL │                           NULL │                            4.0 │                      2018.0 │                          NULL │                    NULL │                   NULL │                    NULL │                   NULL │ 60c73645           │ a55475b1           │ NULL                 │                NULL │                 0.0 │ ab3c25cf         │ ab3c25cf        │           NULL │             NULL │                        0.0 │                        0.0 │                   43315.26 │                         0.0 │\n│   19694 │                     NULL │                    NULL │ ea6782cc                  │ a55475b1                   │ 7241344e        │ a55475b1        │                 NULL │         NULL │         NULL │ 2020-04-14         │ NULL               │ NULL                 │ 2016-04-14           │ NULL                 │              NULL │            NULL │ a55475b1         │        19.0 │        NULL │                 NULL │                 3.0 │              2018.0 │                NULL │ a55475b1                  │ P150_136_157              │             NULL │             NULL │              NULL │ 2019-07-03       │ NULL            │               5155.5425 │                    NULL │             NULL │             NULL │          1 │                     NULL │                     NULL │                NULL │                47.0 │                        NULL │                        9.0 │                          21.0 │                          NULL │ NULL                            │ 2018-03-08                      │                        0.0 │                       NULL │                   NULL │               43315.26 │              NULL │                0.0 │              5155.344 │                   NULL │ NULL                        │ 2017-01-19                  │              5061.540 │                 NULL │                           NULL │                           10.0 │                      2017.0 │                          NULL │                    NULL │                   30.0 │                    NULL │                   NULL │ 96a8fdfe           │ a55475b1           │ NULL                 │                NULL │                NULL │ a55475b1         │ a55475b1        │           NULL │         170000.0 │                       NULL │                       NULL │                       NULL │                        NULL │\n│   19694 │                     NULL │                    NULL │ a55475b1                  │ a55475b1                   │ a55475b1        │ a55475b1        │                 NULL │         NULL │         NULL │ NULL               │ NULL               │ NULL                 │ NULL                 │ NULL                 │              NULL │            NULL │ a55475b1         │        NULL │        NULL │                 NULL │                NULL │                NULL │                NULL │ a55475b1                  │ a55475b1                  │             NULL │             NULL │              NULL │ NULL             │ NULL            │                    NULL │                    NULL │             NULL │             NULL │          2 │                     NULL │                     NULL │                NULL │                NULL │                        NULL │                       NULL │                          NULL │                          NULL │ NULL                            │ NULL                            │                       NULL │                       NULL │                   NULL │                   NULL │              NULL │               NULL │                  NULL │                   NULL │ NULL                        │ NULL                        │                  NULL │                 NULL │                           NULL │                           NULL │                        NULL │                          NULL │                    NULL │                   NULL │                    NULL │                   NULL │ a55475b1           │ a55475b1           │ 2019-07-10           │                NULL │                NULL │ a55475b1         │ a55475b1        │           NULL │             NULL │                       NULL │                       NULL │                       NULL │                        NULL │\n└─────────┴──────────────────────────┴─────────────────────────┴───────────────────────────┴────────────────────────────┴─────────────────┴─────────────────┴──────────────────────┴──────────────┴──────────────┴────────────────────┴────────────────────┴──────────────────────┴──────────────────────┴──────────────────────┴───────────────────┴─────────────────┴──────────────────┴─────────────┴─────────────┴──────────────────────┴─────────────────────┴─────────────────────┴─────────────────────┴───────────────────────────┴───────────────────────────┴──────────────────┴──────────────────┴───────────────────┴──────────────────┴─────────────────┴─────────────────────────┴─────────────────────────┴──────────────────┴──────────────────┴────────────┴──────────────────────────┴──────────────────────────┴─────────────────────┴─────────────────────┴─────────────────────────────┴────────────────────────────┴───────────────────────────────┴───────────────────────────────┴─────────────────────────────────┴─────────────────────────────────┴────────────────────────────┴────────────────────────────┴────────────────────────┴────────────────────────┴───────────────────┴────────────────────┴───────────────────────┴────────────────────────┴─────────────────────────────┴─────────────────────────────┴───────────────────────┴──────────────────────┴────────────────────────────────┴────────────────────────────────┴─────────────────────────────┴───────────────────────────────┴─────────────────────────┴────────────────────────┴─────────────────────────┴────────────────────────┴────────────────────┴────────────────────┴──────────────────────┴─────────────────────┴─────────────────────┴──────────────────┴─────────────────┴────────────────┴──────────────────┴────────────────────────────┴────────────────────────────┴────────────────────────────┴─────────────────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ actualdpdtolerance_344P ┃ amtinstpaidbefduel24m_4187115A ┃ annuity_780A ┃ annuitynextmonth_57A ┃ applicationcnt_361L ┃ applications30d_658L ┃ applicationscnt_1086L ┃ applicationscnt_464L ┃ applicationscnt_629L ┃ applicationscnt_867L ┃ avgdbddpdlast24m_3658932P ┃ avgdbddpdlast3m_4187120P ┃ avgdbdtollast24m_4525197P ┃ avgdpdtolclosure24_3658938P ┃ avginstallast24m_3658937A ┃ avglnamtstart24m_4525187A ┃ avgmaxdpdlast9m_3716943P ┃ avgoutstandbalancel6m_4187114A ┃ avgpmtlast12m_4525200A ┃ bankacctype_710L ┃ cardtype_51L ┃ clientscnt12m_3712952L ┃ clientscnt3m_3712950L ┃ clientscnt6m_3712949L ┃ clientscnt_100L ┃ clientscnt_1022L ┃ clientscnt_1071L ┃ clientscnt_1130L ┃ clientscnt_136L ┃ clientscnt_157L ┃ clientscnt_257L ┃ clientscnt_304L ┃ clientscnt_360L ┃ clientscnt_493L ┃ clientscnt_533L ┃ clientscnt_887L ┃ clientscnt_946L ┃ cntincpaycont9m_3716944L ┃ cntpmts24_3658933L ┃ commnoinclast6m_3546845L ┃ credamount_770A ┃ credtype_322L ┃ currdebt_22A ┃ currdebtcredtyperange_828A ┃ datefirstoffer_1144D ┃ datelastinstal40dpd_247D ┃ datelastunpaid_3546854D ┃ daysoverduetolerancedd_3976961L ┃ deferredmnthsnum_166L ┃ disbursedcredamount_1113A ┃ disbursementtype_67L ┃ downpmt_116A ┃ dtlastpmtallstes_4499206D ┃ eir_270L ┃ equalitydataagreement_891L ┃ equalityempfrom_62L ┃ firstclxcampaign_1125D ┃ firstdatedue_489D ┃ homephncnt_628L ┃ inittransactionamount_650A ┃ inittransactioncode_186L ┃ interestrate_311L ┃ interestrategrace_34L ┃ isbidproduct_1095L ┃ isbidproductrequest_292L ┃ isdebitcard_729L ┃ lastactivateddate_801D ┃ lastapplicationdate_877D ┃ lastapprcommoditycat_1041M ┃ lastapprcommoditytypec_5251766M ┃ lastapprcredamount_781A ┃ lastapprdate_640D ┃ lastcancelreason_561M ┃ lastdelinqdate_224D ┃ lastdependentsnum_448L ┃ lastotherinc_902A ┃ lastotherlnsexpense_631A ┃ lastrejectcommoditycat_161M ┃ lastrejectcommodtypec_5251769M ┃ lastrejectcredamount_222A ┃ lastrejectdate_50D ┃ lastrejectreason_759M ┃ lastrejectreasonclient_4145040M ┃ lastrepayingdate_696D ┃ lastst_736L ┃ maininc_215A ┃ mastercontrelectronic_519L ┃ mastercontrexist_109L ┃ maxannuity_159A ┃ maxannuity_4075009A ┃ maxdbddpdlast1m_3658939P ┃ maxdbddpdtollast12m_3658940P ┃ maxdbddpdtollast6m_4187119P ┃ maxdebt4_972A ┃ maxdpdfrom6mto36m_3546853P ┃ maxdpdinstldate_3546855D ┃ maxdpdinstlnum_3546846P ┃ maxdpdlast12m_727P ┃ maxdpdlast24m_143P ┃ maxdpdlast3m_392P ┃ maxdpdlast6m_474P ┃ maxdpdlast9m_1059P ┃ maxdpdtolerance_374P ┃ maxinstallast24m_3658928A ┃ maxlnamtstart6m_4525199A ┃ maxoutstandbalancel12m_4187113A ┃ maxpmtlast3m_4525190A ┃ mindbddpdlast24m_3658935P ┃ mindbdtollast24m_4525191P ┃ mobilephncnt_593L ┃ monthsannuity_845L ┃ numactivecreds_622L ┃ numactivecredschannel_414L ┃ numactiverelcontr_750L ┃ numcontrs3months_479L ┃ numincomingpmts_3546848L ┃ numinstlallpaidearly3d_817L ┃ numinstls_657L ┃ numinstlsallpaid_934L ┃ numinstlswithdpd10_728L ┃ numinstlswithdpd5_4187116L ┃ numinstlswithoutdpd_562L ┃ numinstmatpaidtearly2d_4499204L ┃ numinstpaid_4499208L ┃ numinstpaidearly3d_3546850L ┃ numinstpaidearly3dest_4493216L ┃ numinstpaidearly5d_1087L ┃ numinstpaidearly5dest_4493211L ┃ numinstpaidearly5dobd_4499205L ┃ numinstpaidearly_338L ┃ numinstpaidearlyest_4493214L ┃ numinstpaidlastcontr_4325080L ┃ numinstpaidlate1d_3546852L ┃ numinstregularpaid_973L ┃ numinstregularpaidest_4493210L ┃ numinsttopaygr_769L ┃ numinsttopaygrest_4493213L ┃ numinstunpaidmax_3546851L ┃ numinstunpaidmaxest_4493212L ┃ numnotactivated_1143L ┃ numpmtchanneldd_318L ┃ numrejects9m_859L ┃ opencred_647L ┃ paytype1st_925L ┃ paytype_783L ┃ payvacationpostpone_4187118D ┃ pctinstlsallpaidearl3d_427L ┃ pctinstlsallpaidlat10d_839L ┃ pctinstlsallpaidlate1d_3546856L ┃ pctinstlsallpaidlate4d_3546849L ┃ pctinstlsallpaidlate6d_3546844L ┃ pmtnum_254L ┃ posfpd10lastmonth_333P ┃ posfpd30lastmonth_3976960P ┃ posfstqpd30lastmonth_3976962P ┃ previouscontdistrict_112M ┃ price_1097A ┃ sellerplacecnt_915L ┃ sellerplacescnt_216L ┃ sumoutstandtotal_3546847A ┃ sumoutstandtotalest_4493215A ┃ totaldebt_9A ┃ totalsettled_863A ┃ totinstallast1m_4525188A ┃ twobodfilling_608L ┃ typesuite_864L ┃ validfrom_1069D ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64                 │ float64                        │ float64      │ float64              │ float64             │ float64              │ float64               │ float64              │ float64              │ float64              │ float64                   │ float64                  │ float64                   │ float64                     │ float64                   │ float64                   │ float64                  │ float64                        │ float64                │ string           │ string       │ float64                │ float64               │ float64               │ float64         │ float64          │ float64          │ float64          │ float64         │ float64         │ float64         │ float64         │ float64         │ float64         │ float64         │ float64         │ float64         │ float64                  │ float64            │ float64                  │ float64         │ string        │ float64      │ float64                    │ string               │ string                   │ string                  │ float64                         │ float64               │ float64                   │ string               │ float64      │ string                    │ float64  │ boolean                    │ boolean             │ string                 │ string            │ float64         │ float64                    │ string                   │ float64           │ float64               │ boolean            │ boolean                  │ boolean          │ string                 │ string                   │ string                     │ string                          │ float64                 │ string            │ string                │ string              │ float64                │ float64           │ float64                  │ string                      │ string                         │ float64                   │ string             │ string                │ string                          │ string                │ string      │ float64      │ float64                    │ float64               │ float64         │ float64             │ float64                  │ float64                      │ float64                     │ float64       │ float64                    │ string                   │ float64                 │ float64            │ float64            │ float64           │ float64           │ float64            │ float64              │ float64                   │ float64                  │ float64                         │ float64               │ float64                   │ float64                   │ float64           │ float64            │ float64             │ float64                    │ float64                │ float64               │ float64                  │ float64                     │ float64        │ float64               │ float64                 │ float64                    │ float64                  │ float64                         │ float64              │ float64                     │ float64                        │ float64                  │ float64                        │ float64                        │ float64               │ float64                      │ float64                       │ float64                    │ float64                 │ float64                        │ float64             │ float64                    │ float64                   │ float64                      │ float64               │ float64              │ float64           │ boolean       │ string          │ string       │ string                       │ float64                     │ float64                     │ float64                         │ float64                         │ float64                         │ float64     │ float64                │ float64                    │ float64                       │ string                    │ float64     │ float64             │ float64              │ float64                   │ float64                      │ float64      │ float64           │ float64                  │ string             │ string         │ string          │\n├─────────┼─────────────────────────┼────────────────────────────────┼──────────────┼──────────────────────┼─────────────────────┼──────────────────────┼───────────────────────┼──────────────────────┼──────────────────────┼──────────────────────┼───────────────────────────┼──────────────────────────┼───────────────────────────┼─────────────────────────────┼───────────────────────────┼───────────────────────────┼──────────────────────────┼────────────────────────────────┼────────────────────────┼──────────────────┼──────────────┼────────────────────────┼───────────────────────┼───────────────────────┼─────────────────┼──────────────────┼──────────────────┼──────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼──────────────────────────┼────────────────────┼──────────────────────────┼─────────────────┼───────────────┼──────────────┼────────────────────────────┼──────────────────────┼──────────────────────────┼─────────────────────────┼─────────────────────────────────┼───────────────────────┼───────────────────────────┼──────────────────────┼──────────────┼───────────────────────────┼──────────┼────────────────────────────┼─────────────────────┼────────────────────────┼───────────────────┼─────────────────┼────────────────────────────┼──────────────────────────┼───────────────────┼───────────────────────┼────────────────────┼──────────────────────────┼──────────────────┼────────────────────────┼──────────────────────────┼────────────────────────────┼─────────────────────────────────┼─────────────────────────┼───────────────────┼───────────────────────┼─────────────────────┼────────────────────────┼───────────────────┼──────────────────────────┼─────────────────────────────┼────────────────────────────────┼───────────────────────────┼────────────────────┼───────────────────────┼─────────────────────────────────┼───────────────────────┼─────────────┼──────────────┼────────────────────────────┼───────────────────────┼─────────────────┼─────────────────────┼──────────────────────────┼──────────────────────────────┼─────────────────────────────┼───────────────┼────────────────────────────┼──────────────────────────┼─────────────────────────┼────────────────────┼────────────────────┼───────────────────┼───────────────────┼────────────────────┼──────────────────────┼───────────────────────────┼──────────────────────────┼─────────────────────────────────┼───────────────────────┼───────────────────────────┼───────────────────────────┼───────────────────┼────────────────────┼─────────────────────┼────────────────────────────┼────────────────────────┼───────────────────────┼──────────────────────────┼─────────────────────────────┼────────────────┼───────────────────────┼─────────────────────────┼────────────────────────────┼──────────────────────────┼─────────────────────────────────┼──────────────────────┼─────────────────────────────┼────────────────────────────────┼──────────────────────────┼────────────────────────────────┼────────────────────────────────┼───────────────────────┼──────────────────────────────┼───────────────────────────────┼────────────────────────────┼─────────────────────────┼────────────────────────────────┼─────────────────────┼────────────────────────────┼───────────────────────────┼──────────────────────────────┼───────────────────────┼──────────────────────┼───────────────────┼───────────────┼─────────────────┼──────────────┼──────────────────────────────┼─────────────────────────────┼─────────────────────────────┼─────────────────────────────────┼─────────────────────────────────┼─────────────────────────────────┼─────────────┼────────────────────────┼────────────────────────────┼───────────────────────────────┼───────────────────────────┼─────────────┼─────────────────────┼──────────────────────┼───────────────────────────┼──────────────────────────────┼──────────────┼───────────────────┼──────────────────────────┼────────────────────┼────────────────┼─────────────────┤\n│   40626 │                    NULL │                           NULL │    1976.2001 │                  0.0 │                 0.0 │                  0.0 │                   0.0 │                  1.0 │                  0.0 │                  0.0 │                      NULL │                     NULL │                      NULL │                        NULL │                      NULL │                      NULL │                     NULL │                           NULL │                   NULL │ NULL             │ NULL         │                    0.0 │                   0.0 │                   0.0 │             0.0 │              0.0 │              0.0 │              0.0 │            NULL │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │                     NULL │               NULL │                      0.0 │         36000.0 │ CAL           │          0.0 │                        0.0 │ NULL                 │ NULL                     │ NULL                    │                            NULL │                   0.0 │                   36000.0 │ GBA                  │          0.0 │ NULL                      │     0.28 │ NULL                       │ NULL                │ NULL                   │ NULL              │             0.0 │                       NULL │ CASH                     │              0.28 │                  NULL │ False              │ NULL                     │ NULL             │ NULL                   │ NULL                     │ a55475b1                   │ a55475b1                        │                    NULL │ NULL              │ a55475b1              │ NULL                │                   NULL │              NULL │                     NULL │ a55475b1                    │ a55475b1                       │                      NULL │ NULL               │ a55475b1              │ a55475b1                        │ NULL                  │ NULL        │         NULL │                        0.0 │                   0.0 │             0.0 │                NULL │                     NULL │                         NULL │                        NULL │           0.0 │                        0.0 │ NULL                     │                    NULL │                0.0 │                0.0 │               0.0 │               0.0 │                0.0 │                  0.0 │                      NULL │                     NULL │                            NULL │                  NULL │                      NULL │                      NULL │               1.0 │               NULL │                 0.0 │                        0.0 │                    0.0 │                   0.0 │                     NULL │                        NULL │            0.0 │                  NULL │                    NULL │                       NULL │                     NULL │                            NULL │                 NULL │                        NULL │                           NULL │                     NULL │                           NULL │                           NULL │                  NULL │                         NULL │                          NULL │                       NULL │                    NULL │                           NULL │                NULL │                       NULL │                      NULL │                         NULL │                   0.0 │                  0.0 │               0.0 │ NULL          │ OTHER           │ OTHER        │ NULL                         │                        NULL │                        NULL │                            NULL │                            NULL │                            NULL │        24.0 │                    0.0 │                        0.0 │                          NULL │ a55475b1                  │        NULL │                 0.0 │                  0.0 │                      NULL │                         NULL │          0.0 │               0.0 │                     NULL │ FO                 │ AL             │ NULL            │\n│   40704 │                    NULL │                           NULL │    3731.2000 │                  0.0 │                 0.0 │                  0.0 │                   0.0 │                  0.0 │                  0.0 │                  0.0 │                      NULL │                     NULL │                      NULL │                        NULL │                      NULL │                      NULL │                     NULL │                           NULL │                   NULL │ NULL             │ NULL         │                    0.0 │                   0.0 │                   0.0 │             0.0 │              0.0 │              0.0 │              0.0 │            NULL │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │                     NULL │               NULL │                      0.0 │         30000.0 │ CAL           │          0.0 │                        0.0 │ NULL                 │ NULL                     │ NULL                    │                            NULL │                   0.0 │                   30000.0 │ GBA                  │          0.0 │ NULL                      │     0.45 │ NULL                       │ NULL                │ NULL                   │ NULL              │             1.0 │                       NULL │ CASH                     │              0.45 │                  NULL │ False              │ NULL                     │ NULL             │ NULL                   │ 2018-11-20               │ a55475b1                   │ a55475b1                        │                    NULL │ NULL              │ P94_109_143           │ NULL                │                   NULL │              NULL │                     NULL │ a55475b1                    │ a55475b1                       │                   54000.0 │ 2018-11-20         │ P198_131_9            │ P94_109_143                     │ NULL                  │ D           │         NULL │                        0.0 │                   0.0 │             0.0 │                NULL │                     NULL │                         NULL │                        NULL │           0.0 │                        0.0 │ NULL                     │                    NULL │                0.0 │                0.0 │               0.0 │               0.0 │                0.0 │                  0.0 │                      NULL │                     NULL │                            NULL │                  NULL │                      NULL │                      NULL │               2.0 │               NULL │                 0.0 │                        0.0 │                    0.0 │                   0.0 │                     NULL │                        NULL │            0.0 │                  NULL │                    NULL │                       NULL │                     NULL │                            NULL │                 NULL │                        NULL │                           NULL │                     NULL │                           NULL │                           NULL │                  NULL │                         NULL │                          NULL │                       NULL │                    NULL │                           NULL │                NULL │                       NULL │                      NULL │                         NULL │                   0.0 │                  0.0 │               0.0 │ False         │ OTHER           │ OTHER        │ NULL                         │                        NULL │                        NULL │                            NULL │                            NULL │                            NULL │        12.0 │                   NULL │                       NULL │                          NULL │ a55475b1                  │        NULL │                 0.0 │                  0.0 │                      NULL │                         NULL │          0.0 │               0.0 │                     NULL │ FO                 │ AL             │ NULL            │\n│   40734 │                    NULL │                           NULL │    3731.2000 │                  0.0 │                 0.0 │                  1.0 │                   0.0 │                  0.0 │                  0.0 │                  1.0 │                      NULL │                     NULL │                      NULL │                        NULL │                      NULL │                      NULL │                     NULL │                           NULL │                   NULL │ NULL             │ NULL         │                    0.0 │                   0.0 │                   0.0 │             0.0 │              0.0 │              0.0 │              0.0 │            NULL │             0.0 │             0.0 │             0.0 │             0.0 │             0.0 │             1.0 │             0.0 │             0.0 │                     NULL │               NULL │                      0.0 │         30000.0 │ CAL           │          0.0 │                        0.0 │ NULL                 │ NULL                     │ NULL                    │                            NULL │                   0.0 │                   30000.0 │ GBA                  │          0.0 │ NULL                      │     0.45 │ NULL                       │ NULL                │ NULL                   │ NULL              │             0.0 │                       NULL │ CASH                     │              0.45 │                  NULL │ False              │ NULL                     │ NULL             │ NULL                   │ 2019-12-26               │ a55475b1                   │ a55475b1                        │                    NULL │ NULL              │ P94_109_143           │ NULL                │                   NULL │              NULL │                     NULL │ a55475b1                    │ a55475b1                       │                   50000.0 │ 2019-12-26         │ P45_84_106            │ P94_109_143                     │ NULL                  │ D           │         NULL │                        0.0 │                   0.0 │             0.0 │                NULL │                     NULL │                         NULL │                        NULL │           0.0 │                        0.0 │ NULL                     │                    NULL │                0.0 │                0.0 │               0.0 │               0.0 │                0.0 │                  0.0 │                      NULL │                     NULL │                            NULL │                  NULL │                      NULL │                      NULL │               1.0 │               NULL │                 0.0 │                        0.0 │                    0.0 │                   1.0 │                     NULL │                        NULL │            0.0 │                  NULL │                    NULL │                       NULL │                     NULL │                            NULL │                 NULL │                        NULL │                           NULL │                     NULL │                           NULL │                           NULL │                  NULL │                         NULL │                          NULL │                       NULL │                    NULL │                           NULL │                NULL │                       NULL │                      NULL │                         NULL │                   0.0 │                  0.0 │               1.0 │ False         │ OTHER           │ OTHER        │ NULL                         │                        NULL │                        NULL │                            NULL │                            NULL │                            NULL │        12.0 │                    0.0 │                        0.0 │                          NULL │ a55475b1                  │        NULL │                 1.0 │                  1.0 │                      NULL │                         NULL │          0.0 │               0.0 │                     NULL │ FO                 │ AL             │ NULL            │\n└─────────┴─────────────────────────┴────────────────────────────────┴──────────────┴──────────────────────┴─────────────────────┴──────────────────────┴───────────────────────┴──────────────────────┴──────────────────────┴──────────────────────┴───────────────────────────┴──────────────────────────┴───────────────────────────┴─────────────────────────────┴───────────────────────────┴───────────────────────────┴──────────────────────────┴────────────────────────────────┴────────────────────────┴──────────────────┴──────────────┴────────────────────────┴───────────────────────┴───────────────────────┴─────────────────┴──────────────────┴──────────────────┴──────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴──────────────────────────┴────────────────────┴──────────────────────────┴─────────────────┴───────────────┴──────────────┴────────────────────────────┴──────────────────────┴──────────────────────────┴─────────────────────────┴─────────────────────────────────┴───────────────────────┴───────────────────────────┴──────────────────────┴──────────────┴───────────────────────────┴──────────┴────────────────────────────┴─────────────────────┴────────────────────────┴───────────────────┴─────────────────┴────────────────────────────┴──────────────────────────┴───────────────────┴───────────────────────┴────────────────────┴──────────────────────────┴──────────────────┴────────────────────────┴──────────────────────────┴────────────────────────────┴─────────────────────────────────┴─────────────────────────┴───────────────────┴───────────────────────┴─────────────────────┴────────────────────────┴───────────────────┴──────────────────────────┴─────────────────────────────┴────────────────────────────────┴───────────────────────────┴────────────────────┴───────────────────────┴─────────────────────────────────┴───────────────────────┴─────────────┴──────────────┴────────────────────────────┴───────────────────────┴─────────────────┴─────────────────────┴──────────────────────────┴──────────────────────────────┴─────────────────────────────┴───────────────┴────────────────────────────┴──────────────────────────┴─────────────────────────┴────────────────────┴────────────────────┴───────────────────┴───────────────────┴────────────────────┴──────────────────────┴───────────────────────────┴──────────────────────────┴─────────────────────────────────┴───────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────┴────────────────────┴─────────────────────┴────────────────────────────┴────────────────────────┴───────────────────────┴──────────────────────────┴─────────────────────────────┴────────────────┴───────────────────────┴─────────────────────────┴────────────────────────────┴──────────────────────────┴─────────────────────────────────┴──────────────────────┴─────────────────────────────┴────────────────────────────────┴──────────────────────────┴────────────────────────────────┴────────────────────────────────┴───────────────────────┴──────────────────────────────┴───────────────────────────────┴────────────────────────────┴─────────────────────────┴────────────────────────────────┴─────────────────────┴────────────────────────────┴───────────────────────────┴──────────────────────────────┴───────────────────────┴──────────────────────┴───────────────────┴───────────────┴─────────────────┴──────────────┴──────────────────────────────┴─────────────────────────────┴─────────────────────────────┴─────────────────────────────────┴─────────────────────────────────┴─────────────────────────────────┴─────────────┴────────────────────────┴────────────────────────────┴───────────────────────────────┴───────────────────────────┴─────────────┴─────────────────────┴──────────────────────┴───────────────────────────┴──────────────────────────────┴──────────────┴───────────────────┴──────────────────────────┴────────────────────┴────────────────┴─────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ employername_160M ┃ num_group1 ┃ pmtamount_36A ┃ processingdate_168D ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ string            │ int64      │ float64       │ string              │\n├─────────┼───────────────────┼────────────┼───────────────┼─────────────────────┤\n│     357 │ c91b12ff          │          5 │        1100.0 │ 2018-08-08          │\n│     357 │ c91b12ff          │          1 │        1200.0 │ 2018-11-28          │\n│     357 │ c91b12ff          │          4 │        1200.0 │ 2018-09-10          │\n└─────────┴───────────────────┴────────────┴───────────────┴─────────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ birth_259D ┃ birthdate_87D ┃ childnum_185L ┃ contaddr_district_15M ┃ contaddr_matchlist_1032L ┃ contaddr_smempladdr_334L ┃ contaddr_zipcode_807M ┃ education_927M ┃ empl_employedfrom_271D ┃ empl_employedtotal_800L ┃ empl_industry_691L ┃ empladdr_district_926M ┃ empladdr_zipcode_114M ┃ familystate_447L ┃ gender_992L ┃ housetype_905L ┃ housingtype_772L ┃ incometype_1044T ┃ isreference_387L ┃ language1_981M ┃ mainoccupationinc_384A ┃ maritalst_703L ┃ num_group1 ┃ personindex_1023L ┃ persontype_1072L ┃ persontype_792L ┃ registaddr_district_1083M ┃ registaddr_zipcode_184M ┃ relationshiptoclient_415T ┃ relationshiptoclient_642T ┃ remitter_829L ┃ role_1084L ┃ role_993L ┃ safeguarantyflag_411L ┃ sex_738L ┃ type_25L       ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ int64   │ string     │ string        │ float64       │ string                │ boolean                  │ boolean                  │ string                │ string         │ string                 │ string                  │ string             │ string                 │ string                │ string           │ string      │ string         │ string           │ string           │ boolean          │ string         │ float64                │ string         │ int64      │ float64           │ float64          │ float64         │ string                    │ string                  │ string                    │ string                    │ boolean       │ string     │ string    │ boolean               │ string   │ string         │\n├─────────┼────────────┼───────────────┼───────────────┼───────────────────────┼──────────────────────────┼──────────────────────────┼───────────────────────┼────────────────┼────────────────────────┼─────────────────────────┼────────────────────┼────────────────────────┼───────────────────────┼──────────────────┼─────────────┼────────────────┼──────────────────┼──────────────────┼──────────────────┼────────────────┼────────────────────────┼────────────────┼────────────┼───────────────────┼──────────────────┼─────────────────┼───────────────────────────┼─────────────────────────┼───────────────────────────┼───────────────────────────┼───────────────┼────────────┼───────────┼───────────────────────┼──────────┼────────────────┤\n│       0 │ 1986-07-01 │ NULL          │          NULL │ P88_18_84             │ False                    │ False                    │ P167_100_165          │ P97_36_170     │ 2017-09-15             │ MORE_FIVE               │ OTHER              │ P142_57_166            │ P167_100_165          │ MARRIED          │ NULL        │ NULL           │ NULL             │ SALARIED_GOVT    │ NULL             │ P10_39_147     │                10800.0 │ NULL           │          0 │               0.0 │              1.0 │             1.0 │ P88_18_84                 │ P167_100_165            │ NULL                      │ NULL                      │ NULL          │ CL         │ NULL      │ True                  │ F        │ PRIMARY_MOBILE │\n│       0 │ NULL       │ NULL          │          NULL │ a55475b1              │ NULL                     │ NULL                     │ a55475b1              │ a55475b1       │ NULL                   │ NULL                    │ NULL               │ a55475b1               │ a55475b1              │ NULL             │ NULL        │ NULL           │ NULL             │ NULL             │ NULL             │ a55475b1       │                   NULL │ NULL           │          1 │               1.0 │              1.0 │             4.0 │ a55475b1                  │ a55475b1                │ SPOUSE                    │ NULL                      │ False         │ EM         │ NULL      │ NULL                  │ NULL     │ PHONE          │\n│       0 │ NULL       │ NULL          │          NULL │ a55475b1              │ NULL                     │ NULL                     │ a55475b1              │ a55475b1       │ NULL                   │ NULL                    │ NULL               │ a55475b1               │ a55475b1              │ NULL             │ NULL        │ NULL           │ NULL             │ NULL             │ NULL             │ a55475b1       │                   NULL │ NULL           │          2 │               2.0 │              4.0 │             5.0 │ a55475b1                  │ a55475b1                │ COLLEAGUE                 │ SPOUSE                    │ False         │ PE         │ NULL      │ NULL                  │ NULL     │ PHONE          │\n└─────────┴────────────┴───────────────┴───────────────┴───────────────────────┴──────────────────────────┴──────────────────────────┴───────────────────────┴────────────────┴────────────────────────┴─────────────────────────┴────────────────────┴────────────────────────┴───────────────────────┴──────────────────┴─────────────┴────────────────┴──────────────────┴──────────────────┴──────────────────┴────────────────┴────────────────────────┴────────────────┴────────────┴───────────────────┴──────────────────┴─────────────────┴───────────────────────────┴─────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────┴────────────┴───────────┴───────────────────────┴──────────┴────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ amount_1115A ┃ classificationofcontr_1114M ┃ contractdate_551D ┃ contractmaturitydate_151D ┃ contractst_516M ┃ contracttype_653M ┃ credlmt_1052A ┃ credlmt_228A ┃ credlmt_3940954A ┃ credor_3940957M ┃ credquantity_1099L ┃ credquantity_984L ┃ debtpastduevalue_732A ┃ debtvalue_227A ┃ dpd_550P ┃ dpd_733P ┃ dpdmax_851P ┃ dpdmaxdatemonth_804T ┃ dpdmaxdateyear_742T ┃ installmentamount_644A ┃ installmentamount_833A ┃ instlamount_892A ┃ interesteffectiverate_369L ┃ interestrateyearly_538L ┃ lastupdate_260D ┃ maxdebtpduevalodued_3940955A ┃ num_group1 ┃ numberofinstls_810L ┃ overdueamountmax_950A ┃ overdueamountmaxdatemonth_494T ┃ overdueamountmaxdateyear_432T ┃ periodicityofpmts_997L ┃ periodicityofpmts_997M ┃ pmtdaysoverdue_1135P ┃ pmtmethod_731M ┃ pmtnumpending_403L ┃ purposeofcred_722M ┃ residualamount_1093A ┃ residualamount_127A ┃ residualamount_3940956A ┃ subjectrole_326M ┃ subjectrole_43M ┃ totalamount_503A ┃ totalamount_881A ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64      │ string                      │ string            │ string                    │ string          │ string            │ float64       │ float64      │ float64          │ string          │ float64            │ float64           │ float64               │ float64        │ float64  │ float64  │ float64     │ float64              │ float64             │ float64                │ float64                │ float64          │ float64                    │ float64                 │ string          │ float64                      │ int64      │ float64             │ float64               │ float64                        │ float64                       │ string                 │ string                 │ float64              │ string         │ float64            │ string             │ float64              │ float64             │ float64                 │ string           │ string          │ float64          │ float64          │\n├─────────┼──────────────┼─────────────────────────────┼───────────────────┼───────────────────────────┼─────────────────┼───────────────────┼───────────────┼──────────────┼──────────────────┼─────────────────┼────────────────────┼───────────────────┼───────────────────────┼────────────────┼──────────┼──────────┼─────────────┼──────────────────────┼─────────────────────┼────────────────────────┼────────────────────────┼──────────────────┼────────────────────────────┼─────────────────────────┼─────────────────┼──────────────────────────────┼────────────┼─────────────────────┼───────────────────────┼────────────────────────────────┼───────────────────────────────┼────────────────────────┼────────────────────────┼──────────────────────┼────────────────┼────────────────────┼────────────────────┼──────────────────────┼─────────────────────┼─────────────────────────┼──────────────────┼─────────────────┼──────────────────┼──────────────────┤\n│     467 │         NULL │ ea6782cc                    │ 2011-06-15        │ 2031-06-13                │ 7241344e        │ 724be82a          │  3.000000e+06 │      10000.0 │     3.000000e+06 │ P164_34_168     │                2.0 │               1.0 │                  NULL │           NULL │      0.0 │      0.0 │        NULL │                 NULL │                NULL │                    0.0 │                  0.000 │             NULL │                       NULL │                    NULL │ 2019-01-20      │                         NULL │          0 │                NULL │                  NULL │                           NULL │                          NULL │ NULL                   │ a55475b1               │                 NULL │ a55475b1       │               NULL │ 96a8fdfe           │                  0.0 │                 0.0 │                    NULL │ fa4f56f1         │ ab3c25cf        │     3.000000e+06 │          10000.0 │\n│     467 │         NULL │ ea6782cc                    │ 2019-01-04        │ 2021-08-04                │ 7241344e        │ 724be82a          │          NULL │         NULL │     1.303650e+05 │ P164_34_168     │                1.0 │               2.0 │                  NULL │           NULL │      0.0 │      0.0 │        NULL │                 NULL │                NULL │                    0.0 │              26571.969 │             NULL │                       NULL │                    NULL │ 2019-01-20      │                         NULL │          1 │                NULL │                  NULL │                           NULL │                          NULL │ NULL                   │ a55475b1               │                 NULL │ a55475b1       │               NULL │ 96a8fdfe           │                 NULL │                NULL │                    NULL │ ab3c25cf         │ ab3c25cf        │     7.800000e+04 │         960000.0 │\n│     467 │      78000.0 │ ea6782cc                    │ 2016-10-25        │ 2019-10-25                │ 7241344e        │ 4257cbed          │          NULL │         NULL │             NULL │ c5a72b57        │               NULL │              NULL │                   0.0 │      26571.969 │     NULL │     NULL │         0.0 │                 11.0 │              2016.0 │                   NULL │                   NULL │          2898.76 │                       NULL │                    NULL │ 2019-01-10      │                          0.0 │          2 │                36.0 │                   0.0 │                           11.0 │                        2016.0 │ NULL                   │ a0b598e4               │                  0.0 │ e914c86c       │               10.0 │ 96a8fdfe           │                 NULL │                NULL │                    NULL │ a55475b1         │ a55475b1        │             NULL │             NULL │\n└─────────┴──────────────┴─────────────────────────────┴───────────────────┴───────────────────────────┴─────────────────┴───────────────────┴───────────────┴──────────────┴──────────────────┴─────────────────┴────────────────────┴───────────────────┴───────────────────────┴────────────────┴──────────┴──────────┴─────────────┴──────────────────────┴─────────────────────┴────────────────────────┴────────────────────────┴──────────────────┴────────────────────────────┴─────────────────────────┴─────────────────┴──────────────────────────────┴────────────┴─────────────────────┴───────────────────────┴────────────────────────────────┴───────────────────────────────┴────────────────────────┴────────────────────────┴──────────────────────┴────────────────┴────────────────────┴────────────────────┴──────────────────────┴─────────────────────┴─────────────────────────┴──────────────────┴─────────────────┴──────────────────┴──────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ case_id ┃ amount_4917619A ┃ deductiondate_4917603D ┃ name_4917606M ┃ num_group1 ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ int64   │ float64         │ string                 │ string        │ int64      │\n├─────────┼─────────────────┼────────────────────────┼───────────────┼────────────┤\n│   49435 │          6885.0 │ 2019-10-16             │ 6b730375      │          7 │\n│   49435 │          6885.0 │ 2019-10-16             │ 6b730375      │          1 │\n│   49435 │          6885.0 │ 2019-10-16             │ 6b730375      │          8 │\n└─────────┴─────────────────┴────────────────────────┴───────────────┴────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ last180dayaveragebalance_704A ┃ last180dayturnover_1134A ┃ last30dayturnover_651A ┃ num_group1 ┃ openingdate_857D ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64                       │ float64                  │ float64                │ int64      │ string           │\n├─────────┼───────────────────────────────┼──────────────────────────┼────────────────────────┼────────────┼──────────────────┤\n│     225 │                          NULL │                     NULL │                   NULL │          0 │ 2016-08-16       │\n│     331 │                          NULL │                     NULL │                   NULL │          0 │ 2015-03-19       │\n│     358 │                          NULL │                     NULL │                   NULL │          0 │ 2014-09-02       │\n└─────────┴───────────────────────────────┴──────────────────────────┴────────────────────────┴────────────┴──────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ case_id ┃ actualdpd_943P ┃ annuity_853A ┃ approvaldate_319D ┃ byoccupationinc_3656910L ┃ cancelreason_3545846M ┃ childnum_21L ┃ creationdate_885D ┃ credacc_actualbalance_314A ┃ credacc_credlmt_575A ┃ credacc_maxhisbal_375A ┃ credacc_minhisbal_90A ┃ credacc_status_367L ┃ credacc_transactions_402L ┃ credamount_590A ┃ credtype_587L ┃ currdebt_94A ┃ dateactivated_425D ┃ district_544M ┃ downpmt_134A ┃ dtlastpmt_581D ┃ dtlastpmtallstes_3545839D ┃ education_1138M ┃ employedfrom_700D ┃ familystate_726L ┃ firstnonzeroinstldate_307D ┃ inittransactioncode_279L ┃ isbidproduct_390L ┃ isdebitcard_527L ┃ mainoccupationinc_437A ┃ maxdpdtolerance_577P ┃ num_group1 ┃ outstandingdebt_522A ┃ pmtnum_8L ┃ postype_4733339M ┃ profession_152M ┃ rejectreason_755M ┃ rejectreasonclient_4145042M ┃ revolvingaccount_394A ┃ status_219L ┃ tenor_203L ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ int64   │ float64        │ float64      │ string            │ float64                  │ string                │ float64      │ string            │ float64                    │ float64              │ float64                │ float64               │ string              │ float64                   │ float64         │ string        │ float64      │ string             │ string        │ float64      │ string         │ string                    │ string          │ string            │ string           │ string                     │ string                   │ boolean           │ boolean          │ float64                │ float64              │ int64      │ float64              │ float64   │ string           │ string          │ string            │ string                      │ float64               │ string      │ float64    │\n├─────────┼────────────────┼──────────────┼───────────────────┼──────────────────────────┼───────────────────────┼──────────────┼───────────────────┼────────────────────────────┼──────────────────────┼────────────────────────┼───────────────────────┼─────────────────────┼───────────────────────────┼─────────────────┼───────────────┼──────────────┼────────────────────┼───────────────┼──────────────┼────────────────┼───────────────────────────┼─────────────────┼───────────────────┼──────────────────┼────────────────────────────┼──────────────────────────┼───────────────────┼──────────────────┼────────────────────────┼──────────────────────┼────────────┼──────────────────────┼───────────┼──────────────────┼─────────────────┼───────────────────┼─────────────────────────────┼───────────────────────┼─────────────┼────────────┤\n│   40704 │            0.0 │    7204.6000 │ NULL              │                     NULL │ P94_109_143           │         NULL │ 2018-11-20        │                       NULL │                  0.0 │                   NULL │                  NULL │ NULL                │                      NULL │         54000.0 │ CAL           │         NULL │ NULL               │ P147_6_101    │          0.0 │ NULL           │ NULL                      │ a55475b1        │ NULL              │ NULL             │ 2018-12-20                 │ CASH                     │ False             │ NULL             │                40000.0 │                 NULL │          0 │                 NULL │      12.0 │ P46_145_78       │ a55475b1        │ P198_131_9        │ P94_109_143                 │                  NULL │ D           │       12.0 │\n│   40734 │            0.0 │    3870.2000 │ NULL              │                     NULL │ P94_109_143           │         NULL │ 2019-12-26        │                       NULL │                  0.0 │                   NULL │                  NULL │ NULL                │                      NULL │         50000.0 │ CAL           │         NULL │ NULL               │ P111_148_100  │          0.0 │ NULL           │ NULL                      │ a55475b1        │ NULL              │ NULL             │ 2020-01-26                 │ CASH                     │ False             │ NULL             │                50000.0 │                 NULL │          0 │                 NULL │      18.0 │ P149_40_170      │ a55475b1        │ P45_84_106        │ P94_109_143                 │                  NULL │ D           │       18.0 │\n│   40737 │            0.0 │    2324.4001 │ NULL              │                      1.0 │ a55475b1              │          0.0 │ 2014-07-17        │                       NULL │                  0.0 │                   NULL │                  NULL │ NULL                │                      NULL │         30000.0 │ CAL           │          0.0 │ NULL               │ a55475b1      │          0.0 │ NULL           │ NULL                      │ P97_36_170      │ 2014-01-15        │ MARRIED          │ 2014-08-17                 │ CASH                     │ False             │ NULL             │                16000.0 │                 NULL │          0 │                  0.0 │      18.0 │ P46_145_78       │ a55475b1        │ a55475b1          │ a55475b1                    │                  NULL │ D           │       18.0 │\n└─────────┴────────────────┴──────────────┴───────────────────┴──────────────────────────┴───────────────────────┴──────────────┴───────────────────┴────────────────────────────┴──────────────────────┴────────────────────────┴───────────────────────┴─────────────────────┴───────────────────────────┴─────────────────┴───────────────┴──────────────┴────────────────────┴───────────────┴──────────────┴────────────────┴───────────────────────────┴─────────────────┴───────────────────┴──────────────────┴────────────────────────────┴──────────────────────────┴───────────────────┴──────────────────┴────────────────────────┴──────────────────────┴────────────┴──────────────────────┴───────────┴──────────────────┴─────────────────┴───────────────────┴─────────────────────────────┴───────────────────────┴─────────────┴────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ amount_4527230A ┃ name_4527232M ┃ num_group1 ┃ recorddate_4527225D ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64         │ string        │ int64      │ string              │\n├─────────┼─────────────────┼───────────────┼────────────┼─────────────────────┤\n│   28631 │       1946.0000 │ f980a1ea      │          2 │ 2019-09-13          │\n│   28631 │        711.0000 │ f980a1ea      │          3 │ 2019-09-13          │\n│   28631 │       3616.4001 │ f980a1ea      │          0 │ 2019-09-13          │\n└─────────┴─────────────────┴───────────────┴────────────┴─────────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ amount_416A ┃ contractenddate_991D ┃ num_group1 ┃ openingdate_313D ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ int64   │ float64     │ string               │ int64      │ string           │\n├─────────┼─────────────┼──────────────────────┼────────────┼──────────────────┤\n│     225 │       0.000 │ NULL                 │          0 │ 2016-08-16       │\n│     331 │     260.374 │ 2018-03-18           │          0 │ 2015-03-19       │\n│     358 │       0.000 │ NULL                 │          0 │ 2014-09-02       │\n└─────────┴─────────────┴──────────────────────┴────────────┴──────────────────┘\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ case_id ┃ collater_typofvalofguarant_298M ┃ collater_typofvalofguarant_407M ┃ collater_valueofguarantee_1124L ┃ collater_valueofguarantee_876L ┃ collaterals_typeofguarante_359M ┃ collaterals_typeofguarante_669M ┃ num_group1 ┃ num_group2 ┃ pmts_dpd_1073P ┃ pmts_dpd_303P ┃ pmts_month_158T ┃ pmts_month_706T ┃ pmts_overdue_1140A ┃ pmts_overdue_1152A ┃ pmts_year_1139T ┃ pmts_year_507T ┃ subjectroles_name_541M ┃ subjectroles_name_838M ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ string                          │ string                          │ float64                         │ float64                        │ string                          │ string                          │ int64      │ int64      │ float64        │ float64       │ float64         │ float64         │ float64            │ float64            │ float64         │ float64        │ string                 │ string                 │\n├─────────┼─────────────────────────────────┼─────────────────────────────────┼─────────────────────────────────┼────────────────────────────────┼─────────────────────────────────┼─────────────────────────────────┼────────────┼────────────┼────────────────┼───────────────┼─────────────────┼─────────────────┼────────────────────┼────────────────────┼─────────────────┼────────────────┼────────────────────────┼────────────────────────┤\n│    6683 │ 8fd95e4b                        │ a55475b1                        │                    6.742800e+06 │                           NULL │ a55475b1                        │ 9276e4bb                        │          0 │          0 │           NULL │          NULL │             2.0 │            NULL │               NULL │               NULL │          2017.0 │           NULL │ a55475b1               │ ab3c25cf               │\n│    6683 │ 8fd95e4b                        │ a55475b1                        │                    6.700000e+06 │                           NULL │ a55475b1                        │ 0e63c0f0                        │          0 │          1 │           NULL │          NULL │             3.0 │            NULL │               NULL │               NULL │          2017.0 │           NULL │ a55475b1               │ a55475b1               │\n│    6683 │ a55475b1                        │ a55475b1                        │                            NULL │                           NULL │ a55475b1                        │ a55475b1                        │          0 │          2 │            0.0 │          NULL │             4.0 │            NULL │                0.0 │               NULL │          2017.0 │           NULL │ a55475b1               │ a55475b1               │\n└─────────┴─────────────────────────────────┴─────────────────────────────────┴─────────────────────────────────┴────────────────────────────────┴─────────────────────────────────┴─────────────────────────────────┴────────────┴────────────┴────────────────┴───────────────┴─────────────────┴─────────────────┴────────────────────┴────────────────────┴─────────────────┴────────────────┴────────────────────────┴────────────────────────┘\n\n\n\n\nWhere to find columns?\n\nc = {i.name:ibis.read_parquet(i).columns for i in p.glob(\"*.parquet\")}\n\n\n\nwith pd.option_context('display.max_rows', None):\n    fdef=pd.read_csv('home-credit-credit-risk-model-stability/feature_definitions.csv').set_index('Variable')\n    lookuptable=(pd.DataFrame([pd.Series({\"File\":i, \"Variable\":j}) for i, j in c.items()])\n        .explode('Variable')\n        .assign(File=lambda x: x[\"File\"].str.replace('\\.parquet$','', regex=True))\n        .groupby('Variable')\n        .agg({\"File\":lambda x: x.to_list()})\n        .join(fdef)\n        .reset_index())\n    display(lookuptable)\n\n\n\n\n\n\n\n\n\nVariable\nFile\nDescription\n\n\n\n\n0\nMONTH\n[train_base]\nNaN\n\n\n1\nWEEK_NUM\n[train_base]\nNaN\n\n\n2\nactualdpd_943P\n[train_applprev_1_0, train_applprev_1_1]\nDays Past Due (DPD) of previous contract (actu...\n\n\n3\nactualdpdtolerance_344P\n[train_static_0_0, train_static_0_1]\nDPD of client with tolerance.\n\n\n4\naddres_district_368M\n[train_person_2]\nDistrict of the person's address.\n\n\n5\naddres_role_871L\n[train_person_2]\nRole of person's address.\n\n\n6\naddres_zip_823M\n[train_person_2]\nZip code of the address.\n\n\n7\namount_1115A\n[train_credit_bureau_b_1]\nCredit amount of the active contract provided ...\n\n\n8\namount_416A\n[train_deposit_1]\nDeposit amount.\n\n\n9\namount_4527230A\n[train_tax_registry_a_1]\nTax deductions amount tracked by the governmen...\n\n\n10\namount_4917619A\n[train_tax_registry_b_1]\nTax deductions amount tracked by the governmen...\n\n\n11\namtdebitincoming_4809443A\n[train_other_1]\nIncoming debit card transactions amount.\n\n\n12\namtdebitoutgoing_4809440A\n[train_other_1]\nOutgoing debit card transactions amount.\n\n\n13\namtdepositbalance_4809441A\n[train_other_1]\nDeposit balance of client.\n\n\n14\namtdepositincoming_4809444A\n[train_other_1]\nAmount of incoming deposits to client's account.\n\n\n15\namtdepositoutgoing_4809442A\n[train_other_1]\nAmount of outgoing deposits from client's acco...\n\n\n16\namtinstpaidbefduel24m_4187115A\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid before due date in ...\n\n\n17\nannualeffectiverate_199L\n[train_credit_bureau_a_1_0, train_credit_burea...\nInterest rate of the closed contracts.\n\n\n18\nannualeffectiverate_63L\n[train_credit_bureau_a_1_0, train_credit_burea...\nInterest rate for the active contracts.\n\n\n19\nannuity_780A\n[train_static_0_0, train_static_0_1]\nMonthly annuity amount.\n\n\n20\nannuity_853A\n[train_applprev_1_0, train_applprev_1_1]\nMonthly annuity for previous applications.\n\n\n21\nannuitynextmonth_57A\n[train_static_0_0, train_static_0_1]\nNext month's amount of annuity.\n\n\n22\napplicationcnt_361L\n[train_static_0_0, train_static_0_1]\nNumber of applications associated with the sam...\n\n\n23\napplications30d_658L\n[train_static_0_0, train_static_0_1]\nNumber of applications made by the client in t...\n\n\n24\napplicationscnt_1086L\n[train_static_0_0, train_static_0_1]\nNumber of applications associated with the sam...\n\n\n25\napplicationscnt_464L\n[train_static_0_0, train_static_0_1]\nNumber of applications made in the last 30 day...\n\n\n26\napplicationscnt_629L\n[train_static_0_0, train_static_0_1]\nNumber of applications with the same employer ...\n\n\n27\napplicationscnt_867L\n[train_static_0_0, train_static_0_1]\nNumber of applications associated with the sam...\n\n\n28\napprovaldate_319D\n[train_applprev_1_0, train_applprev_1_1]\nApproval Date of Previous Application\n\n\n29\nassignmentdate_238D\n[train_static_cb_0]\nTax authority data - date of assignment.\n\n\n30\nassignmentdate_4527235D\n[train_static_cb_0]\nTax authority data - Date of assignment.\n\n\n31\nassignmentdate_4955616D\n[train_static_cb_0]\nTax authority assignment date.\n\n\n32\navgdbddpdlast24m_3658932P\n[train_static_0_0, train_static_0_1]\nAverage days past or before due of payment dur...\n\n\n33\navgdbddpdlast3m_4187120P\n[train_static_0_0, train_static_0_1]\nAverage days past or before due of payment dur...\n\n\n34\navgdbdtollast24m_4525197P\n[train_static_0_0, train_static_0_1]\nAverage days of payment before due date within...\n\n\n35\navgdpdtolclosure24_3658938P\n[train_static_0_0, train_static_0_1]\nAverage DPD (days past due) with tolerance wit...\n\n\n36\navginstallast24m_3658937A\n[train_static_0_0, train_static_0_1]\nAverage instalments paid by the client over th...\n\n\n37\navglnamtstart24m_4525187A\n[train_static_0_0, train_static_0_1]\nAverage loan amount in the last 24 months.\n\n\n38\navgmaxdpdlast9m_3716943P\n[train_static_0_0, train_static_0_1]\nAverage Days Past Due (DPD) of the client in l...\n\n\n39\navgoutstandbalancel6m_4187114A\n[train_static_0_0, train_static_0_1]\nAverage outstanding balance of applicant for t...\n\n\n40\navgpmtlast12m_4525200A\n[train_static_0_0, train_static_0_1]\nAverage of payments made by the client in the ...\n\n\n41\nbankacctype_710L\n[train_static_0_0, train_static_0_1]\nType of applicant's bank account.\n\n\n42\nbirth_259D\n[train_person_1]\nDate of birth of the person.\n\n\n43\nbirthdate_574D\n[train_static_cb_0]\nClient's date of birth (credit bureau data).\n\n\n44\nbirthdate_87D\n[train_person_1]\nBirth date of the person.\n\n\n45\nbyoccupationinc_3656910L\n[train_applprev_1_0, train_applprev_1_1]\nApplicant's income from previous applications.\n\n\n46\ncacccardblochreas_147M\n[train_applprev_2]\nCard blocking reason.\n\n\n47\ncancelreason_3545846M\n[train_applprev_1_0, train_applprev_1_1]\nApplication cancellation reason.\n\n\n48\ncardtype_51L\n[train_static_0_0, train_static_0_1]\nType of credit card.\n\n\n49\ncase_id\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n50\nchildnum_185L\n[train_person_1]\nNumber of children of the applicant.\n\n\n51\nchildnum_21L\n[train_applprev_1_0, train_applprev_1_1]\nNumber of children in the previous application.\n\n\n52\nclassificationofcontr_1114M\n[train_credit_bureau_b_1]\nClassificiation of the active contract.\n\n\n53\nclassificationofcontr_13M\n[train_credit_bureau_a_1_0, train_credit_burea...\nClassificiation of the active contract.\n\n\n54\nclassificationofcontr_400M\n[train_credit_bureau_a_1_0, train_credit_burea...\nClassificiation of the closed contract.\n\n\n55\nclientscnt12m_3712952L\n[train_static_0_0, train_static_0_1]\nNumber of clients that have used the same mobi...\n\n\n56\nclientscnt3m_3712950L\n[train_static_0_0, train_static_0_1]\nNumber of clients who have the same mobile pho...\n\n\n57\nclientscnt6m_3712949L\n[train_static_0_0, train_static_0_1]\nTotal number of clients who have used the same...\n\n\n58\nclientscnt_100L\n[train_static_0_0, train_static_0_1]\nNumber of applications with matching employer'...\n\n\n59\nclientscnt_1022L\n[train_static_0_0, train_static_0_1]\nNumber of clients sharing the same mobile phone.\n\n\n60\nclientscnt_1071L\n[train_static_0_0, train_static_0_1]\nNumber of applications where the alternative p...\n\n\n61\nclientscnt_1130L\n[train_static_0_0, train_static_0_1]\nNumber of applications where client's phone nu...\n\n\n62\nclientscnt_136L\n[train_static_0_0, train_static_0_1]\nNumber of applications associated with same em...\n\n\n63\nclientscnt_157L\n[train_static_0_0, train_static_0_1]\nNumber of clients whose employer has the same ...\n\n\n64\nclientscnt_257L\n[train_static_0_0, train_static_0_1]\nNumber of clients that share an alternative ph...\n\n\n65\nclientscnt_304L\n[train_static_0_0, train_static_0_1]\nNumber of clients with the same phone number.\n\n\n66\nclientscnt_360L\n[train_static_0_0, train_static_0_1]\nNumber of clients that have the same alternati...\n\n\n67\nclientscnt_493L\n[train_static_0_0, train_static_0_1]\nNumber of clients with matching phone numbers ...\n\n\n68\nclientscnt_533L\n[train_static_0_0, train_static_0_1]\nNumber of clients with same client's and alter...\n\n\n69\nclientscnt_887L\n[train_static_0_0, train_static_0_1]\nNumber of clients sharing the same employer's ...\n\n\n70\nclientscnt_946L\n[train_static_0_0, train_static_0_1]\nNumber of clients with matching mobile and emp...\n\n\n71\ncntincpaycont9m_3716944L\n[train_static_0_0, train_static_0_1]\nNumber of incoming payments in the past 9 months.\n\n\n72\ncntpmts24_3658933L\n[train_static_0_0, train_static_0_1]\nNumber of months with any incoming payment in ...\n\n\n73\ncollater_typofvalofguarant_298M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral valuation type (active contract).\n\n\n74\ncollater_typofvalofguarant_407M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral valuation type (closed contract).\n\n\n75\ncollater_valueofguarantee_1124L\n[train_credit_bureau_a_2_4, train_credit_burea...\nValue of collateral for active contract.\n\n\n76\ncollater_valueofguarantee_876L\n[train_credit_bureau_a_2_4, train_credit_burea...\nValue of collateral for closed contract.\n\n\n77\ncollaterals_typeofguarante_359M\n[train_credit_bureau_a_2_4, train_credit_burea...\nType of collateral that was used as a guarante...\n\n\n78\ncollaterals_typeofguarante_669M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral type for the active contract.\n\n\n79\ncommnoinclast6m_3546845L\n[train_static_0_0, train_static_0_1]\nNumber of communications indicating low income...\n\n\n80\ncontaddr_district_15M\n[train_person_1]\nZip code of a contact person's address.\n\n\n81\ncontaddr_matchlist_1032L\n[train_person_1]\nIndicates whether the contact address is found...\n\n\n82\ncontaddr_smempladdr_334L\n[train_person_1]\nIndicates whether the contact address is the s...\n\n\n83\ncontaddr_zipcode_807M\n[train_person_1]\nZip code of contact address.\n\n\n84\ncontractdate_551D\n[train_credit_bureau_b_1]\nContract date of the active contract\n\n\n85\ncontractenddate_991D\n[train_deposit_1]\nEnd date of deposit contract.\n\n\n86\ncontractmaturitydate_151D\n[train_credit_bureau_b_1]\nEnd date of active contract.\n\n\n87\ncontractssum_5085716L\n[train_static_cb_0]\nTotal sum of values of contracts retrieved fro...\n\n\n88\ncontractst_516M\n[train_credit_bureau_b_1]\nContract status.\n\n\n89\ncontractst_545M\n[train_credit_bureau_a_1_0, train_credit_burea...\nContract status.\n\n\n90\ncontractst_964M\n[train_credit_bureau_a_1_0, train_credit_burea...\nContract status of terminated credit contract.\n\n\n91\ncontractsum_5085717L\n[train_credit_bureau_a_1_0, train_credit_burea...\nSum of other contract values.\n\n\n92\ncontracttype_653M\n[train_credit_bureau_b_1]\nContract Type\n\n\n93\nconts_role_79M\n[train_person_2]\nType of contact role of a person.\n\n\n94\nconts_type_509L\n[train_applprev_2]\nPerson contact type in previous application.\n\n\n95\ncreationdate_885D\n[train_applprev_1_0, train_applprev_1_1]\nDate when previous application was created.\n\n\n96\ncredacc_actualbalance_314A\n[train_applprev_1_0, train_applprev_1_1]\nActual balance on credit account.\n\n\n97\ncredacc_cards_status_52L\n[train_applprev_2]\nCard status of the previous credit account.\n\n\n98\ncredacc_credlmt_575A\n[train_applprev_1_0, train_applprev_1_1]\nCredit card credit limit provided for previous...\n\n\n99\ncredacc_maxhisbal_375A\n[train_applprev_1_0, train_applprev_1_1]\nMaximal historical balance of previous credit ...\n\n\n100\ncredacc_minhisbal_90A\n[train_applprev_1_0, train_applprev_1_1]\nMinimum historical balance of previous credit ...\n\n\n101\ncredacc_status_367L\n[train_applprev_1_0, train_applprev_1_1]\nAccount status of previous credit applications.\n\n\n102\ncredacc_transactions_402L\n[train_applprev_1_0, train_applprev_1_1]\nNumber of transactions made with the previous ...\n\n\n103\ncredamount_590A\n[train_applprev_1_0, train_applprev_1_1]\nLoan amount or card limit of previous applicat...\n\n\n104\ncredamount_770A\n[train_static_0_0, train_static_0_1]\nLoan amount or credit card limit.\n\n\n105\ncredlmt_1052A\n[train_credit_bureau_b_1]\nCredit limit of an active loan.\n\n\n106\ncredlmt_228A\n[train_credit_bureau_b_1]\nCredit limit for closed loans.\n\n\n107\ncredlmt_230A\n[train_credit_bureau_a_1_0, train_credit_burea...\nCredit limit of the closed credit contracts fr...\n\n\n108\ncredlmt_3940954A\n[train_credit_bureau_b_1]\nCredit limit for active loan.\n\n\n109\ncredlmt_935A\n[train_credit_bureau_a_1_0, train_credit_burea...\nCredit limit for active loan.\n\n\n110\ncredor_3940957M\n[train_credit_bureau_b_1]\nCreditor's name\n\n\n111\ncredquantity_1099L\n[train_credit_bureau_b_1]\nNumber of credits in credit bureau\n\n\n112\ncredquantity_984L\n[train_credit_bureau_b_1]\nNumber of closed credits in credit bureau.\n\n\n113\ncredtype_322L\n[train_static_0_0, train_static_0_1]\nType of credit.\n\n\n114\ncredtype_587L\n[train_applprev_1_0, train_applprev_1_1]\nCredit type of previous application.\n\n\n115\ncurrdebt_22A\n[train_static_0_0, train_static_0_1]\nCurrent debt amount of the client.\n\n\n116\ncurrdebt_94A\n[train_applprev_1_0, train_applprev_1_1]\nPrevious application's current debt.\n\n\n117\ncurrdebtcredtyperange_828A\n[train_static_0_0, train_static_0_1]\nCurrent amount of debt of the applicant.\n\n\n118\ndate_decision\n[train_base]\nNaN\n\n\n119\ndateactivated_425D\n[train_applprev_1_0, train_applprev_1_1]\nContract activation date of the applicant's pr...\n\n\n120\ndatefirstoffer_1144D\n[train_static_0_0, train_static_0_1]\nDate of first customer relationship management...\n\n\n121\ndatelastinstal40dpd_247D\n[train_static_0_0, train_static_0_1]\nDate of last instalment that was more than 40 ...\n\n\n122\ndatelastunpaid_3546854D\n[train_static_0_0, train_static_0_1]\nDate of the last unpaid instalment.\n\n\n123\ndateofbirth_337D\n[train_static_cb_0]\nClient's date of birth.\n\n\n124\ndateofbirth_342D\n[train_static_cb_0]\nClient's date of birth.\n\n\n125\ndateofcredend_289D\n[train_credit_bureau_a_1_0, train_credit_burea...\nEnd date of an active credit contract.\n\n\n126\ndateofcredend_353D\n[train_credit_bureau_a_1_0, train_credit_burea...\nEnd date of a closed credit contract.\n\n\n127\ndateofcredstart_181D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate when the credit contract was closed.\n\n\n128\ndateofcredstart_739D\n[train_credit_bureau_a_1_0, train_credit_burea...\nStart date of a closed credit contract.\n\n\n129\ndateofrealrepmt_138D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of credit's closure (contract termination...\n\n\n130\ndays120_123L\n[train_static_cb_0]\nNumber of credit bureau queries for the last 1...\n\n\n131\ndays180_256L\n[train_static_cb_0]\nNumber of credit bureau queries for last 180 d...\n\n\n132\ndays30_165L\n[train_static_cb_0]\nNumber of credit bureau queries for the last 3...\n\n\n133\ndays360_512L\n[train_static_cb_0]\nNumber of Credit Bureau queries for last 360 d...\n\n\n134\ndays90_310L\n[train_static_cb_0]\nNumber of credit bureau queries for the last 9...\n\n\n135\ndaysoverduetolerancedd_3976961L\n[train_static_0_0, train_static_0_1]\nNumber of days that past after the due date (w...\n\n\n136\ndebtoutstand_525A\n[train_credit_bureau_a_1_0, train_credit_burea...\nOutstanding amount of existing contract.\n\n\n137\ndebtoverdue_47A\n[train_credit_bureau_a_1_0, train_credit_burea...\nAmount that is currently past due on a client'...\n\n\n138\ndebtpastduevalue_732A\n[train_credit_bureau_b_1]\nAmount of unpaid debt for existing contracts.\n\n\n139\ndebtvalue_227A\n[train_credit_bureau_b_1]\nOutstanding amount for existing debt contracts.\n\n\n140\ndeductiondate_4917603D\n[train_tax_registry_b_1]\nTax deduction date.\n\n\n141\ndeferredmnthsnum_166L\n[train_static_0_0, train_static_0_1]\nNumber of deferred months.\n\n\n142\ndescription_351M\n[train_credit_bureau_a_1_0, train_credit_burea...\nCategorization of clients by credit bureau.\n\n\n143\ndescription_5085714M\n[train_static_cb_0]\nCategorization of clients by credit bureau.\n\n\n144\ndisbursedcredamount_1113A\n[train_static_0_0, train_static_0_1]\nDisbursed credit amount after consolidation.\n\n\n145\ndisbursementtype_67L\n[train_static_0_0, train_static_0_1]\nType of disbursement.\n\n\n146\ndistrict_544M\n[train_applprev_1_0, train_applprev_1_1]\nDistrict of the address used in the previous l...\n\n\n147\ndownpmt_116A\n[train_static_0_0, train_static_0_1]\nAmount of downpayment.\n\n\n148\ndownpmt_134A\n[train_applprev_1_0, train_applprev_1_1]\nPrevious application downpayment amount.\n\n\n149\ndpd_550P\n[train_credit_bureau_b_1]\nThe number of days past due for active loans w...\n\n\n150\ndpd_733P\n[train_credit_bureau_b_1]\nDays past due (DPD) for guaranteed loans that ...\n\n\n151\ndpdmax_139P\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximal days past due for active contract.\n\n\n152\ndpdmax_757P\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximum days past due for a closed contract.\n\n\n153\ndpdmax_851P\n[train_credit_bureau_b_1]\nMaximal past due days for active contracts in ...\n\n\n154\ndpdmaxdatemonth_442T\n[train_credit_bureau_a_1_0, train_credit_burea...\nMax DPD occurrence month for terminated contra...\n\n\n155\ndpdmaxdatemonth_804T\n[train_credit_bureau_b_1]\nMonth when the maximum Day Past Due (DPD) occu...\n\n\n156\ndpdmaxdatemonth_89T\n[train_credit_bureau_a_1_0, train_credit_burea...\nMonth when maximum days past due occurred on t...\n\n\n157\ndpdmaxdateyear_596T\n[train_credit_bureau_a_1_0, train_credit_burea...\nYear when maximum Days Past Due (DPD) occurred...\n\n\n158\ndpdmaxdateyear_742T\n[train_credit_bureau_b_1]\nYear of the maximum Days Past Due (DPD) on an ...\n\n\n159\ndpdmaxdateyear_896T\n[train_credit_bureau_a_1_0, train_credit_burea...\nYear of maximum Days Past Due of closed contra...\n\n\n160\ndtlastpmt_581D\n[train_applprev_1_0, train_applprev_1_1]\nDate of last payment made by the applicant.\n\n\n161\ndtlastpmtallstes_3545839D\n[train_applprev_1_0, train_applprev_1_1]\nDate of the applicant's last payment.\n\n\n162\ndtlastpmtallstes_4499206D\n[train_static_0_0, train_static_0_1]\nDate of last payment made by the applicant.\n\n\n163\neducation_1103M\n[train_static_cb_0]\nLevel of education of the client provided by e...\n\n\n164\neducation_1138M\n[train_applprev_1_0, train_applprev_1_1]\nApplicant's education level from their previou...\n\n\n165\neducation_88M\n[train_static_cb_0]\nEducation level of the client.\n\n\n166\neducation_927M\n[train_person_1]\nEducation level of the person.\n\n\n167\neir_270L\n[train_static_0_0, train_static_0_1]\nInterest rate.\n\n\n168\nempl_employedfrom_271D\n[train_person_1]\nStart date of employment.\n\n\n169\nempl_employedtotal_800L\n[train_person_1]\nEmployment length of a person.\n\n\n170\nempl_industry_691L\n[train_person_1]\nEmployment Industry of the person.\n\n\n171\nempladdr_district_926M\n[train_person_1]\nDistrict where the employer's address is located.\n\n\n172\nempladdr_zipcode_114M\n[train_person_1]\nZipcode of employer's address.\n\n\n173\nemployedfrom_700D\n[train_applprev_1_0, train_applprev_1_1]\nEmployment start date from the previous applic...\n\n\n174\nemployername_160M\n[train_tax_registry_c_1]\nEmployer's name.\n\n\n175\nempls_economicalst_849M\n[train_person_2]\nThe economical status of the person (num_group...\n\n\n176\nempls_employedfrom_796D\n[train_person_2]\nStart of employment (num_group1 - person, num_...\n\n\n177\nempls_employer_name_740M\n[train_person_2]\nEmployer's name (num_group1 - person, num_grou...\n\n\n178\nequalitydataagreement_891L\n[train_static_0_0, train_static_0_1]\nFlag indicating sudden changes in client's soc...\n\n\n179\nequalityempfrom_62L\n[train_static_0_0, train_static_0_1]\nFlag indicating a sudden change in the client'...\n\n\n180\nfamilystate_447L\n[train_person_1]\nFamily state of the person.\n\n\n181\nfamilystate_726L\n[train_applprev_1_0, train_applprev_1_1]\nFamily State in previous application of applic...\n\n\n182\nfinancialinstitution_382M\n[train_credit_bureau_a_1_0, train_credit_burea...\nName of financial institution that is linked t...\n\n\n183\nfinancialinstitution_591M\n[train_credit_bureau_a_1_0, train_credit_burea...\nFinancial institution name of the active contr...\n\n\n184\nfirstclxcampaign_1125D\n[train_static_0_0, train_static_0_1]\nDate of the client's first campaign.\n\n\n185\nfirstdatedue_489D\n[train_static_0_0, train_static_0_1]\nDate of the first due date.\n\n\n186\nfirstnonzeroinstldate_307D\n[train_applprev_1_0, train_applprev_1_1]\nDate of first instalment in the previous appli...\n\n\n187\nfirstquarter_103L\n[train_static_cb_0]\nNumber of results obtained from credit bureau ...\n\n\n188\nfor3years_128L\n[train_static_cb_0]\nNumber of rejected applications in the past 3 ...\n\n\n189\nfor3years_504L\n[train_static_cb_0]\nClient's credit history data over the last thr...\n\n\n190\nfor3years_584L\n[train_static_cb_0]\nNumber of cancellations in the last 3 years.\n\n\n191\nformonth_118L\n[train_static_cb_0]\nNumber of rejections in a month.\n\n\n192\nformonth_206L\n[train_static_cb_0]\nNumber of cancelations in the previous month.\n\n\n193\nformonth_535L\n[train_static_cb_0]\nCredit history for the last month.\n\n\n194\nforquarter_1017L\n[train_static_cb_0]\nNumber of cancellations recorded in the credit...\n\n\n195\nforquarter_462L\n[train_static_cb_0]\nNumber of credit applications that were reject...\n\n\n196\nforquarter_634L\n[train_static_cb_0]\nCredit history for the last quarter.\n\n\n197\nfortoday_1092L\n[train_static_cb_0]\nClient's credit history for today.\n\n\n198\nforweek_1077L\n[train_static_cb_0]\nNumber of cancelations in the last week.\n\n\n199\nforweek_528L\n[train_static_cb_0]\nCredit history for the last week.\n\n\n200\nforweek_601L\n[train_static_cb_0]\nNumber of rejected applications in the last week.\n\n\n201\nforyear_618L\n[train_static_cb_0]\nNumber of application rejections in the previo...\n\n\n202\nforyear_818L\n[train_static_cb_0]\nNumber of cancelations that occurred in last y...\n\n\n203\nforyear_850L\n[train_static_cb_0]\nCredit history for the last year.\n\n\n204\nfourthquarter_440L\n[train_static_cb_0]\nNumber of results in fourth quarter.\n\n\n205\ngender_992L\n[train_person_1]\nGender of a person.\n\n\n206\nhomephncnt_628L\n[train_static_0_0, train_static_0_1]\nNumber of distinct home phones on client's app...\n\n\n207\nhousetype_905L\n[train_person_1]\nHouse type of the person.\n\n\n208\nhousingtype_772L\n[train_person_1]\nType of housing of the person.\n\n\n209\nincometype_1044T\n[train_person_1]\nType of income of the person\n\n\n210\ninittransactionamount_650A\n[train_static_0_0, train_static_0_1]\nInitial transaction amount of the credit appli...\n\n\n211\ninittransactioncode_186L\n[train_static_0_0, train_static_0_1]\nTransaction type of the initial credit transac...\n\n\n212\ninittransactioncode_279L\n[train_applprev_1_0, train_applprev_1_1]\nType of the initial transaction made in the pr...\n\n\n213\ninstallmentamount_644A\n[train_credit_bureau_b_1]\nInstalment amount of a closed and secured cred...\n\n\n214\ninstallmentamount_833A\n[train_credit_bureau_b_1]\nInstalment amount for a secured and active con...\n\n\n215\ninstlamount_768A\n[train_credit_bureau_a_1_0, train_credit_burea...\nInstalment amount for the active contract in c...\n\n\n216\ninstlamount_852A\n[train_credit_bureau_a_1_0, train_credit_burea...\nInstalment amount for closed contract.\n\n\n217\ninstlamount_892A\n[train_credit_bureau_b_1]\nInstalment amount for active credit contract.\n\n\n218\ninteresteffectiverate_369L\n[train_credit_bureau_b_1]\nInterest rate on active contract.\n\n\n219\ninterestrate_311L\n[train_static_0_0, train_static_0_1]\nThe interest rate of the active credit contract.\n\n\n220\ninterestrate_508L\n[train_credit_bureau_a_1_0, train_credit_burea...\nInterest rate for a closed contract in the cre...\n\n\n221\ninterestrategrace_34L\n[train_static_0_0, train_static_0_1]\nInterest rate during the grace period.\n\n\n222\ninterestrateyearly_538L\n[train_credit_bureau_b_1]\nAnnual interest rate for existing contract obt...\n\n\n223\nisbidproduct_1095L\n[train_static_0_0, train_static_0_1]\nFlag indicating if the product is a cross-sell.\n\n\n224\nisbidproduct_390L\n[train_applprev_1_0, train_applprev_1_1]\nFlag for determining if the product is a cross...\n\n\n225\nisbidproductrequest_292L\n[train_static_0_0, train_static_0_1]\nFlag indicating if the product is a cross-sell.\n\n\n226\nisdebitcard_527L\n[train_applprev_1_0, train_applprev_1_1]\nPrevious application flag indicating if produc...\n\n\n227\nisdebitcard_729L\n[train_static_0_0, train_static_0_1]\nFlag indicating if the product is a debit card.\n\n\n228\nisreference_387L\n[train_person_1]\nFlag indicating whether the person is a refere...\n\n\n229\nlanguage1_981M\n[train_person_1]\nThe primary language of the person.\n\n\n230\nlast180dayaveragebalance_704A\n[train_debitcard_1]\nAverage balance on debit card in the last 180 ...\n\n\n231\nlast180dayturnover_1134A\n[train_debitcard_1]\nDebit card's turnover within the last 180 days.\n\n\n232\nlast30dayturnover_651A\n[train_debitcard_1]\nDebit card turnover for the last 30 days.\n\n\n233\nlastactivateddate_801D\n[train_static_0_0, train_static_0_1]\nContract activation date for previous applicat...\n\n\n234\nlastapplicationdate_877D\n[train_static_0_0, train_static_0_1]\nDate of previous customer's application.\n\n\n235\nlastapprcommoditycat_1041M\n[train_static_0_0, train_static_0_1]\nCommodity category of the last loan applicatio...\n\n\n236\nlastapprcommoditytypec_5251766M\n[train_static_0_0, train_static_0_1]\nCommodity type of the last application.\n\n\n237\nlastapprcredamount_781A\n[train_static_0_0, train_static_0_1]\nCredit amount from the client's last application.\n\n\n238\nlastapprdate_640D\n[train_static_0_0, train_static_0_1]\nDate of approval on client's most recent previ...\n\n\n239\nlastcancelreason_561M\n[train_static_0_0, train_static_0_1]\nCancellation reason of the last application.\n\n\n240\nlastdelinqdate_224D\n[train_static_0_0, train_static_0_1]\nDate of the last delinquency occurrence.\n\n\n241\nlastdependentsnum_448L\n[train_static_0_0, train_static_0_1]\nNumber of dependents in the client's last loan...\n\n\n242\nlastotherinc_902A\n[train_static_0_0, train_static_0_1]\nAmount of other income reported by the client ...\n\n\n243\nlastotherlnsexpense_631A\n[train_static_0_0, train_static_0_1]\nMonthly expenses on other loans from the last ...\n\n\n244\nlastrejectcommoditycat_161M\n[train_static_0_0, train_static_0_1]\nCategory of commodity in the applicant's last ...\n\n\n245\nlastrejectcommodtypec_5251769M\n[train_static_0_0, train_static_0_1]\nCommodity type of the last rejected application.\n\n\n246\nlastrejectcredamount_222A\n[train_static_0_0, train_static_0_1]\nCredit amount on last rejected application.\n\n\n247\nlastrejectdate_50D\n[train_static_0_0, train_static_0_1]\nDate of most recent rejected application by th...\n\n\n248\nlastrejectreason_759M\n[train_static_0_0, train_static_0_1]\nReason for rejection on the most recent reject...\n\n\n249\nlastrejectreasonclient_4145040M\n[train_static_0_0, train_static_0_1]\nReason for the client's last loan rejection.\n\n\n250\nlastrepayingdate_696D\n[train_static_0_0, train_static_0_1]\nDate of the last payment made by the applicant.\n\n\n251\nlastst_736L\n[train_static_0_0, train_static_0_1]\nStatus of the client's previous credit applica...\n\n\n252\nlastupdate_1112D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of last update for an active contract fro...\n\n\n253\nlastupdate_260D\n[train_credit_bureau_b_1]\nLast update date for the active contracts.\n\n\n254\nlastupdate_388D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of last update for a closed contract in t...\n\n\n255\nmaininc_215A\n[train_static_0_0, train_static_0_1]\nClient's primary income amount.\n\n\n256\nmainoccupationinc_384A\n[train_person_1]\nAmount of the main income of the client.\n\n\n257\nmainoccupationinc_437A\n[train_applprev_1_0, train_applprev_1_1]\nClient's main income amount in their previous ...\n\n\n258\nmaritalst_385M\n[train_static_cb_0]\nMarital status of the client.\n\n\n259\nmaritalst_703L\n[train_person_1]\nMarital status of the client.\n\n\n260\nmaritalst_893M\n[train_static_cb_0]\nMarital status of the client\n\n\n261\nmastercontrelectronic_519L\n[train_static_0_0, train_static_0_1]\nFlag indicating the existence of the master co...\n\n\n262\nmastercontrexist_109L\n[train_static_0_0, train_static_0_1]\nFlag indicating whether or not the applicant h...\n\n\n263\nmaxannuity_159A\n[train_static_0_0, train_static_0_1]\nMaximum annuity previously obtained by client.\n\n\n264\nmaxannuity_4075009A\n[train_static_0_0, train_static_0_1]\nMaximal annuity offered to the client in the c...\n\n\n265\nmaxdbddpdlast1m_3658939P\n[train_static_0_0, train_static_0_1]\nMaximum number of days past due in the last mo...\n\n\n266\nmaxdbddpdtollast12m_3658940P\n[train_static_0_0, train_static_0_1]\nMaximum number of days past due in last 12 mon...\n\n\n267\nmaxdbddpdtollast6m_4187119P\n[train_static_0_0, train_static_0_1]\nMaximum number of days past due in last 6 mont...\n\n\n268\nmaxdebt4_972A\n[train_static_0_0, train_static_0_1]\nMaximal principal debt of the client in the hi...\n\n\n269\nmaxdebtpduevalodued_3940955A\n[train_credit_bureau_b_1]\nDays past due at the time of the maximum debt.\n\n\n270\nmaxdpdfrom6mto36m_3546853P\n[train_static_0_0, train_static_0_1]\nMaximum Days Past Due (DPD) in the period rang...\n\n\n271\nmaxdpdinstldate_3546855D\n[train_static_0_0, train_static_0_1]\nDate of instalment on which client was most da...\n\n\n272\nmaxdpdinstlnum_3546846P\n[train_static_0_0, train_static_0_1]\nInstalment number of which client was most day...\n\n\n273\nmaxdpdlast12m_727P\n[train_static_0_0, train_static_0_1]\nMaximum days past due in the past 12 months.\n\n\n274\nmaxdpdlast24m_143P\n[train_static_0_0, train_static_0_1]\nMaximal days past due in the last 24 months.\n\n\n275\nmaxdpdlast3m_392P\n[train_static_0_0, train_static_0_1]\nMaximum number of days past due in last 3 months.\n\n\n276\nmaxdpdlast6m_474P\n[train_static_0_0, train_static_0_1]\nMaximum days past due in the last 6 months.\n\n\n277\nmaxdpdlast9m_1059P\n[train_static_0_0, train_static_0_1]\nMaximum days past due in last 9 months.\n\n\n278\nmaxdpdtolerance_374P\n[train_static_0_0, train_static_0_1]\nMaximum number of days past due (with tolerance).\n\n\n279\nmaxdpdtolerance_577P\n[train_applprev_1_0, train_applprev_1_1]\nMaximum DPD with tolerance (on previous applic...\n\n\n280\nmaxinstallast24m_3658928A\n[train_static_0_0, train_static_0_1]\nMaximum instalment in the last 24 months\n\n\n281\nmaxlnamtstart6m_4525199A\n[train_static_0_0, train_static_0_1]\nMaximum loan amount started in the last 6 months.\n\n\n282\nmaxoutstandbalancel12m_4187113A\n[train_static_0_0, train_static_0_1]\nMaximum outstanding balance in the last 12 mon...\n\n\n283\nmaxpmtlast3m_4525190A\n[train_static_0_0, train_static_0_1]\nMaximum payment made by the client in the last...\n\n\n284\nmindbddpdlast24m_3658935P\n[train_static_0_0, train_static_0_1]\nMinimum days past due (or days before due) in ...\n\n\n285\nmindbdtollast24m_4525191P\n[train_static_0_0, train_static_0_1]\nMinimum days before due in last 24 months.\n\n\n286\nmobilephncnt_593L\n[train_static_0_0, train_static_0_1]\nNumber of persons with the same mobile phone n...\n\n\n287\nmonthlyinstlamount_332A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMonthly instalment amount for active contract.\n\n\n288\nmonthlyinstlamount_674A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMonthly amount of instalment payment on a clos...\n\n\n289\nmonthsannuity_845L\n[train_static_0_0, train_static_0_1]\nMonthly annuity amount for the applicant.\n\n\n290\nname_4527232M\n[train_tax_registry_a_1]\nName of employer.\n\n\n291\nname_4917606M\n[train_tax_registry_b_1]\nName of employer.\n\n\n292\nnominalrate_281L\n[train_credit_bureau_a_1_0, train_credit_burea...\nInterest rate of the active contract.\n\n\n293\nnominalrate_498L\n[train_credit_bureau_a_1_0, train_credit_burea...\nInterest rate for closed contract.\n\n\n294\nnum_group1\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n295\nnum_group2\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n296\nnumactivecreds_622L\n[train_static_0_0, train_static_0_1]\nNumber of active credits.\n\n\n297\nnumactivecredschannel_414L\n[train_static_0_0, train_static_0_1]\nNumber of active credits.\n\n\n298\nnumactiverelcontr_750L\n[train_static_0_0, train_static_0_1]\nNumber of active revolving credits.\n\n\n299\nnumberofcontrsvalue_258L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of active contracts in credit bureau.\n\n\n300\nnumberofcontrsvalue_358L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of closed credit contracts.\n\n\n301\nnumberofinstls_229L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of instalments on closed contract.\n\n\n302\nnumberofinstls_320L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of instalments of the active contract.\n\n\n303\nnumberofinstls_810L\n[train_credit_bureau_b_1]\nNumber of instalments for the active contract.\n\n\n304\nnumberofoutstandinstls_520L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of outstanding instalment for closed co...\n\n\n305\nnumberofoutstandinstls_59L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of outstanding instalments for the acti...\n\n\n306\nnumberofoverdueinstlmax_1039L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of outstanding instalments for active c...\n\n\n307\nnumberofoverdueinstlmax_1151L\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximum number of past due installments for a ...\n\n\n308\nnumberofoverdueinstlmaxdat_148D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of maximum number of past due instalments...\n\n\n309\nnumberofoverdueinstlmaxdat_641D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of maximum number of past due instalments...\n\n\n310\nnumberofoverdueinstls_725L\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximum number of past due instalments for an ...\n\n\n311\nnumberofoverdueinstls_834L\n[train_credit_bureau_a_1_0, train_credit_burea...\nNumber of past due instalments for a closed co...\n\n\n312\nnumberofqueries_373L\n[train_static_cb_0]\nNumber of queries to credit bureau.\n\n\n313\nnumcontrs3months_479L\n[train_static_0_0, train_static_0_1]\nNumber of contracts in last 3 months.\n\n\n314\nnumincomingpmts_3546848L\n[train_static_0_0, train_static_0_1]\nNumber of incoming payments.\n\n\n315\nnuminstlallpaidearly3d_817L\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid at least 3 days pri...\n\n\n316\nnuminstls_657L\n[train_static_0_0, train_static_0_1]\nNumber of instalments.\n\n\n317\nnuminstlsallpaid_934L\n[train_static_0_0, train_static_0_1]\nNumber of paid instalments.\n\n\n318\nnuminstlswithdpd10_728L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that were overdue for 10...\n\n\n319\nnuminstlswithdpd5_4187116L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that were overdue by at ...\n\n\n320\nnuminstlswithoutdpd_562L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that were not past due d...\n\n\n321\nnuminstmatpaidtearly2d_4499204L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that have been paid more...\n\n\n322\nnuminstpaid_4499208L\n[train_static_0_0, train_static_0_1]\nNumber of paid instalments.\n\n\n323\nnuminstpaidearly3d_3546850L\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid more than three day...\n\n\n324\nnuminstpaidearly3dest_4493216L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that have been paid more...\n\n\n325\nnuminstpaidearly5d_1087L\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid more than 5 days pr...\n\n\n326\nnuminstpaidearly5dest_4493211L\n[train_static_0_0, train_static_0_1]\nNumber of instalments that were paid more than...\n\n\n327\nnuminstpaidearly5dobd_4499205L\n[train_static_0_0, train_static_0_1]\nNumber of installments paid more than 5 days p...\n\n\n328\nnuminstpaidearly_338L\n[train_static_0_0, train_static_0_1]\nNumber of installments paid prior to the due d...\n\n\n329\nnuminstpaidearlyest_4493214L\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid before the due date.\n\n\n330\nnuminstpaidlastcontr_4325080L\n[train_static_0_0, train_static_0_1]\nNumber of paid installments from the client's ...\n\n\n331\nnuminstpaidlate1d_3546852L\n[train_static_0_0, train_static_0_1]\nNumber of instalments paid more than 1 day pas...\n\n\n332\nnuminstregularpaid_973L\n[train_static_0_0, train_static_0_1]\nNumber of fully paid regular installments in t...\n\n\n333\nnuminstregularpaidest_4493210L\n[train_static_0_0, train_static_0_1]\nNumber of fully paid regular installments on c...\n\n\n334\nnuminsttopaygr_769L\n[train_static_0_0, train_static_0_1]\nNumber of unpaid instalments.\n\n\n335\nnuminsttopaygrest_4493213L\n[train_static_0_0, train_static_0_1]\nNumber of unpaid instalments.\n\n\n336\nnuminstunpaidmax_3546851L\n[train_static_0_0, train_static_0_1]\nMaximum number of unpaid instalments.\n\n\n337\nnuminstunpaidmaxest_4493212L\n[train_static_0_0, train_static_0_1]\nMaximum number of unpaid instalments.\n\n\n338\nnumnotactivated_1143L\n[train_static_0_0, train_static_0_1]\nNumber of non-activated credits.\n\n\n339\nnumpmtchanneldd_318L\n[train_static_0_0, train_static_0_1]\nNumber of previous loan contracts for the appl...\n\n\n340\nnumrejects9m_859L\n[train_static_0_0, train_static_0_1]\nNumber of credit applications that were reject...\n\n\n341\nopencred_647L\n[train_static_0_0, train_static_0_1]\nNumber of active loans from the previous appli...\n\n\n342\nopeningdate_313D\n[train_deposit_1]\nDeposit account opening date.\n\n\n343\nopeningdate_857D\n[train_debitcard_1]\nDebit card opening date.\n\n\n344\noutstandingamount_354A\n[train_credit_bureau_a_1_0, train_credit_burea...\nOutstanding amount for closed credit contract ...\n\n\n345\noutstandingamount_362A\n[train_credit_bureau_a_1_0, train_credit_burea...\nActive contract's outstanding amount.\n\n\n346\noutstandingdebt_522A\n[train_applprev_1_0, train_applprev_1_1]\nAmount of outstanding debt on the client's pre...\n\n\n347\noverdueamount_31A\n[train_credit_bureau_a_1_0, train_credit_burea...\nPast due amount for a closed contract.\n\n\n348\noverdueamount_659A\n[train_credit_bureau_a_1_0, train_credit_burea...\nPast due amount for active contract.\n\n\n349\noverdueamountmax2_14A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximal past due amount for an active contract.\n\n\n350\noverdueamountmax2_398A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximal overdue amount for a closed contract.\n\n\n351\noverdueamountmax2date_1002D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of maximal past due amount for a closed c...\n\n\n352\noverdueamountmax2date_1142D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate of maximal past due amount for an active ...\n\n\n353\noverdueamountmax_155A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximal past due amount for active contract.\n\n\n354\noverdueamountmax_35A\n[train_credit_bureau_a_1_0, train_credit_burea...\nMaximal past due amount for a closed contract.\n\n\n355\noverdueamountmax_950A\n[train_credit_bureau_b_1]\nMaximal past due amount for active contract.\n\n\n356\noverdueamountmaxdatemonth_284T\n[train_credit_bureau_a_1_0, train_credit_burea...\nMonth when the maximum past due amount occurre...\n\n\n357\noverdueamountmaxdatemonth_365T\n[train_credit_bureau_a_1_0, train_credit_burea...\nMonth when maximum past due amount occurred fo...\n\n\n358\noverdueamountmaxdatemonth_494T\n[train_credit_bureau_b_1]\nMonth when the maximum past due amount was rec...\n\n\n359\noverdueamountmaxdateyear_2T\n[train_credit_bureau_a_1_0, train_credit_burea...\nYear when the maximum past due amount occurred...\n\n\n360\noverdueamountmaxdateyear_432T\n[train_credit_bureau_b_1]\nYear when max past due amount occurred for act...\n\n\n361\noverdueamountmaxdateyear_994T\n[train_credit_bureau_a_1_0, train_credit_burea...\nYear when maximum past due amount occurred for...\n\n\n362\npaytype1st_925L\n[train_static_0_0, train_static_0_1]\nType of first payment of the client.\n\n\n363\npaytype_783L\n[train_static_0_0, train_static_0_1]\nType of payment.\n\n\n364\npayvacationpostpone_4187118D\n[train_static_0_0, train_static_0_1]\nDate of last payment holiday instalment.\n\n\n365\npctinstlsallpaidearl3d_427L\n[train_static_0_0, train_static_0_1]\nPercentage of installments paid at least 3 day...\n\n\n366\npctinstlsallpaidlat10d_839L\n[train_static_0_0, train_static_0_1]\nPercentage of installments that were paid 10 o...\n\n\n367\npctinstlsallpaidlate1d_3546856L\n[train_static_0_0, train_static_0_1]\nPercentage of installments that are paid 1 or ...\n\n\n368\npctinstlsallpaidlate4d_3546849L\n[train_static_0_0, train_static_0_1]\nPercentage of installments that were paid 4 or...\n\n\n369\npctinstlsallpaidlate6d_3546844L\n[train_static_0_0, train_static_0_1]\nPercentage of installments that were paid 6 or...\n\n\n370\nperiodicityofpmts_1102L\n[train_credit_bureau_a_1_0, train_credit_burea...\nFrequency of instalments for a closed contract.\n\n\n371\nperiodicityofpmts_837L\n[train_credit_bureau_a_1_0, train_credit_burea...\nFrequency of instalments for an active contract.\n\n\n372\nperiodicityofpmts_997L\n[train_credit_bureau_b_1]\nFrequency of instalments for active credit con...\n\n\n373\nperiodicityofpmts_997M\n[train_credit_bureau_b_1]\nFrequency of instalments for active credit con...\n\n\n374\npersonindex_1023L\n[train_person_1]\nOrder of the person specified on the applicati...\n\n\n375\npersontype_1072L\n[train_person_1]\nPerson type.\n\n\n376\npersontype_792L\n[train_person_1]\nPerson type.\n\n\n377\npmtamount_36A\n[train_tax_registry_c_1]\nTax deductions amount for credit bureau payments.\n\n\n378\npmtaverage_3A\n[train_static_cb_0]\nAverage of tax deductions.\n\n\n379\npmtaverage_4527227A\n[train_static_cb_0]\nAverage of tax deductions.\n\n\n380\npmtaverage_4955615A\n[train_static_cb_0]\nAverage of tax deductions.\n\n\n381\npmtcount_4527229L\n[train_static_cb_0]\nNumber of tax deductions.\n\n\n382\npmtcount_4955617L\n[train_static_cb_0]\nNumber of tax deductions.\n\n\n383\npmtcount_693L\n[train_static_cb_0]\nNumber of tax deductions.\n\n\n384\npmtdaysoverdue_1135P\n[train_credit_bureau_b_1]\nNumber of days past due for existing contracts...\n\n\n385\npmtmethod_731M\n[train_credit_bureau_b_1]\nInstalment payment method for existing contrac...\n\n\n386\npmtnum_254L\n[train_static_0_0, train_static_0_1]\nTotal number of loan payments made by the client.\n\n\n387\npmtnum_8L\n[train_applprev_1_0, train_applprev_1_1]\nNumber of payments made for the previous appli...\n\n\n388\npmtnumpending_403L\n[train_credit_bureau_b_1]\nNumber of pending payments for active contract.\n\n\n389\npmts_date_1107D\n[train_credit_bureau_b_2]\nPayment date for an active contract according ...\n\n\n390\npmts_dpd_1073P\n[train_credit_bureau_a_2_4, train_credit_burea...\nDays past due of the payment for the active co...\n\n\n391\npmts_dpd_303P\n[train_credit_bureau_a_2_4, train_credit_burea...\nDays past due of the payment for terminated co...\n\n\n392\npmts_dpdvalue_108P\n[train_credit_bureau_b_2]\nValue of past due payment for active contract ...\n\n\n393\npmts_month_158T\n[train_credit_bureau_a_2_4, train_credit_burea...\nMonth of payment for a closed contract (num_gr...\n\n\n394\npmts_month_706T\n[train_credit_bureau_a_2_4, train_credit_burea...\nMonth of payment for active contract (num_grou...\n\n\n395\npmts_overdue_1140A\n[train_credit_bureau_a_2_4, train_credit_burea...\nOverdue payment for an active contract (num_gr...\n\n\n396\npmts_overdue_1152A\n[train_credit_bureau_a_2_4, train_credit_burea...\nOverdue payment for a closed contract (num_gro...\n\n\n397\npmts_pmtsoverdue_635A\n[train_credit_bureau_b_2]\nActive contract that has overdue payments (num...\n\n\n398\npmts_year_1139T\n[train_credit_bureau_a_2_4, train_credit_burea...\nYear of payment for an active contract (num_gr...\n\n\n399\npmts_year_507T\n[train_credit_bureau_a_2_4, train_credit_burea...\nPayment year for a closed credit contract (num...\n\n\n400\npmtscount_423L\n[train_static_cb_0]\nNumber of tax deduction payments.\n\n\n401\npmtssum_45A\n[train_static_cb_0]\nSum of tax deductions for the client.\n\n\n402\nposfpd10lastmonth_333P\n[train_static_0_0, train_static_0_1]\nAverage FPD10 (Share of contracts with first i...\n\n\n403\nposfpd30lastmonth_3976960P\n[train_static_0_0, train_static_0_1]\nAverage FPD30 (Share of contracts with first i...\n\n\n404\nposfstqpd30lastmonth_3976962P\n[train_static_0_0, train_static_0_1]\nAverage FSTPD30 (share of contracts with first...\n\n\n405\npostype_4733339M\n[train_applprev_1_0, train_applprev_1_1]\nType of point of sale.\n\n\n406\npreviouscontdistrict_112M\n[train_static_0_0, train_static_0_1]\nContact district of the client's previous appr...\n\n\n407\nprice_1097A\n[train_static_0_0, train_static_0_1]\nCredit price.\n\n\n408\nprocessingdate_168D\n[train_tax_registry_c_1]\nDate when the tax deduction is processed.\n\n\n409\nprofession_152M\n[train_applprev_1_0, train_applprev_1_1]\nProfession of the client during their previous...\n\n\n410\nprolongationcount_1120L\n[train_credit_bureau_a_1_0, train_credit_burea...\nCount of prolongations on terminated contract ...\n\n\n411\nprolongationcount_599L\n[train_credit_bureau_a_1_0, train_credit_burea...\nCount of active contract prolongations.\n\n\n412\npurposeofcred_426M\n[train_credit_bureau_a_1_0, train_credit_burea...\nPurpose of credit for active contract.\n\n\n413\npurposeofcred_722M\n[train_credit_bureau_b_1]\nPurpose of credit for active contracts.\n\n\n414\npurposeofcred_874M\n[train_credit_bureau_a_1_0, train_credit_burea...\nPurpose of credit on a closed contract.\n\n\n415\nrecorddate_4527225D\n[train_tax_registry_a_1]\nDate of tax deduction record.\n\n\n416\nrefreshdate_3813885D\n[train_credit_bureau_a_1_0, train_credit_burea...\nDate when the credit bureau's public sources h...\n\n\n417\nregistaddr_district_1083M\n[train_person_1]\nDistrict of person's registered address.\n\n\n418\nregistaddr_zipcode_184M\n[train_person_1]\nRegistered address's zip code of a person.\n\n\n419\nrejectreason_755M\n[train_applprev_1_0, train_applprev_1_1]\nReason for previous application rejection.\n\n\n420\nrejectreasonclient_4145042M\n[train_applprev_1_0, train_applprev_1_1]\nReason for rejection of the client's previous ...\n\n\n421\nrelatedpersons_role_762T\n[train_person_2]\nRelationship type of a client's related person...\n\n\n422\nrelationshiptoclient_415T\n[train_person_1]\nRelationship to the client.\n\n\n423\nrelationshiptoclient_642T\n[train_person_1]\nRelationship to the client.\n\n\n424\nremitter_829L\n[train_person_1]\nFlag indicating whether the client is a remitter.\n\n\n425\nrequesttype_4525192L\n[train_static_cb_0]\nTax authority request type.\n\n\n426\nresidualamount_1093A\n[train_credit_bureau_b_1]\nResidual amount of closed guarantee contract.\n\n\n427\nresidualamount_127A\n[train_credit_bureau_b_1]\nResidual amount of active guarantee contract.\n\n\n428\nresidualamount_3940956A\n[train_credit_bureau_b_1]\nResidual amount for the active contract.\n\n\n429\nresidualamount_488A\n[train_credit_bureau_a_1_0, train_credit_burea...\nResidual amount of a closed contract.\n\n\n430\nresidualamount_856A\n[train_credit_bureau_a_1_0, train_credit_burea...\nResidual amount for the active contract.\n\n\n431\nresponsedate_1012D\n[train_static_cb_0]\nTax authority's response date.\n\n\n432\nresponsedate_4527233D\n[train_static_cb_0]\nTax authority's response date.\n\n\n433\nresponsedate_4917613D\n[train_static_cb_0]\nTax authority's response date.\n\n\n434\nrevolvingaccount_394A\n[train_applprev_1_0, train_applprev_1_1]\nRevolving account that was present in the appl...\n\n\n435\nriskassesment_302T\n[train_static_cb_0]\nEstimated probability that the client will def...\n\n\n436\nriskassesment_940T\n[train_static_cb_0]\nEstimate of client's creditworthiness.\n\n\n437\nrole_1084L\n[train_person_1]\nType of contact role.\n\n\n438\nrole_993L\n[train_person_1]\nPerson's role.\n\n\n439\nsafeguarantyflag_411L\n[train_person_1]\nFlag indicating if client is using a flexible ...\n\n\n440\nsecondquarter_766L\n[train_static_cb_0]\nNumber of results in second quarter.\n\n\n441\nsellerplacecnt_915L\n[train_static_0_0, train_static_0_1]\nNumber of sellerplaces where the same client's...\n\n\n442\nsellerplacescnt_216L\n[train_static_0_0, train_static_0_1]\nNumber of sellerplaces where the same client's...\n\n\n443\nsex_738L\n[train_person_1]\nGender of the client.\n\n\n444\nstatus_219L\n[train_applprev_1_0, train_applprev_1_1]\nPrevious application status.\n\n\n445\nsubjectrole_182M\n[train_credit_bureau_a_1_0, train_credit_burea...\nSubject role in active credit contract.\n\n\n446\nsubjectrole_326M\n[train_credit_bureau_b_1]\nSubject role in active credit contract.\n\n\n447\nsubjectrole_43M\n[train_credit_bureau_b_1]\nSubject role in closed credit contract.\n\n\n448\nsubjectrole_93M\n[train_credit_bureau_a_1_0, train_credit_burea...\nSubject role in closed credit contract.\n\n\n449\nsubjectroles_name_541M\n[train_credit_bureau_a_2_4, train_credit_burea...\nName of subject role in closed credit contract...\n\n\n450\nsubjectroles_name_838M\n[train_credit_bureau_a_2_4, train_credit_burea...\nName of subject role in active credit contract...\n\n\n451\nsumoutstandtotal_3546847A\n[train_static_0_0, train_static_0_1]\nSum of total outstanding amount.\n\n\n452\nsumoutstandtotalest_4493215A\n[train_static_0_0, train_static_0_1]\nSum of total outstanding amount.\n\n\n453\ntarget\n[train_base]\nNaN\n\n\n454\ntenor_203L\n[train_applprev_1_0, train_applprev_1_1]\nNumber of instalments in the previous applicat...\n\n\n455\nthirdquarter_1082L\n[train_static_cb_0]\nNumber of results in third quarter.\n\n\n456\ntotalamount_503A\n[train_credit_bureau_b_1]\nTotal amount of active secured credit for a cl...\n\n\n457\ntotalamount_6A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal amount of closed contracts.\n\n\n458\ntotalamount_881A\n[train_credit_bureau_b_1]\nTotal amount of secured credit from closed con...\n\n\n459\ntotalamount_996A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal amount of active contracts in the credit...\n\n\n460\ntotaldebt_9A\n[train_static_0_0, train_static_0_1]\nTotal amount of debt.\n\n\n461\ntotaldebtoverduevalue_178A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal amount of past due debt on active contra...\n\n\n462\ntotaldebtoverduevalue_718A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal overdue debt amount for closed credit co...\n\n\n463\ntotaloutstanddebtvalue_39A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal outstanding debt for active contracts in...\n\n\n464\ntotaloutstanddebtvalue_668A\n[train_credit_bureau_a_1_0, train_credit_burea...\nTotal outstanding debt for the closed contract...\n\n\n465\ntotalsettled_863A\n[train_static_0_0, train_static_0_1]\nSum of all payments made by the client.\n\n\n466\ntotinstallast1m_4525188A\n[train_static_0_0, train_static_0_1]\nTotal amount of monthly instalments paid in th...\n\n\n467\ntwobodfilling_608L\n[train_static_0_0, train_static_0_1]\nType of application process.\n\n\n468\ntype_25L\n[train_person_1]\nContact type of a person.\n\n\n469\ntypesuite_864L\n[train_static_0_0, train_static_0_1]\nPersons accompanying the client during the loa...\n\n\n470\nvalidfrom_1069D\n[train_static_0_0, train_static_0_1]\nDate since the client has an active campaign.\n\n\n\n\n\n\n\n\n\nlookuptable[lookuptable.File.apply(lambda x: 'train_credit_bureau_a_2_6' in x)]\n\n\n\n\n\n\n\n\n\nVariable\nFile\nDescription\n\n\n\n\n49\ncase_id\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n73\ncollater_typofvalofguarant_298M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral valuation type (active contract).\n\n\n74\ncollater_typofvalofguarant_407M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral valuation type (closed contract).\n\n\n75\ncollater_valueofguarantee_1124L\n[train_credit_bureau_a_2_4, train_credit_burea...\nValue of collateral for active contract.\n\n\n76\ncollater_valueofguarantee_876L\n[train_credit_bureau_a_2_4, train_credit_burea...\nValue of collateral for closed contract.\n\n\n77\ncollaterals_typeofguarante_359M\n[train_credit_bureau_a_2_4, train_credit_burea...\nType of collateral that was used as a guarante...\n\n\n78\ncollaterals_typeofguarante_669M\n[train_credit_bureau_a_2_4, train_credit_burea...\nCollateral type for the active contract.\n\n\n294\nnum_group1\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n295\nnum_group2\n[train_person_2, train_applprev_2, train_credi...\nNaN\n\n\n390\npmts_dpd_1073P\n[train_credit_bureau_a_2_4, train_credit_burea...\nDays past due of the payment for the active co...\n\n\n391\npmts_dpd_303P\n[train_credit_bureau_a_2_4, train_credit_burea...\nDays past due of the payment for terminated co...\n\n\n393\npmts_month_158T\n[train_credit_bureau_a_2_4, train_credit_burea...\nMonth of payment for a closed contract (num_gr...\n\n\n394\npmts_month_706T\n[train_credit_bureau_a_2_4, train_credit_burea...\nMonth of payment for active contract (num_grou...\n\n\n395\npmts_overdue_1140A\n[train_credit_bureau_a_2_4, train_credit_burea...\nOverdue payment for an active contract (num_gr...\n\n\n396\npmts_overdue_1152A\n[train_credit_bureau_a_2_4, train_credit_burea...\nOverdue payment for a closed contract (num_gro...\n\n\n398\npmts_year_1139T\n[train_credit_bureau_a_2_4, train_credit_burea...\nYear of payment for an active contract (num_gr...\n\n\n399\npmts_year_507T\n[train_credit_bureau_a_2_4, train_credit_burea...\nPayment year for a closed credit contract (num...\n\n\n449\nsubjectroles_name_541M\n[train_credit_bureau_a_2_4, train_credit_burea...\nName of subject role in closed credit contract...\n\n\n450\nsubjectroles_name_838M\n[train_credit_bureau_a_2_4, train_credit_burea...\nName of subject role in active credit contract...\n\n\n\n\n\n\n\n\n\n# use jupyter notebook\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\nOUT = widgets.Output()\n\ndef f(feature=widgets.Text()):\n    # OUT.clear_output()\n    # with OUT:\n    display(feature)\n    # return(feature)\n    if feature ==\"\":\n        return display(lookuptable);\n    else:\n        return display(lookuptable.query(f'Variable.str.contains(\"{feature}\")'));\nw=interactive(f)"
  },
  {
    "objectID": "2024/01-06 GUI Graphic Interface/Game of Life/01 Note.html",
    "href": "2024/01-06 GUI Graphic Interface/Game of Life/01 Note.html",
    "title": "Game of Life Demo Project",
    "section": "",
    "text": "Introduction\nFollowing Realpython’s game of life to learn python packaging, command line interface, and more, “Pythonic” code style! This blog review will take apart some of the component evaluate them seperately.\nThe result being something like this!\n\n\n\nTerminal View\n\n\n\n\nResources\n\nRealpython’s Tutorial:conway-game-of-life\nFull-Code:github.com/phume03/ConwaysGameOfLife\n\n\nfrom platform import python_version\n\npy_version = python_version()\nprint(\"I'm using python version: %s\"%(  py_version ) )\n\nI'm using python version: 3.11.2\n\n\n\n\nCreate a Grid\ncollections object are used to createa a grid. This collection is similar to a dictionary. It looks like a dict, act like a dict.\n\nimport collections\nneighbors = (\n            (-1, -1),  # Above left\n            (-1, 0),  # Above\n            (-1, 1),  # Above right\n            (0, -1),  # Left\n            (0, 1),  # Right\n            (1, -1),  # Below left\n            (1, 0),  # Below\n            (1, 1),  # Below right\n        )\nnum_neighbors = collections.defaultdict(int)\nnum_neighbors[(0, 0)] =1\nnum_neighbors[(0, 1)] = 2\nnum_neighbors\n\ndefaultdict(int, {(0, 0): 1, (0, 1): 2})\n\n\nIf I place string here it don’t have any safeguard at all.\n\nnum_neighbors[\"absard\"] = 'else'\nnum_neighbors\n\ndefaultdict(int, {(0, 0): 1, (0, 1): 2, 'absard': 'else'})\n\n\n\ndefault_dict = {}\ndefault_dict[(1,2)] = 1\n\nThe difference? &gt; The difference is that a defaultdict will “default” a value if that key has not been set yet. If you didn’t use a defaultdict you’d have to check to see if that key exists, and if it doesn’t, set it to what you want.\n\nThe lambda is defining a factory for the default value. That function gets called whenever it needs a default value. You could hypothetically have a more complicated default function.\n\n\nempty_dict = {}\nnum_neighbors['not exists']\ntry:\n    empty_dict['not exists']\nexcept KeyError as e:\n    print(e)\n\n'not exists'\n\n\n\n\nTry out some of the pattern\n\nfrom rplife import grid, patterns\n\nblinker = patterns.Pattern(\"Blinker\", {(2, 1), (2, 2), (2, 3)}) # create a Pattern data class\ngrid1 = grid.LifeGrid(blinker)\n\nprint(grid1.as_string([0, 0, 5, 5]))\nprint(grid1.evolve().as_string([0,0,5,5]))\n\n Blinker  \n · · · · ·\n · · · · ·\n · ♥ ♥ ♥ ·\n · · · · ·\n · · · · ·\n Blinker  \n · · · · ·\n · · ♥ · ·\n · · ♥ · ·\n · · ♥ · ·\n · · · · ·\n\n\nEverytime you reexecute this code a pattern change!\n\nfor itr in range(0,2):\n    print('%sth time:'%(itr + 1))\n    print(grid1.evolve().as_string([0,0,5,5]))\n\n1th time:\n Blinker  \n · · · · ·\n · · ♥ · ·\n · · ♥ · ·\n · · ♥ · ·\n · · · · ·\n2th time:\n Blinker  \n · · · · ·\n · · · · ·\n · ♥ ♥ ♥ ·\n · · · · ·\n · · · · ·\n\n\nTry something bigger!\n\nimport rplife.grid as grid\nuser_pattern = patterns.get_pattern(\"Glider Gun\")\ngrid2 = grid.LifeGrid(user_pattern)\nfor itr in range(0, 2):\n    print(grid2.evolve().as_string([0,0,20,20]))\n\n               Glider Gun               \n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · ♥ · · · · · · ·\n · · · · · · · · · · · ♥ ♥ · · · · · · ♥\n ♥ ♥ · · · · · · · · ♥ ♥ · · · · ♥ ♥ · ·\n ♥ ♥ · · · · · · · ♥ ♥ ♥ · · · · ♥ ♥ · ·\n · · · · · · · · · · ♥ ♥ · · · · ♥ ♥ · ·\n · · · · · · · · · · · ♥ ♥ · · · · · · ·\n · · · · · · · · · · · · ♥ · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n               Glider Gun               \n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · ♥ ♥ · · · · · · ·\n · · · · · · · · · · ♥ · ♥ · · · · · · ♥\n ♥ ♥ · · · · · · · ♥ · · · · · · ♥ ♥ ♥ ·\n ♥ ♥ · · · · · · · ♥ · · ♥ · · ♥ · · ♥ ·\n · · · · · · · · · ♥ · · · · · · ♥ ♥ · ·\n · · · · · · · · · · ♥ · ♥ · · · · · · ·\n · · · · · · · · · · · ♥ ♥ · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n · · · · · · · · · · · · · · · · · · · ·\n\n\n\n\nCommand Line Interface\nTo make all the function avaible you need to set-up two things:\n\nset up a “main.py” file\nimport argparse package to customise behaviour of this module;\n\nimport argparse\nfrom rplife import __version__, patterns, views\n\ndef get_command_line_args():\n    parser = argparse.ArgumentParser(\n        prog=\"rplife\",\n        description=\"Conway's Game of Life in your terminal\",\n    )\n    parser.add_argument(\n        \"--version\", action=\"version\", version=f\"%(prog)s v{__version__}\"\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--pattern\", # add an attribute to this line\n        choices=[pat.name for pat in patterns.get_all_patterns()],# this points python interface\n        default=\"Blinker\",\n        help=\"take a pattern for the Game of Life (default: %(default)s)\",\n    )\n    #... (more `add_argument` here)...\n    return parser.parse_args()\n“–pattern” will actuall became “pattern”. Once the parser.parse_args() object returned a result (call it arg), you can access this by doing “arg.pattern”.\nHowever this don’t seem to work by itself\n\nfrom rplife.cli import get_command_line_args\ntry:\n    get_command_line_args()\nexcept:\n    print(\"`get_command_line_args()` don't seem to work outside package\")\n\n`get_command_line_args()` will not work without inside package\n\n\nusage: rplife [-h] [--version] [-p {Blinker}] [-a] [-v {CursesView}]\n              [-g NUM_GENERATIONS]\nrplife: error: unrecognized arguments: --f=/Users/frankliang/Library/Jupyter/runtime/kernel-v3362f02a81df870f5337d5b5a338843c79cc3dd90.json"
  },
  {
    "objectID": "2024/01-01 Setup Delta Lake/02.html",
    "href": "2024/01-01 Setup Delta Lake/02.html",
    "title": "Setup Duckdb",
    "section": "",
    "text": "import duckdb\nduckdb.sql('SELECT 42').show()\n\n┌───────┐\n│  42   │\n│ int32 │\n├───────┤\n│    42 │\n└───────┘\n\n\n\n\ncon = duckdb.connect()\nduckdb.sql('SELECT * FROM \"ingest/bank-data.parquet\" limit 3')#.df() method return pandas dataframe\n\n┌─────────────────────┬──────────────────┬─────────┬──────────────┬───────────────┬─────────────┬───────────┐\n│        Date         │   Description    │  Type   │ Money In (£) │ Money Out (£) │ Balance (£) │ Category  │\n│      timestamp      │     varchar      │ varchar │    double    │    double     │   double    │  varchar  │\n├─────────────────────┼──────────────────┼─────────┼──────────────┼───────────────┼─────────────┼───────────┤\n│ 2023-10-02 00:00:00 │ STGCOACH/CTYLINK │ DEB     │          0.0 │           2.0 │      5770.0 │ Transport │\n│ 2023-10-02 00:00:00 │ PRET A MANGER    │ DEB     │          0.0 │           5.0 │      5764.0 │ Cafe      │\n│ 2023-10-02 00:00:00 │ STGCOACH/CTYLINK │ DEB     │          0.0 │           2.0 │      5762.0 │ Transport │\n└─────────────────────┴──────────────────┴─────────┴──────────────┴───────────────┴─────────────┴───────────┘\n\n\n\nimport duckdb\n\nwith duckdb.connect('file.db') as con:\n    con.sql('CREATE TABLE test(i INTEGER)')\n    con.sql('INSERT INTO test VALUES (42)')\n    con.table('test').show()\n\n\ncon=duckdb.connect('file.db')\ncon.table('test')\n\ncon.install_extension(\"spatial\")\ncon.install_extension(\"spatial\")"
  },
  {
    "objectID": "2024/01-01 Setup Delta Lake/02.html#set-up-duckdb",
    "href": "2024/01-01 Setup Delta Lake/02.html#set-up-duckdb",
    "title": "Setup Duckdb",
    "section": "",
    "text": "import duckdb\nduckdb.sql('SELECT 42').show()\n\n┌───────┐\n│  42   │\n│ int32 │\n├───────┤\n│    42 │\n└───────┘\n\n\n\n\ncon = duckdb.connect()\nduckdb.sql('SELECT * FROM \"ingest/bank-data.parquet\" limit 3')#.df() method return pandas dataframe\n\n┌─────────────────────┬──────────────────┬─────────┬──────────────┬───────────────┬─────────────┬───────────┐\n│        Date         │   Description    │  Type   │ Money In (£) │ Money Out (£) │ Balance (£) │ Category  │\n│      timestamp      │     varchar      │ varchar │    double    │    double     │   double    │  varchar  │\n├─────────────────────┼──────────────────┼─────────┼──────────────┼───────────────┼─────────────┼───────────┤\n│ 2023-10-02 00:00:00 │ STGCOACH/CTYLINK │ DEB     │          0.0 │           2.0 │      5770.0 │ Transport │\n│ 2023-10-02 00:00:00 │ PRET A MANGER    │ DEB     │          0.0 │           5.0 │      5764.0 │ Cafe      │\n│ 2023-10-02 00:00:00 │ STGCOACH/CTYLINK │ DEB     │          0.0 │           2.0 │      5762.0 │ Transport │\n└─────────────────────┴──────────────────┴─────────┴──────────────┴───────────────┴─────────────┴───────────┘\n\n\n\nimport duckdb\n\nwith duckdb.connect('file.db') as con:\n    con.sql('CREATE TABLE test(i INTEGER)')\n    con.sql('INSERT INTO test VALUES (42)')\n    con.table('test').show()\n\n\ncon=duckdb.connect('file.db')\ncon.table('test')\n\ncon.install_extension(\"spatial\")\ncon.install_extension(\"spatial\")"
  },
  {
    "objectID": "2024/01-01 Setup Delta Lake/02.html#details",
    "href": "2024/01-01 Setup Delta Lake/02.html#details",
    "title": "Setup Duckdb",
    "section": "Details",
    "text": "Details\nCreate a DuctDBPyConnection object. You can then use this connection with python with as live connection object. - If the database file does not exist, it will be created - the file extension may be .db, .duckdb, or anything else - The special value :memory: (the default) can be used to create an in-memory database. - Read-only mode is required if multiple Python processes want to access the same database file at the same time. - providing the special value :default: to connect.\n\nParametrised Query\n\nduckdb.execute(\"\"\"\n    SELECT\n        $my_param,\n        $other_param,\n        $also_param\n    \"\"\",\n    {\n        'my_param': 5,\n        'other_param': 'DuckDB',\n        'also_param': [42]\n    }\n).fetchall()\n\n[(5, 'DuckDB', [42])]\n\n\n\n# create a table\ncon.execute(\"CREATE TABLE items(item VARCHAR, value DECIMAL(10, 2), count INTEGER)\")\n# insert two items into the table\ncon.execute(\"INSERT INTO items VALUES ('jeans', 20.0, 1), ('hammer', 42.2, 2)\")\n\n# retrieve the items again\ncon.execute(\"SELECT * FROM items\")\nprint(con.fetchall())\n\n[('jeans', Decimal('20.00'), 1), ('hammer', Decimal('42.20'), 2)]\n\n\n\ncon.execute(\"SELECT * FROM items\")\nprint(con.fetchall())\n\n[('jeans', Decimal('20.00'), 1), ('hammer', Decimal('42.20'), 2)]\n\n\n\n\nReplacement Scan\n\nmy_statement = duckdb.sql('SELECT * FROM \"ingest/bank-data.parquet\"')\n\n\nduckdb.sql(\"select * from my_statement order by random() limit 3\")\n\n┌─────────────────────┬────────────────────┬─────────┬──────────────┬───────────────┬─────────────┬────────────────┐\n│        Date         │    Description     │  Type   │ Money In (£) │ Money Out (£) │ Balance (£) │    Category    │\n│      timestamp      │      varchar       │ varchar │    double    │    double     │   double    │    varchar     │\n├─────────────────────┼────────────────────┼─────────┼──────────────┼───────────────┼─────────────┼────────────────┤\n│ 2023-11-23 00:00:00 │ TESCO STORES 2487  │ DEB     │          0.0 │           3.0 │      3777.0 │ Food & Grocery │\n│ 2023-06-14 00:00:00 │ COSTA COFFEE 43011 │ DEB     │          0.0 │           6.0 │      3590.0 │ Cafe           │\n│ 2023-06-08 00:00:00 │ CO-OP GROUP 070527 │ DEB     │          0.0 │           4.0 │      4341.0 │ Food & Grocery │\n└─────────────────────┴────────────────────┴─────────┴──────────────┴───────────────┴─────────────┴────────────────┘\n\n\n\nduckdb.sql('''\n           select *, date_trunc('week', Date) as month\n           from my_statement\n           where Category != 'Rent & Essential'\n           ''')\\\n           .aggregate('month, round(sum(\"Money Out (£)\")/7, 2) as avg_per_day')\\\n           .aggregate('round(mean(avg_per_day), 2)')\n\n┌─────────────────────────────┐\n│ round(mean(avg_per_day), 2) │\n│           double            │\n├─────────────────────────────┤\n│                       67.77 │\n└─────────────────────────────┘\n\n\n\n\naggregate(): this is part of ibis frameworkd\n\nmy_statement.aggregate('Category as g, sum(\"Balance (£)\")')\n\n┌──────────────────┬────────────────────┐\n│        g         │ sum(\"Balance (£)\") │\n│     varchar      │       double       │\n├──────────────────┼────────────────────┤\n│ Rent & Essential │            28377.0 │\n│ Cafe             │           457548.0 │\n│ Cloth & Shopping │            19190.0 │\n│ Transport        │           644101.0 │\n│ Subscri & Apple  │           251391.0 │\n│ Other            │          1283628.0 │\n│ Food & Grocery   │           682436.0 │\n└──────────────────┴────────────────────┘\n\n\n\n\nJupyter Extensions\n\n%load_ext sql\nconn = duckdb.connect()\n%sql conn --alias duckdb\n\n\n%sql select * from my_statement order by random() limit 3\n\nRunning query in 'duckdb'\n\n\n\n\n\n\nDate\nDescription\nType\nMoney In (£)\nMoney Out (£)\nBalance (£)\nCategory\n\n\n\n\n2023-08-31 00:00:00\nFULL FIBRE LIMITED\nBGC\n2327.0\n0.0\n5153.0\nOther\n\n\n2023-08-31 00:00:00\nSTGCOACH/CTYLINK\nDEB\n0.0\n2.0\n5154.0\nTransport\n\n\n2023-10-02 00:00:00\nEXETER CITY COUNCI\nDD\n0.0\n29.0\n5666.0\nSubscri & Apple\n\n\n\n\n\n\n\n%%sql\nselect * \nfrom my_statement\nwhere \"Money Out (£)\" &gt; 100\n\nRunning query in 'duckdb'\n\n\n\n\n\n\nDate\nDescription\nType\nMoney In (£)\nMoney Out (£)\nBalance (£)\nCategory\n\n\n\n\n2023-10-16 00:00:00\nAllSaints Exeter\nDEB\n0.0\n130.0\n4753.0\nOther\n\n\n2023-10-26 00:00:00\nAPPLE\nDEB\n0.0\n1099.0\n3273.0\nSubscri & Apple\n\n\n2023-10-30 00:00:00\nSTEPHEN GIBSON\nFPO\n0.0\n350.0\n2808.0\nRent & Essential\n\n\n2023-11-06 00:00:00\nTIME FLIES\nDEB\n0.0\n175.0\n4485.0\nOther\n\n\n2023-11-20 00:00:00\nWH Smith Exeter\nDEB\n0.0\n102.0\n3892.0\nOther\n\n\n2023-11-27 00:00:00\nAMZNMktplace\nDEB\n0.0\n133.0\n3468.0\nOther\n\n\n2023-11-27 00:00:00\nSTEPHEN GIBSON\nFPO\n0.0\n350.0\n3065.0\nRent & Essential\n\n\n2023-11-29 00:00:00\nUKVISAFEE6JAA02219\nDEB\n0.0\n827.0\n2176.0\nRent & Essential\n\n\n2023-11-29 00:00:00\nIHS123411215PA01 2\nDEB\n0.0\n1248.0\n928.0\nOther\n\n\n2023-07-03 00:00:00\nTRAINLINE\nDEB\n0.0\n106.0\n4376.0\nTransport\n\n\n\n\nTruncated to displaylimit of 10.\n\n\n\n\n\nPloting without load data to in memory\n\n%sqlplot histogram --column \"Money Out (£)\" --table my_statement\n\n\n\n\n\n\n\n\n\n\nggplot api (in development)\n\n%%sql \nclean_statement &lt;&lt; select --save to a variable\n    Category,\n    \"Money Out (£)\" as Spending\n    from my_statement\n\nRunning query in 'duckdb'\n\n\n\ntype(clean_statement)\nimport pandas as pd\n\n\nclean_statement = clean_statement.DataFrame() if type(clean_statement) is not pd.DataFrame else clean_statement # the result must be converted to DataFrame to allow ploting in the next line\nfrom sql.ggplot import ggplot, aes, geom_boxplot, geom_histogram, facet_wrap\n(\n    ggplot(table=\"clean_statement\", mapping=aes(x=\"Spending\")) +\n    geom_histogram(bins=10,fill=\"Category\")\n)\n\n\n\n\n\n\n\n\n\n\nAbout Ibis Framework\n\n%pip install ibis\n\n11703.48s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\nCollecting ibis\n  Downloading ibis-3.3.0-py3-none-any.whl.metadata (1.3 kB)\nDownloading ibis-3.3.0-py3-none-any.whl (16 kB)\nDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg&lt;=0.29.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\nInstalling collected packages: ibis\nSuccessfully installed ibis-3.3.0\nNote: you may need to restart the kernel to use updated packages."
  },
  {
    "objectID": "2024/01-01 Deep Learning Py/01-13.html",
    "href": "2024/01-01 Deep Learning Py/01-13.html",
    "title": "First ML Model",
    "section": "",
    "text": "Top Note\nLoss Function, Optimizerm, Arch\n\nimport keras\ndef plot_history(self):\n    loss=self.history['loss']\n    acc=self.history['accuracy']\n    val_loss=self.history['val_loss']\n    val_acc=self.history['val_accuracy']\n\n    epochs=range(len(loss))\n\n    _, (ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n    ax1.plot(epochs, loss,'k.',label='Training Loss')\n    ax1.plot(epochs, val_loss,'k-', label='Validation Loss')\n    ax1.set_title('Loss Function')\n\n    ax2.plot(epochs,acc, 'k.')\n    ax2.plot(epochs,val_acc,'k-')\n    ax2.set_title('Accuracy')\nkeras.src.callbacks.History.plot=plot_history"
  },
  {
    "objectID": "2024/01-01 Deep Learning Py/01-13.html#reuters-classification",
    "href": "2024/01-01 Deep Learning Py/01-13.html#reuters-classification",
    "title": "First ML Model",
    "section": "Reuters Classification",
    "text": "Reuters Classification\nhere are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.\n\n# class utilities\n# you thought they'd have a plot method by now\nimport keras\ndef plot_history(self):\n    loss=self.history['loss']\n    acc=self.history['accuracy']\n    val_loss=self.history['val_loss']\n    val_acc=self.history['val_accuracy']\n\n    epochs=range(len(loss))\n\n    _, (ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n    ax1.plot(epochs, loss,'k.')\n    ax1.plot(epochs, val_loss,'k-')\n    ax1.set_title('Loss Function')\n\n    ax2.plot(epochs,acc, 'k.')\n    ax2.plot(epochs,val_acc,'k-')\n    ax2.set_title('Accuracy')\nkeras.src.callbacks.History.plot=plot_history\n\n\ndef to_one_hot(labels, dimension=46):\n    results = np.zeros((len(labels), dimension))\n    for i, label in enumerate(labels):\n        results[i, label] = 1.\n    return results\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results\n\n\nfrom keras.datasets import reuters\n\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n    num_words=10000)\n\n\nword_index = reuters.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n    train_data[0]])                                                          #1\ndecoded_newswire\n#1 - Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”\n         \n\n'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'\n\n\n\nimport numpy as np\n\n\n\nx_train = vectorize_sequences(train_data)            #1\nx_test = vectorize_sequences(test_data)              #2\n\n#1 - Vectorized training data\n#2 - Vectorized test data\n         \n\n\none_hot_train_labels = to_one_hot(train_labels)        #1\none_hot_test_labels = to_one_hot(test_labels)          #2\n\n#1 - Vectorized training labels\n#2 - Vectorized test labels\n\n\n\nfor para in train_data[:3]:\n    words=[reverse_word_index.get(i-3,'?') for i in range(len(para))]\n    sentence=' '.join(words)\n    print(textwrap.fill(sentence,70))\n\n? ? ? ? the of to in said and a mln 3 for vs dlrs it reuter 000 1 pct\non from is that its cts by at year be with 2 will was billion he u s\nnet has would an as 5 not loss 4 1986 company which but this shr last\nare lt have or 6 bank 7 were 8 had oil trade share one about 0 inc 9\nnew profit also market they two shares stock corp tonnes 10 up been\nrevs\n? ? ? ? the of to in said and a mln 3 for vs dlrs it reuter 000 1 pct\non from is that its cts by at year be with 2 will was billion he u s\nnet has would an as 5 not loss 4 1986 company which but this shr last\nare\n? ? ? ? the of to in said and a mln 3 for vs dlrs it reuter 000 1 pct\non from is that its cts by at year be with 2 will was billion he u s\nnet has would an as 5 not loss 4 1986 company which but this shr last\nare lt have or 6 bank 7 were 8 had oil trade share one about 0 inc 9\nnew profit also market they two shares stock corp tonnes 10 up been\nrevs prices sales 1987 per may after april march more price than\nquarter first other rate 15 group february 1985 government if exchange\nthree january co against dollar could we offer over told 20 agreement\nweek production note 30 their some foreign interest no japan tax 50\nexpected 12 total under all rose\n\n\n\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(46, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nx_val = x_train[:1000]\npartial_x_train = x_train[1000:]\n\ny_val = one_hot_train_labels[:1000]\npartial_y_train = one_hot_train_labels[1000:]\n\n# this coponent can be rerun often\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\n\n\nhistory.plot()\nplt.gcf().suptitle('A rmsporp optimiser on categorical_crossentropy')\n\nText(0.5, 0.98, 'A rmsporp optimiser on categorical_crossentropy')"
  },
  {
    "objectID": "2024/01-01 Deep Learning Py/01-13.html#the-boston-housing-price",
    "href": "2024/01-01 Deep Learning Py/01-13.html#the-boston-housing-price",
    "title": "First ML Model",
    "section": "The Boston Housing Price",
    "text": "The Boston Housing Price\n\nfrom keras.datasets import boston_housing\n\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n57026/57026 [==============================] - 0s 1us/step\n\n\n\nmean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data /= std\n\ntest_data -= mean\ntest_data /= std\n\n\nfrom keras import models\nfrom keras import layers\n\ndef build_model():\n    model = models.Sequential()                                  #1\n    model.add(layers.Dense(64, activation='relu',\n                           input_shape=(train_data.shape[1],)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model\n\n#1 - Because you’ll need to instantiate the same model multiple times, you use a function to construct it.  \n         \n\n\nimport numpy as np\n\nk = 4\nnum_val_samples = len(train_data) // k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print('processing fold #', i)\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]    #1\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n\n    partial_train_data = np.concatenate(                                     #2\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n\n    model = build_model()                                                    #3\n    model.fit(partial_train_data, partial_train_targets,                     #4\n              epochs=num_epochs, batch_size=1, verbose=0)\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)      #5\n    all_scores.append(val_mae)\n\n#1 - Prepares the validation data: data from partition #k\n#2 - Prepares the training data: data from all other partitions\n#3 - Builds the Keras model (already compiled)\n#4 - Trains the model (in silent mode, verbose = 0)\n#5 - Evaluates the model on the validation data\n         \n\nprocessing fold # 0\nprocessing fold # 1\nprocessing fold # 2\nprocessing fold # 3\n\n\n\nall_scores\nnp.mean(all_scores)\n\n2.540156304836273"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gold Fish Blog",
    "section": "",
    "text": "Previous\n  \n  \n    \n    Next"
  },
  {
    "objectID": "index.html#highlight-projects",
    "href": "index.html#highlight-projects",
    "title": "Gold Fish Blog",
    "section": "",
    "text": "Previous\n  \n  \n    \n    Next"
  },
  {
    "objectID": "index.html#all-projects-blog",
    "href": "index.html#all-projects-blog",
    "title": "Gold Fish Blog",
    "section": "All Projects & Blog",
    "text": "All Projects & Blog\nI wrote about random topic. Try out a new “{R} or {Python}” package.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nFrank Vs Iceberg\n\n\n\n\n\n\nApache-Iceberg\n\n\nspark\n\n\n\n\n\n\n\n\n\nMar 26, 2025\n\n\nF. L\n\n\n\n\n\n\n\n\n\n\n\n\n3D Buildings on Mars - Part II Explore ESRI\n\n\n\n\n\n\njavascripts\n\n\nESRI\n\n\n\n\n\n\n\n\n\nOct 24, 2024\n\n\nF. L\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding on Mars Part I - Interactive 3D City + THREE.js\n\n\n\n\n\n\njavascripts\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Force Network Using D3\n\n\n\n\n\n\nPython\n\n\nJavaScript\n\n\nVisualization\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nF. L\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Status Flow Experiment - Part 1\n\n\n\n\n\n\nStatistics\n\n\nExperiment\n\n\nr-package\n\n\n\n\n\n\n\n\n\nSep 8, 2024\n\n\nFanzhou Liang\n\n\n\n\n\n\n\n\n\n\n\n\nPerlin Noise Flow Field\n\n\n\n\n\n\nArt\n\n\nMath\n\n\n\n\n\n\n\n\n\nAug 28, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nQuize rewrite from Book: Rethink Statistics\n\n\n\n\n\n\nBooks\n\n\nNote\n\n\n\n\n\n\n\n\n\nAug 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGeo-Spatial Analysis on UFO sighting data\n\n\n\n\n\n\nExperiment\n\n\nGeo-Spatial\n\n\nData-Analysis\n\n\n\n\n\n\n\n\n\nJun 8, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Discriminant Analysis\n\n\n\n\n\n\nStatistics\n\n\nfrom-scratch\n\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Principle Component Analyis\n\n\n\n\n\n\nStatistics\n\n\nfrom-scratch\n\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nWork with AMES Data\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nML with Tidymodel\n\n\n\n\n\n\nMachine-Learning\n\n\nR\n\n\nBasics\n\n\n\n\n\n\n\n\n\nApr 21, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nEngineering Features for Time Series\n\n\n\n\n\n\nMachine-Learning\n\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\nYoung Frank\n\n\n\n\n\n\n\n\n\n\n\n\nUse R in a jupyter notebook?\n\n\n\n\n\n\nMachine-Learning\n\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\nYoung Frank\n\n\n\n\n\n\n\n\n\n\n\n\nData Normality\n\n\n\n\n\n\nMachine-Learning\n\n\nPython\n\n\nStatistic\n\n\nExperiment\n\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\nYoung Frank\n\n\n\n\n\n\n\n\n\n\n\n\nWhat data look like?\n\n\n\n\n\n\nKaggle\n\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nF.L and Team\n\n\n\n\n\n\n\n\n\n\n\n\nExplore Data in train_static\n\n\n\n\n\n\nKaggle\n\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nF.L and Team\n\n\n\n\n\n\n\n\n\n\n\n\nTry out vega altairs geo spatial visualisation\n\n\n\n\n\n\nPython\n\n\nVisualisation\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nGame of Life Demo Project\n\n\n\n\n\n\nBackend-Demo\n\n\nPython\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nVisualise Matrix Transformation as Paper Folding\n\n\n\n\n\n\nPython\n\n\nMachine-Learning\n\n\nDeep-Learning\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nF. L\n\n\n\n\n\n\n\n\n\n\n\n\nFirst ML Model\n\n\n\n\n\n\nPython\n\n\nMachine-Learning\n\n\nDeep-Learning\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nF. L\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning with Python Summary Page\n\n\n\n\n\n\nPython\n\n\nMachine-Learning\n\n\nDeep-Learning\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nF. L\n\n\n\n\n\n\n\n\n\n\n\n\nSetup Spark and and delta Lake\n\n\n\n\n\n\nbasic\n\n\nDelta\n\n\nDuckDB\n\n\nCode-Snippet\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nF.L\n\n\n\n\n\n\n\n\n\n\n\n\nSetup Duckdb\n\n\n\n\n\n\nbasic\n\n\nDelta\n\n\nDuckDB\n\n\nCode-Snippet\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nF.L\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#reference-quarto-blog-styling",
    "href": "index.html#reference-quarto-blog-styling",
    "title": "Gold Fish Blog",
    "section": "Reference (Quarto Blog Styling)",
    "text": "Reference (Quarto Blog Styling)\nThis blog is build using Quarto! Here are some useful links for customizing the blog.\n\nCreate a cover image for your blog\nQuarto expose custom SASS variables for you to use (with dark theme bonus) Quarto HTML Theme\nVery useful to use SASS to customize the blog: SASS Basics\nBooststrap GIT Example SASS: bootstrap/scss/_variables.scss\nModify scrolbar style: StackOverflow: How can I change the scrollbar style in Bootstrap?\nBootstrap Carousel\nFor coloring the blog (suggested by ChatGPT): Color Hunt\nLightbox for images: Quarto Lightbox"
  },
  {
    "objectID": "2024/08-25 Rethink Statistic/01.html",
    "href": "2024/08-25 Rethink Statistic/01.html",
    "title": "Quize rewrite from Book: Rethink Statistics",
    "section": "",
    "text": "My favorite Statistical Quize re-write.\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nFirst Birth Second Birth\nIntro\nIn high-school Biology class, I was taught the probability of mother given birth to boys to girls is always “fifty-fifty”. There’s a catch. In China, I know a few relatives who are living in rural who are very conservative, would go all the way to have a boys.\nWith statistics, we can test that.\n\nThese data indicate the gender (male=1, female=0) of officially reported first and second born children in 100 two-child families.\n\n\nbirth1 &lt;- c(1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0, 0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0, 1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0, 1,0,1,1,1,0,1,1,1,1) \nbirth2 &lt;- c(0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0, 1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1, 1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1, 0,0,0,1,1,1,0,0,0,0)\n\nWe are going to turn these boring 1 and 0 turns into insights.\nQuestions\nThese are the original questions:\n\nUse rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the dens command (part of the rethinking package) is probably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome?\n\nIn this example: Pior-belife is the birth of Boys can be any number out the 200,\n\n## Gird Approximate Baysian ==================================\n## create a even distributed point of p_grid\np_grid &lt;- seq(from=0 ,to=1 , length.out=1000 ) \n\n## Prior: although piror is just one we are creating for good practice\nprior &lt;- rep(1 , 1000) \n\n## Likelyhood:\nboys = sum(c(birth1, birth2))\nlikelihood &lt;- dbinom(boys , size=200 , prob=p_grid )\n\n## Posterior:\nposterior &lt;- likelihood * prior \nposterior &lt;- posterior / sum(posterior) \n\n## Plot =======================================================\np_to_posterior =tibble::tibble(p = p_grid, den = posterior)\nposterior_g = ggplot(p_to_posterior) + \n  geom_line(aes(x= p, y = den)) + \n  geom_vline(aes(xintercept = boys/200), linetype = \"dashed\", color=\"red\")\nposterior_g +\n  xlab(\"Probability of Boys\") + \n  ylab(\"~Probability of Probability...\") + \n  ggtitle(\n    \"Posterior\"\n    ,\"Posterior against &lt;span style='color:red'&gt;actual measurement&lt;/span&gt;\"\n  ) + \n  theme(\n    title = ggtext::element_markdown()\n  )\n\n\n\n\n\n\n\n\nThe posterior itself is description of probabilities. But resample can help explain this:\n\np_samp = sample(p_grid, prob = posterior, size = 1e4, replace=T)\n\n\n\nSample from Posterior\n## Sample Probabiles from Probabilies\nggplot() + \n  geom_histogram(\n    data=tibble(x= round(p_samp * 200))\n    ,aes(x=x)\n    ,binwidth=0.5\n    ,alpha=0.5\n    ,fill=\"lightgrey\"\n    ) +\n  geom_line(\n  ) + \n  geom_vline(aes(xintercept = boys), linetype = \"dashed\", color=\"red\") +\n  geom_vline(aes(xintercept = 100), linetype = \"solid\", linewidth=0.25,color=\"black\") +\n  xlab(\"Baby Boy per 200 birth\") + \n  ylab(\"Samples\") +\n  ggtitle(\n    \"Resampled from Posterior\",\n    glue::glue(\"Experiment has &lt;span style='color:red'&gt;{boys} baby boys born&lt;/span&gt; out of 200 babies\")\n  ) + \n  theme(\n    title = ggtext::element_markdown()\n  )\n\n\n\n\n\n\n\n\n\nOne would expect of 200 babies, there are 100 babies. But in fact the expected baby count is much higher.\nThis sample some-what represent experiment and represent the posterior function.\nOne way to discribe shape in number is HDIV (Highest Density Interval)\n\nbb_boys = sort(p_samp * 200)\n## vectorized function for finding highest interval\nfind_hdiv = function(v,p) {\n  v = sort(v)\n  n_win = ceiling(length(v) * p)\n  itr = 1:(length(v) - n_win + 1)\n  l_bonds = v[itr]\n  r_bonds = v[itr + n_win - 1]\n  i = which((r_bonds - l_bonds) == min(r_bonds - l_bonds))\n  return(unique(cbind(l_bonds[i], r_bonds[i])))\n}\nfind_hdiv(bb_boys, 0.95)\n\n        [,1]     [,2]\n[1,] 96.8969 123.9239\n\n\nWe can say of 200 birth, we expect probabily 95% of the time between 97 - 124 baby boys.\n\n3H5. The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?\n\nRichard McElreath suggest you can again, explore this topic by resample model posterior.\n\n## first born is girl\nb01 = birth2[birth1 ==0]\n\n## okay sample the same size\nboy_after_girl = rbinom(length(p_samp), size = length(b01), prob = p_samp)\n\nggplot() + \n  geom_histogram(\n    data=tibble(x=boy_after_girl)\n    , aes(x=x)\n    ,binwidth=0.5\n    ,alpha=0.5\n    ,fill=\"lightgrey\"\n    ) + \n    geom_vline(aes(xintercept = sum(b01)), linetype = \"dashed\", color=\"red\") +\n  xlab(\"baby brothers\") + \n  ylab(\"experiments\") +\n  ggtitle(\n    \"Resampled from the Same Posterior\",\n    glue::glue(\"After given birth to girls &lt;span style='color:red'&gt;{sum(b01)} baby boys born&lt;/span&gt; after&lt;br&gt; {length(b01)} girls, far exceed expectation of this model\")\n  ) + \n  theme(\n    title = ggtext::element_markdown()\n  )\n\n\n\n\n\n\n\n\nSo perhaps the gender are dependent after all?\nThe quiz has end here, but as a data analyst, I wonder how I would do this before I know any Bayesian statistics.\n\nbirth1 &lt;- c(1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0, 0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0, 1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0, 1,0,1,1,1,0,1,1,1,1) \nbirth2 &lt;- c(0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0, 1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1, 1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1, 0,0,0,1,1,1,0,0,0,0)\n\nbirth_data = tibble(\n  first_born = birth1,\n  second_born = birth2\n)\nbirth_data |&gt; \n  count(first_born, second_born)\n\n# A tibble: 4 × 3\n  first_born second_born     n\n       &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;\n1          0           0    10\n2          0           1    39\n3          1           0    30\n4          1           1    21\n\n\nIf any point here, I maybe feeling almost composed to create a density heat map from here if there were just 4 cross group from this simple binomial process.\nIf any fancier I would add in “binomial” process the frequency way and then add a bar plot.\n\n## calculate expected frequency\nbirth_exp = cross_join(\n  birth_data |&gt; \n  count(first_born) |&gt; \n  mutate(p = n/sum(n)) |&gt; \n  select(first_born, p),\n  birth_data |&gt; \n    count(second_born) |&gt; \n    mutate(p = n/sum(n)) |&gt; \n    select(second_born, p),\n  suffix=c(\"_first\",\"_second\")\n) |&gt; \n  mutate(p_exp = p_first * p_second) |&gt; \n  select(first_born,second_born, p_exp)\n\n\n## assume a 0.5 coin flip \nbirth_cals = birth_data |&gt; \n  count(first_born, second_born) |&gt; \n  mutate(p=n/sum(n) ) |&gt; \n  left_join(birth_exp,c(\"first_born\",\"second_born\")) |&gt; \n  mutate(d = dbinom(x = n, prob = p_exp, size=sum(n) ) ) |&gt; \n  mutate(across(c(first_born, second_born), ~ifelse(.x==0,\"Girl\",\"Boy\"))) |&gt; \n  mutate(name = paste(first_born, second_born)) |&gt; \n  select(-c(first_born, second_born)) |&gt; \n  relocate(name)\nbirth_cals\n\n# A tibble: 4 × 5\n  name          n     p p_exp       d\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Girl Girl    10  0.1  0.196 0.00430\n2 Girl Boy     39  0.39 0.294 0.00994\n3 Boy Girl     30  0.3  0.204 0.00662\n4 Boy Boy      21  0.21 0.306 0.00950\n\n\nNow I would struggle to interpretative “d” value. The reason they are small is because they are essentially density. Although in theory, the d is the red line\nI would have also be too stubborn and have a whole argument about data management that the values first born and second born should be in different column not the same column because they are essentially different dimensions.\nthe improved Bayesian version\nThere are four groups we will be observing boy-boy,boy-girl,girl-boy,girl-girl (the orders are alphabetical)\nWe will be creating Bayesian “forks of garden” for all these category. Here are a few points to mess with my brain:\n\nI can use first born as a “prior” condition. But then there are no more Bayesian inferences, just applying classic Bayesian formula, Dr. McElreath will scold me.\nGeneric Grouped Analytic: I can use binomial model as likelihood, that also means I will choose probability of these four group independently;\n\nThis means probability of observed data are logically observed over all.\nPredictably, if we re-plot these all these, the red-line will always be on center of the ridge, because this version of Bayesian model love optimizing mode.\nBut applying this model you will probably see four hills, but the peak of the hill are positioned differently, so you maybe able to say, hi, look, “”\n\nAlternatively, we construct a global Bayesian model and then compare them against four local group.\n\nStarting with the last bullet point on the list\n\nbirth_cals |&gt; \n  filter(stringr::str_detect(name, \"Boy$\")) |&gt; \n  ggplot() + \n  facet_grid(rows=vars(name)) +\n  \n  geom_histogram(\n    data=tibble(x=boy_after_girl)\n    , aes(x=x)\n    ,binwidth=0.5\n    ,alpha=0.5\n    ,fill=\"lightgrey\"\n    ) +\n  geom_vline(aes(xintercept = n),linetype = \"dashed\", color=\"red\") + \n  xlab(\"baby brothers\") + \n  ylab(\"experiments\") +\n  ggtitle(\n    \"The Global Model\",\n    glue::glue(\"For example in row(3), after given birth to girls &lt;span style='color:red'&gt;{sum(b01)} baby boys born&lt;/span&gt; after&lt;br&gt; {length(b01)} girls, far exceed expectation of this model\")\n  ) + \n  theme(\n    title = ggtext::element_markdown()\n  )"
  },
  {
    "objectID": "2024/08-07 Noise Art and Distribution/01.html",
    "href": "2024/08-07 Noise Art and Distribution/01.html",
    "title": "Perlin Noise Flow Field",
    "section": "",
    "text": "flat_matrix |&gt; \n  mutate(v=round(value, 2)) |&gt; \n  ggplot(aes(x,y)) +\n  geom_tile(aes(fill=v)) + \n  scale_fill_viridis_b(option = \"G\") + \n  coord_equal()\n\n\n\n\n\n\n\n\n\nStatistic Characters of Perlin Noise\n\nvs = sort(flat_matrix$value)\nm = mean(vs)\nsd = sd(vs)\nrd = rnorm(length(vs), m,sd)\n\ntibble(\n  rank = 1:length(vs),\n  perline_noise = vs,\n  normal = rd |&gt; sort()\n) |&gt; \n  ggplot() + \n  geom_density(aes(x = perline_noise),color=\"blue\",linewidth=1) + \n  geom_density(aes(x = normal),linetype=\"dashed\") + \n  ggtitle(\n    glue::glue(\"&lt;span style='color: blue'&gt;&lt;b&gt;Perline Noise&lt;/b&gt;&lt;/span&gt; compares to &lt;br&gt;\"\n               ,\"a &lt;b&gt;Normal Distribution&lt;/b&gt;\")\n  ) + \n  theme(\n    title = ggtext::element_markdown()\n  )\n\n\n\n\n\n\n\n\n\n\nVector Field\n\nrec = c(xmin = 50, xmax = 100, ymin = 50, ymax = 100)\n\nflat_matrix |&gt; \n  mutate(v=round(value, 2)) |&gt; \n  ggplot(aes(x,y)) +\n  geom_tile(aes(fill=v)) + \n  scale_fill_viridis_b(option = \"G\") + \n  coord_equal() + \n  annotate(\"rect\", xmin=50,xmax=100,ymin=50,ymax=100,fill=NA,color=\"green\")\n\n\n\n\n\n\n\n\nExperiment with a small square of data\n\n## select a small sample\nflat_matrix |&gt; \n  filter(x |&gt; between(rec[\"xmin\"], rec[\"xmax\"]) & y |&gt; between(rec[\"ymin\"], rec[\"ymax\"])) |&gt; \n  mutate(v=round(value, 2)) |&gt; \n  ggplot(aes(x,y)) +\n  geom_tile(aes(fill=v)) +\n  scale_fill_viridis_b(option = \"G\") +\n  coord_equal()\n\n\n\n\n\n\n\n\n\nres=5\ninit_angle = 0.01 * 2 * pi #10% of a circle which is 36°\n\nflat_matrix |&gt; \n  ## sample a few point\n  filter(x |&gt; between(rec[\"xmin\"], rec[\"xmax\"]) & y |&gt; between(rec[\"ymin\"], rec[\"ymax\"])) |&gt; \n  filter(x %%res ==0 & y %% res == 0) |&gt; \n  \n  ## make a vector points\n  mutate(\n    stv = diff(c(min(value),value)) / diff(range(value)),\n    angle = stv * 2 * pi,\n    d = res * 0.66,\n    x_p = sin(angle + init_angle) * d + x,\n    y_p = cos(angle + init_angle) * d + y\n  ) |&gt; \n  \n  mutate(v=round(value, 2)) |&gt; \n  ggplot(aes(x,y)) +\n  geom_segment(aes(x=x,xend=x_p,y=y,yend=y_p),arrow=arrow(length = unit(0.1,\"inches\")),alpha=0.7) +\n  geom_point(aes(color=v),alpha=0.33) +\n  geom_point(aes(x=x_p, y_p,color=v),alpha=1) +\n  scale_color_viridis_b(option = \"G\") +\n  coord_equal()\n\n\n\n\n\n\n\n\n\n\nCreate a few usefully function in this code chunk\nmake_rect = function(x=200, y=200, d = 400) {\n  return( c(xmin = x - d/2, xmax = x + d/2, ymin = y - d/2, ymax = y + d/2))\n}\nannotate_rect = function(rec,...) {\n  return(list(annotate(\"rect\"\n                       , xmin=rec['xmin']\n                       , xmax=rec['xmax']\n                       , ymin=rec['ymin']\n                       , ymax=rec['ymax']\n                       , fill = NA\n                       , ...)))\n}\ncrop = function(data, rec = make_rect()) {\n  data |&gt; \n  ## sample a few point\n    filter(x |&gt; between(rec[\"xmin\"], rec[\"xmax\"]) & y |&gt; between(rec[\"ymin\"], rec[\"ymax\"])) \n}\nde_res = function(data, res = 15) {\n  data |&gt; \n    filter(x %%res ==0 & y %% res == 0)\n}\nval_to_angle = function(data,value,init_angle = 0.01 * 2 * pi) {\n  data |&gt; \n    mutate(\n      angle = {{value}} * 2 * pi + init_angle,\n    )\n}\n\n\n\nres = 15\n\nangle_matrix = flat_matrix |&gt; \n  ## sample a few point\n  val_to_angle(value)\n\ng_grid = angle_matrix |&gt; \n  de_res(res=res) |&gt; \n  crop() |&gt; \n  ## make a vector points\n  mutate(\n    d = res * 0.6,\n    x_p = cos(angle ) * d + x,\n    y_p = sin(angle ) * d + y\n  ) |&gt; \n  mutate(v=round(value, 2)) |&gt; \n  ggplot(aes(x,y)) +\n  geom_segment(aes(x=x,xend=x_p,y=y,yend=y_p),arrow=arrow(length = unit(0.02,\"inches\")),alpha=0.7) +\n  scale_color_viridis_b(option = \"G\") +\n  coord_equal()\ng_grid\n\n\n\n\n\n\n\n\nNice! Next try release particle to the data grid:\n\nthe particles needs to layout on a grid\nthe angle send particle to the next closest grid\nbut if were were unlucky the particle will git the center of a grid\n\nThis is very simple in this grid system because we simply just round to neast point?\n\n\nCode\nsample_rec = make_rect(100,300,100)\n\nt_angle_matrix = angle_matrix |&gt; \n  # de_res(res=res) |&gt; \n  crop()\n\n## Set a Group Parameter\nseed_particle = t_angle_matrix |&gt;\n  crop(sample_rec) |&gt; \n  slice_sample(n=16) |&gt; \n  mutate(.group = row_number(), id=0)\n\nparticles_path = seed_particle\nparticle = seed_particle\n\n### A Simple Find My Particle graph \nfor (i in 1:500) {\n  next_coord = particle |&gt; \n    mutate(\n        x = x + round(cos(angle))\n      , y = round(y + sin(angle))\n      , id = i\n      ) |&gt; \n    select(id,x,y,.group)\n  particle = t_angle_matrix |&gt; \n    inner_join(next_coord,c(\"x\",\"y\"))\n  if(nrow(particle) == 0) {\n    break\n  }\n  particles_path = particles_path |&gt; bind_rows(particle)\n}\n\n# particle_smooth = smooth.spline(particles_path)\ng_grid + \n  geom_path(\n    data = particles_path |&gt; \n      mutate(xx=x,yy=y),\n     # mutate(xx=particle_smooth$x, tt=particle_smooth$y),\n    aes(x = xx, y = yy,group=.group),color=\"#4c16fb\",size=0.7,\n    # stat=\"smooth\"\n  ) + \n  # facet_wrap(~.group) +\n  annotate_rect(sample_rec,color=\"#2ecc71\",alpha=0.5,linewidth=0.8) +\n  ggtitle(\n    \"\",\n    subtitle=glue::glue(\n      \"Random &lt;span style='color:#4c16fb'&gt;&lt;b&gt;partical path&lt;/b&gt;&lt;/span&gt; samped from &lt;br&gt;the \",\n      \"&lt;span style ='color:#2ecc71' &gt;&lt;b&gt;green rectangle&lt;/b&gt;&lt;/span&gt;\"\n      )\n  ) +\n  theme(\n    title=ggtext::element_markdown()\n  )\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nSomething poetic about this system. Imagine the dotes are people, vectors are social tendencies, influence of our parent’s, influence of the school teacher.\nwe are all born on this green square but different people scattered up to different places yet fate draw them to one place.\n[tbc]"
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/01.html",
    "href": "2024/04-20 ML with Tidymodel/01.html",
    "title": "ML with Tidymodel",
    "section": "",
    "text": "This notebook summaries key point from Hadley Wickham’s Tidy Model with R. The book only covers basic usage of tidy-model and some other dimension reduction techniques.\nLink to the Book: https://www.tmwr.org/\n\n\n\n\nCode\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(tidymodels)\ndata(\"ames\")\n\n## refer to tmap: https://r-tmap.github.io/tmap-book/visual-variables.html\n## for osm data: https://cran.r-project.org/web/packages/osmdata/vignettes/osmdata.html\n## for query streets: https://wiki.openstreetmap.org/wiki/Key%3ahighway\n\names_sf = sf::st_as_sf(ames,coords = c(\"Longitude\",\"Latitude\"), crs=4326)\names_bbox = sf::st_bbox(ames_sf)\nosm_streets = opq(bbox = ames_bbox) |&gt; \n  add_osm_feature(key=\"highway\",value = c(\n                                          'secondary'\n                                          ,'primary'\n                                          ,'tertiary'\n                                          ,'unclassified'\n                                          ,'residential')) |&gt; \n  # add_osm_feature(key=\"highway\",value = 'motorway') |&gt; \n  osmdata_sf()\n\n## view a intersection\nstreets_sf = sf::st_intersection(sf::st_as_sfc(ames_bbox), osm_streets$osm_lines)\n\ntm_shape(streets_sf) + \n  tm_lines(col='grey') +\ntm_shape(ames_sf) + \n  tm_dots( shape = \"Lot_Shape\"\n          ,col=\"Neighborhood\"\n          ,style = \"cont\"\n          ,size=0.05\n          ,border.col=NA\n          ,border.lwd=0.01) + \n  tm_layout(legend.show=FALSE)\n\n\nWarning: tm_scale_continuous is supposed to be applied to numerical data\n\n\n\n\n\n\n\n\n\names familar with this data may come handy compare different model output later.\n\n\n\n\nCode\nlibrary(tidymodels)\n# \nggplot2::theme_set(theme_minimal())\ntidymodels_prefer()\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")\n\n\n\n\n\n\n\n\n\nFirst thing they want to tell you is the data is not normal so require you to normalise somehow.\n\n\n\nCode\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n\n\n\names &lt;- ames |&gt; mutate(Sale_Price = log10(Sale_Price))\n\n\n# ames |&gt; \n#   head(1) |&gt; \n#   glimpse()\n\n\n\n\nFollowing code create a linear model. Prediction uses these variables:\n\n## preview columns in ames data\names |&gt; \n  select(Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type, Latitude, Longitude) |&gt; \n  slice_sample(n=1) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 6\n$ Neighborhood &lt;fct&gt; Mitchell\n$ Gr_Liv_Area  &lt;int&gt; 974\n$ Year_Built   &lt;int&gt; 1991\n$ Bldg_Type    &lt;fct&gt; OneFam\n$ Latitude     &lt;dbl&gt; 41.98651\n$ Longitude    &lt;dbl&gt; -93.60664\n\n\nTher are their transformation:\n\nNeighborhood: convert low frequency one to “other”, then make dummy varible\nGr_Liv_Area: log10 treatment\nYear_Built: year\nBldg_Type: convert building type into dummy varible\nLatitude: spine function treatment\nLongitude: spine function treatment\n\nlibrary(tidymodels)\ndata(ames)\n\n## Normalise Prediction\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\n## Split Data Sets\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\n## Recipy for Preprocessing Data, Build receipy object\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n  \n## Linear Model\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n## Finaly Evaluate Lazy Object\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(ames_rec)\n\n## Fit a Model\nlm_fit &lt;- fit(lm_wflow, ames_train)"
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/01.html#introduction-of-data",
    "href": "2024/04-20 ML with Tidymodel/01.html#introduction-of-data",
    "title": "ML with Tidymodel",
    "section": "",
    "text": "Code\nlibrary(tmap)\nlibrary(osmdata)\nlibrary(tidymodels)\ndata(\"ames\")\n\n## refer to tmap: https://r-tmap.github.io/tmap-book/visual-variables.html\n## for osm data: https://cran.r-project.org/web/packages/osmdata/vignettes/osmdata.html\n## for query streets: https://wiki.openstreetmap.org/wiki/Key%3ahighway\n\names_sf = sf::st_as_sf(ames,coords = c(\"Longitude\",\"Latitude\"), crs=4326)\names_bbox = sf::st_bbox(ames_sf)\nosm_streets = opq(bbox = ames_bbox) |&gt; \n  add_osm_feature(key=\"highway\",value = c(\n                                          'secondary'\n                                          ,'primary'\n                                          ,'tertiary'\n                                          ,'unclassified'\n                                          ,'residential')) |&gt; \n  # add_osm_feature(key=\"highway\",value = 'motorway') |&gt; \n  osmdata_sf()\n\n## view a intersection\nstreets_sf = sf::st_intersection(sf::st_as_sfc(ames_bbox), osm_streets$osm_lines)\n\ntm_shape(streets_sf) + \n  tm_lines(col='grey') +\ntm_shape(ames_sf) + \n  tm_dots( shape = \"Lot_Shape\"\n          ,col=\"Neighborhood\"\n          ,style = \"cont\"\n          ,size=0.05\n          ,border.col=NA\n          ,border.lwd=0.01) + \n  tm_layout(legend.show=FALSE)\n\n\nWarning: tm_scale_continuous is supposed to be applied to numerical data\n\n\n\n\n\n\n\n\n\names familar with this data may come handy compare different model output later.\n\n\n\n\nCode\nlibrary(tidymodels)\n# \nggplot2::theme_set(theme_minimal())\ntidymodels_prefer()\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")\n\n\n\n\n\n\n\n\n\nFirst thing they want to tell you is the data is not normal so require you to normalise somehow.\n\n\n\nCode\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\") +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n\n\n\names &lt;- ames |&gt; mutate(Sale_Price = log10(Sale_Price))\n\n\n# ames |&gt; \n#   head(1) |&gt; \n#   glimpse()"
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/01.html#spoil-alert",
    "href": "2024/04-20 ML with Tidymodel/01.html#spoil-alert",
    "title": "ML with Tidymodel",
    "section": "",
    "text": "Following code create a linear model. Prediction uses these variables:\n\n## preview columns in ames data\names |&gt; \n  select(Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type, Latitude, Longitude) |&gt; \n  slice_sample(n=1) |&gt; \n  glimpse()\n\nRows: 1\nColumns: 6\n$ Neighborhood &lt;fct&gt; Mitchell\n$ Gr_Liv_Area  &lt;int&gt; 974\n$ Year_Built   &lt;int&gt; 1991\n$ Bldg_Type    &lt;fct&gt; OneFam\n$ Latitude     &lt;dbl&gt; 41.98651\n$ Longitude    &lt;dbl&gt; -93.60664\n\n\nTher are their transformation:\n\nNeighborhood: convert low frequency one to “other”, then make dummy varible\nGr_Liv_Area: log10 treatment\nYear_Built: year\nBldg_Type: convert building type into dummy varible\nLatitude: spine function treatment\nLongitude: spine function treatment\n\nlibrary(tidymodels)\ndata(ames)\n\n## Normalise Prediction\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\n## Split Data Sets\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\n## Recipy for Preprocessing Data, Build receipy object\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n  \n## Linear Model\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n## Finaly Evaluate Lazy Object\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(ames_rec)\n\n## Fit a Model\nlm_fit &lt;- fit(lm_wflow, ames_train)"
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/01.html#splittingfeature-selectioncreate-a-data-budget",
    "href": "2024/04-20 ML with Tidymodel/01.html#splittingfeature-selectioncreate-a-data-budget",
    "title": "ML with Tidymodel",
    "section": "Splitting/Feature Selection/Create a “Data Budget”",
    "text": "Splitting/Feature Selection/Create a “Data Budget”\n\nlibrary(tidymodels)\n\n\nSimple 80-20 split\nThe basics is the same, split train test. For this purpose you are splitting the data by 80-20.\n\names_split &lt;- rsample::initial_split(ames, prop = 0.80)\names_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2344/586/2930&gt;\n\n\nRegards to spliting portion here is the advice from the Book:\n\nA test set should be avoided only when the data are pathologically small.\n\n\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\ndim(ames_train)\n\n[1] 2344   74\n\n\n\n\nValidation Split 60-20-20\n\nset.seed(52)\n# To put 60% into training, 20% in validation, and 20% in testing:\names_val_split &lt;- rsample::initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;1758/586/586/2930&gt;\n\n\n\names_train &lt;- training(ames_val_split)\names_test &lt;- testing(ames_val_split)\names_val &lt;- validation(ames_val_split)\n\n\n\nConcepts\n\nindependent experimental unit: (knowing database basic this is just matter of object uid versus alternate uid) for example, measuring one patient\nmulti-level-data/multiple rows per experimental unit:\n\n\nData splitting should occur at the independent experimental unit level of the data!!!\n\n\nSimple resampling across rows would lead to some data within an experimental unit being in the training set and others in the test set.\n\n\n\nPracrtical Implication\n\nthe book admit the practice of train and split at first for a validation of the model but follow up using all the data point possible for a better estimation of data."
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/01.html#fitting-model-with-parsnip",
    "href": "2024/04-20 ML with Tidymodel/01.html#fitting-model-with-parsnip",
    "title": "ML with Tidymodel",
    "section": "Fitting Model with Parsnip",
    "text": "Fitting Model with Parsnip\n\nlinear_reg\nrand_forest\n\n\nLinear Regression Family\n\nlm\nglmnet: fits generalised linear and model via penalized maximum likelihood.\nstan\n\n\n# switch computational backend for different model\nlinear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n#  regularized regression is the glmnet model \nlinear_reg(penalty=1) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    family = \"gaussian\")\n\n# To estimate with regularization, the second case, a Bayesian model can be fit using the rstanarm package:\nlinear_reg() |&gt; \n  set_engine(\"stan\") |&gt; \n  translate()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: stan \n\nModel fit template:\nrstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), \n    weights = missing_arg(), family = stats::gaussian, refresh = 0)\n\n\n\nlm_model = linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  translate()\n\nlm_model |&gt; \n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -313.623       -2.074        2.965  \n\n\n\nlm_xy_fit &lt;- \n  lm_model %&gt;% \n  fit_xy(\n    x = ames_train %&gt;% select(Longitude, Latitude),\n    y = ames_train %&gt;% pull(Sale_Price)\n  )\n\nlm_xy_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)    Longitude     Latitude  \n   -313.623       -2.074        2.965  \n\n\n\n\nTree Model\n\nrand_forest(trees = 1000, min_n = 5) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\") %&gt;% \n  translate()\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 5\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    num.trees = 1000, min.node.size = min_rows(~5, x), num.threads = 1, \n    verbose = FALSE, seed = sample.int(10^5, 1))"
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/01.html#capture-model-results",
    "href": "2024/04-20 ML with Tidymodel/01.html#capture-model-results",
    "title": "ML with Tidymodel",
    "section": "Capture Model Results",
    "text": "Capture Model Results\n\nRaw original way (useful to check og documentation)\n\nlm_form_fit &lt;- \n  lm_model %&gt;% \n  # Recall that Sale_Price has been pre-logged\n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nlm_form_fit %&gt;% extract_fit_engine() %&gt;% vcov()\n\n            (Intercept)    Longitude     Latitude\n(Intercept)  273.852441  2.052444651 -1.942540743\nLongitude      2.052445  0.021122353 -0.001771692\nLatitude      -1.942541 -0.001771692  0.042265807\n\n\n\nmodel_res &lt;- \n  lm_form_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  summary()\n\n# The model coefficient table is accessible via the `coef` method.\nparam_est &lt;- coef(model_res)\nclass(param_est)\n\n[1] \"matrix\" \"array\" \n\n\n\nparam_est\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -313.622655 16.5484876 -18.95174 5.089063e-73\nLongitude     -2.073783  0.1453353 -14.26896 8.697331e-44\nLatitude       2.965370  0.2055865  14.42395 1.177304e-44\n\n\n\n\nThe Tidy ecosystem for model result\nWhat’s good about tidy is so you can reuse model.\n\ntidy(lm_form_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -314.      16.5       -19.0 5.09e-73\n2 Longitude      -2.07     0.145     -14.3 8.70e-44\n3 Latitude        2.97     0.206      14.4 1.18e-44"
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/01.html#evaluate-test-set-use-last_fit-method",
    "href": "2024/04-20 ML with Tidymodel/01.html#evaluate-test-set-use-last_fit-method",
    "title": "ML with Tidymodel",
    "section": "Evaluate Test Set use `last_fit()` method",
    "text": "Evaluate Test Set use `last_fit()` method\n\nfinal_lm_res &lt;- last_fit(lm_wflow, ames_split)\nfinal_lm_res\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [2342/588]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\n\n…the modeling process encompasses more than just estimating the parameters of an algorithm that connects predictors to an outcome. This process also includes pre-processing steps and operations taken after a model is fit. We introduced a concept called a model workflow that can capture the important components of the modeling process. Multiple workflows can also be created inside of a workflow set. The last_fit() function is convenient for fitting a final model to the training set and evaluating with the test set.\nFor the Ames data, the related code that we’ll see used again is:\n\n\nlibrary(tidymodels)\ndata(ames)\n\n## normalise y\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\n## split data\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\n## linear models\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n## validating result\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\n\nlm_fit &lt;- fit(lm_wflow, ames_train)"
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/01.html#yardstick-basic-usage",
    "href": "2024/04-20 ML with Tidymodel/01.html#yardstick-basic-usage",
    "title": "ML with Tidymodel",
    "section": "Yardstick Basic Usage",
    "text": "Yardstick Basic Usage\nYardstick is tool that produce performance matrix with consistent interface.\n## Create a prediction frame\names_test_res &lt;- predict(lm_fit, new_data = ames_test %&gt;% select(-Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 1\n#&gt;   .pred\n#&gt;   &lt;dbl&gt;\n#&gt; 1  5.07\n#&gt; 2  5.31\n\n\n## Bind prediction with Actual value\names_test_res &lt;- bind_cols(ames_test_res, ames_test %&gt;% select(Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 2\n#&gt;   .pred Sale_Price\n#&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1  5.07       5.02\n#&gt; 2  5.31       5.39\n\n## YARDSTICK!! given a dataframe just do these two\nrmse(ames_test_res, truth = Sale_Price, estimate = .pred)\n\n\n## compare multiple matrix\names_metrics &lt;- metric_set(rmse, rsq, mae)\names_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)"
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/02.html",
    "href": "2024/04-20 ML with Tidymodel/02.html",
    "title": "Work with AMES Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\ntheme_set(theme_minimal())\n\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))"
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/02.html#study-features",
    "href": "2024/04-20 ML with Tidymodel/02.html#study-features",
    "title": "Work with AMES Data",
    "section": "Study Features",
    "text": "Study Features\nMy first intuition is create a factor analysis so I can try understand which dimensions are related which are not.\nExplore data using pca for analysing top performing featuresfviz_pca_ind\n\names.numb = ames |&gt; \n  select(where(is.numeric)) |&gt; \n  select(-starts_with(\"Year_\"), -Longitude, - Latitude, -Sale_Price)\n\n# ames.numb |&gt; glimpse()\n\names.fct = ames.numb |&gt; \n  prcomp(scale=T) # the function\n\n# # fviz_eig from factoextra package\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\names.fct |&gt; fviz_eig() #Extract and visualize the eigenvalues/variances of dimensions\n\n\n\n\n\n\n\n\nFrom this plot the elbow seems to be two or three. This visualization seems to be better at explaining factors (I will continue my fascination with PCA later).\n\nfviz_pca_var(\n  ames.fct\n  ,col.var = \"contrib\"\n  ,gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\n  ,repel = TRUE     # Avoid text overlapping\n  )\n\n\n\n\n\n\n\n\nActually you can use PCA with tidy-model (because you can also train and predict)\n\names_tts = ames |&gt; initial_split(prop = 0.8, strata = Sale_Price)\names_train = ames_tts |&gt; training()\names_test = ames_tts |&gt; testing()\n\n\npca_trans = ames_train |&gt; \n  select(where(is.numeric)) |&gt; \n  recipe(~.) |&gt; \n  step_normalize(all_numeric()) |&gt; \n  step_pca(all_numeric(), num_comp = 2)# still \n\nGetting data out, according to receipt example, instead of train or predict you do so by prep and bake.\nThis results in scoring of the two latent feature for each individual row.\n\n#\npca_estimate = pca_trans |&gt; prep()\npca_data &lt;- bake(pca_estimate,ames_train)\npca_data\n\n# A tibble: 2,342 × 2\n     PC1    PC2\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1  2.52  0.852\n 2  2.11  2.60 \n 3  3.33  0.465\n 4  2.69 -0.510\n 5  2.42 -1.40 \n 6  2.72 -1.75 \n 7  1.10  1.38 \n 8  3.28  0.290\n 9  2.02  0.177\n10  3.57 -0.204\n# ℹ 2,332 more rows\n\n\nExtract coeficient feature for against all score?\n\n## visualise coeficiency results\nrequire(tidytext)\n\nLoading required package: tidytext\n\ntidy(pca_estimate,number=2,type=\"coef\") |&gt; \n  filter(component %in% c(\"PC1\",\"PC2\")) |&gt; \n  ggplot(aes(x = value, y=terms, fill=component)) +\n  geom_col() + \n  scale_fill_brewer(palette = \"Set1\") + \n  ggtitle(\"Extracting Feature from Tidyverse Principle Component\")\n\n\n\n\n\n\n\n\nBy calculating corelation matrix is indeed, you replicate the same value from the latent feature.\n\nX &lt;- ames_train |&gt; select(where(is.numeric), -Sale_Price)\n\ncor(X, pca_data ) |&gt; \n  data.frame() |&gt; \n  rownames_to_column() |&gt; \n  pivot_longer(c(PC1, PC2)) |&gt; \n  ggplot(aes(y=rowname, x=value, fill=name)) + \n  geom_col() + \n  ggtitle(\"Calcuate Corelation Matrix from Scratch\")\n\n\n\n\n\n\n\n\n\ntidy(pca_estimate,number=2,type=\"variance\") |&gt; \n  ggplot(aes(y = value, x = component,fill=terms)) + \n  geom_col() + \n  facet_wrap(~terms,scales = \"free\")\n\n\n\n\n\n\n\n\nExperimenting closest feature.\nHowever I am interested in see which feature should below in which under laying factor?\nSo I created this visualisaion:\n\n\nmagic function for compare features\ncompare_factor = function(tidybaked, pca1, pca2) {\n  \n  pca1Quo = quo(get(pca1))\n  pca2Quo = quo(get(pca2))\n  \n  tidybaked |&gt; \n    filter(component %in% c(pca1,pca2)) |&gt; \n    pivot_wider(\n       names_from=component\n      ,values_from=value\n    ) |&gt; \n    mutate(diff = !!pca2Quo - !!pca1Quo) |&gt; \n    arrange(diff) |&gt; \n    rowwise() |&gt; \n    mutate(\n      mxv=max(!!pca2Quo, !!pca1Quo), miv=min(!!pca2Quo, !!pca1Quo)\n    ) |&gt;\n    mutate(\n       terms = fct_inorder(terms)\n      ,is_pc2=diff &gt;= 0\n      ,abs_diff=abs(diff)) |&gt; \n    ggplot(aes(y= terms,alpha=abs_diff)) + \n    geom_linerange(\n      aes(xmin=miv,xmax=mxv,color=is_pc2)\n      ,linewidth=2\n      ) + \n    geom_point(aes(x=!!pca1Quo),color=\"lightcoral\") + \n    geom_point(aes(x=!!pca2Quo),color=\"cyan3\") +\n    scale_alpha_continuous(trans=\"sqrt\") +\n    theme(legend.position=\"none\")\n}\n\n\nThis compares principle component 1 against 2\n\ntidy(pca_estimate,number=2,type=\"coef\") |&gt; \n  compare_factor(\"PC1\", \"PC2\") +\n  ggtitle(\n    \"Feature Alocation of two principle component\"\n    ,\"Compare coeficency of PCA1 against PCA2\")\n\n\n\n\n\n\n\n\nThis compares principle component from 2 against 3\n\ntidy(pca_estimate,number=2,type=\"coef\") |&gt; \n  compare_factor(\"PC2\", \"PC3\") +\n  ggtitle(\n    \"Feature Aloocation of two principle component\"\n    ,\"Compare PCA2 against PCA3\")\n\n\n\n\n\n\n\n\nHowever I don’t understand this visualisation enough to understand if what this PCA says about data (if they are latend or not). So I am creating below experiment to understand PCA.\n\nExperiment of a rotated 2D surface in a 3D surfaces\nFirst is by creating on latent variable we understand.\nIf project a 2D sheet in a 3D space the surface point on that 2D sheet has underlaying latent space of the 2 dimension.\nFirst we need to sample a surface point and rotate this surface\n\nx &lt;- rnorm(1000)\ny &lt;- rnorm(1000)\n\nd = data.frame(\n   x = x\n  ,y = y \n) |&gt; \n  arrange(x) |&gt; \n  mutate(z = x + y)\nplotly::plot_ly(d\n  ,x=~x\n  ,y=~y\n  ,z=~z\n  ,marker=list(\n    color=\"blue\"\n    ,size=2))\n\nNo trace type specified:\n  Based on info supplied, a 'scatter3d' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d\n\n\nNo scatter3d mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\nrequire(factoextra)\nnormal_fct = d |&gt; \n  prcomp()\n\nnormal_fct |&gt; \n  get_eig()\n\n        eigenvalue variance.percent cumulative.variance.percent\nDim.1 2.912219e+00     7.337731e+01                    73.37731\nDim.2 1.056609e+00     2.662269e+01                   100.00000\nDim.3 8.190886e-31     2.063805e-29                   100.00000\n\n## screen plot is ploting wahts\nnormal_fct |&gt; \n  fviz_eig()\n\n\n\n\n\n\n\n\nIn this case, the true under-laying variance, it depends on how that surface you generated rotate.\n\nnormal_fct |&gt; \n  fviz_pca_var(repel=T)\n\n\n\n\n\n\n\n\n\nfct_d = d |&gt; \n  recipe() |&gt; \n  step_normalize() |&gt; \n  step_pca()\n\nfct_d |&gt; prep() |&gt; tidy()\n\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_LECN5\n2      2 step      pca       TRUE    FALSE pca_27riA"
  },
  {
    "objectID": "2024/04-20 ML with Tidymodel/02.html#actual-model",
    "href": "2024/04-20 ML with Tidymodel/02.html#actual-model",
    "title": "Work with AMES Data",
    "section": "Actual Model",
    "text": "Actual Model\n\n## classic geo-special machine-learning traning\names_rec = ames_train |&gt; \n  recipe(Sale_Price ~ \n           Neighborhood \n         + Gr_Liv_Area \n         + Year_Built \n         + Bldg_Type\n         + Latitude \n         + Longitude, data=ames_train) |&gt; \n  step_log(Gr_Liv_Area, base=10) |&gt; \n  step_other(Neighborhood, threshold=0.01) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\")) |&gt; \n  step_ns(Latitude,Longitude,deg_free=20)\n  \nlm_model=linear_reg() |&gt; set_engine(\"lm\")\ntree_model=rand_forest() |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"regression\")\n\n\n## base line for use single model\nlm_wflow =\n  workflow() |&gt; \n  add_model(lm_model) |&gt; \n  add_recipe(ames_rec)\n\n## use multiple model requires receipy and model match up\nwflowset = workflow_set(\n  list(ames_rec), \n  models=list(lm_model, tree_model))\n\n\nwflowset |&gt; \n  workflow_map() |&gt; \n  mutate(fitted = map(info, ~ fit(.x$workflow[[1]],ames_train) ))\n\n✖ The workflow requires packages that are not installed: 'ranger'. Skipping this workflow.\n\n\nError in `mutate()`:\nℹ In argument: `fitted = map(info, ~fit(.x$workflow[[1]], ames_train))`.\nCaused by error in `map()`:\nℹ In index: 2.\nCaused by error in `fit_xy()`:\n! Please install the ranger package to use this engine.\n\n\nYou won’t be able to fit model because you have not learned re-sample yet."
  },
  {
    "objectID": "2024/06-08 UFO Map/01.html",
    "href": "2024/06-08 UFO Map/01.html",
    "title": "Geo-Spatial Analysis on UFO sighting data",
    "section": "",
    "text": "I first encounter UFO for my master degree. Not literally. I need a project for my “Data Viz” module. But back then my geo-spatial analysis skills was limited.\nThis little project will look over these data an attempt to answer questions such as:\n\nWhere can I find aliens?\nWhat does a UFO typically looks like (based on description)?\nHas alien left us since the 1980s? (temporal patterns)\n\n\n\n\nUFO Heatmap 1910-2014\n\n\n\n\nData includes both logitude location and times. So can do both time series or spatial viz. Source: https://www.kaggle.com/NUFORC/ufo-sightings\nThe complete data includes entries where the location of the sighting was not found or blank (0.8146%) or have an erroneous or blank time (8.0237%). Since the reports date back to the 20th century, some older data might be obscured. Data contains city, state, time, description, and duration of each sighting.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(leaflet)\n\n.label_data_src = \"data source: NUFORC data by Sigmond Axel (2014)\"\n\n## read data and clean date\nufo &lt;- read_csv('./scrubbed.csv') |&gt; \n  janitor::clean_names() |&gt; \n  mutate(\n     datetime = as_datetime(datetime, format = \"%m/%e/%Y %R\")\n    ,date_posted = as_date(date_posted, format=\"%m/%e/%Y\")) |&gt; \n  mutate(id = row_number()) |&gt; \n  relocate(id)\n\n## convert to sf object\nufo_points = ufo |&gt; \n  filter(!is.na(latitude) & !is.na(longitude)) |&gt; \n  filter(!(latitude == 0 & longitude == 0)) |&gt; \n  drop_na() |&gt; \n  sf::st_as_sf(coords=c(\"longitude\",\"latitude\"),crs=4326)\n\n# df_scr &lt;- read_csv('./scrubbed.csv') |&gt; janitor::clean_names()\n\n## Below used to analysis how to format date\n# ## analysis to check which one is digit which one is date, which one is month\n# df_com |&gt; \n#   select(datetime) |&gt; \n#   mutate(\n#     md1 = str_extract(datetime, \"^\\\\d{1,2}\")\n#     ,md2 = str_extract(datetime, \"(?&lt;=[\\\\d]{2}\\\\/)\\\\d{1,2}\")\n#   ) |&gt; \n#   distinct(md1,md2)\n# ## first digit is month, second is date\n# stopifnot(0!=df_com |&gt; \n#   select(datetime) |&gt; \n#   mutate(datetime_ = as_datetime(datetime, format=\"%m/%e/%Y %R\")) |&gt; \n#   filter(day(datetime_) &gt; 12) |&gt; \n#   nrow())\n# ## the format you want to extract is \"%m/%e/%Y %R\" use lubridate dattiem"
  },
  {
    "objectID": "2024/06-08 UFO Map/01.html#introduction",
    "href": "2024/06-08 UFO Map/01.html#introduction",
    "title": "Geo-Spatial Analysis on UFO sighting data",
    "section": "",
    "text": "I first encounter UFO for my master degree. Not literally. I need a project for my “Data Viz” module. But back then my geo-spatial analysis skills was limited.\nThis little project will look over these data an attempt to answer questions such as:\n\nWhere can I find aliens?\nWhat does a UFO typically looks like (based on description)?\nHas alien left us since the 1980s? (temporal patterns)\n\n\n\n\nUFO Heatmap 1910-2014\n\n\n\n\nData includes both logitude location and times. So can do both time series or spatial viz. Source: https://www.kaggle.com/NUFORC/ufo-sightings\nThe complete data includes entries where the location of the sighting was not found or blank (0.8146%) or have an erroneous or blank time (8.0237%). Since the reports date back to the 20th century, some older data might be obscured. Data contains city, state, time, description, and duration of each sighting.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(leaflet)\n\n.label_data_src = \"data source: NUFORC data by Sigmond Axel (2014)\"\n\n## read data and clean date\nufo &lt;- read_csv('./scrubbed.csv') |&gt; \n  janitor::clean_names() |&gt; \n  mutate(\n     datetime = as_datetime(datetime, format = \"%m/%e/%Y %R\")\n    ,date_posted = as_date(date_posted, format=\"%m/%e/%Y\")) |&gt; \n  mutate(id = row_number()) |&gt; \n  relocate(id)\n\n## convert to sf object\nufo_points = ufo |&gt; \n  filter(!is.na(latitude) & !is.na(longitude)) |&gt; \n  filter(!(latitude == 0 & longitude == 0)) |&gt; \n  drop_na() |&gt; \n  sf::st_as_sf(coords=c(\"longitude\",\"latitude\"),crs=4326)\n\n# df_scr &lt;- read_csv('./scrubbed.csv') |&gt; janitor::clean_names()\n\n## Below used to analysis how to format date\n# ## analysis to check which one is digit which one is date, which one is month\n# df_com |&gt; \n#   select(datetime) |&gt; \n#   mutate(\n#     md1 = str_extract(datetime, \"^\\\\d{1,2}\")\n#     ,md2 = str_extract(datetime, \"(?&lt;=[\\\\d]{2}\\\\/)\\\\d{1,2}\")\n#   ) |&gt; \n#   distinct(md1,md2)\n# ## first digit is month, second is date\n# stopifnot(0!=df_com |&gt; \n#   select(datetime) |&gt; \n#   mutate(datetime_ = as_datetime(datetime, format=\"%m/%e/%Y %R\")) |&gt; \n#   filter(day(datetime_) &gt; 12) |&gt; \n#   nrow())\n# ## the format you want to extract is \"%m/%e/%Y %R\" use lubridate dattiem"
  },
  {
    "objectID": "2024/06-08 UFO Map/01.html#exploratory-analysis",
    "href": "2024/06-08 UFO Map/01.html#exploratory-analysis",
    "title": "Geo-Spatial Analysis on UFO sighting data",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\nGrand Map View\nHere is an interactive visualisation for all the code\n\n\n[1] \"The propotion of data visualised is: 0.83\"\n\n\n[1] \"1910-01-02 UTC\" \"2014-05-08 UTC\"\n\n\n\n\ncreate UFO icon map - make icon\n## over all scale using \nufo_icon = makeIcon(iconUrl = \"./asset/ufo.png\",\n                    iconWidth = 25, iconHeight = 25)\n\n## a javascript function which creates clusters\nclus_func_js = function() {\n  JS(\"  \n    function(cluster) {  \n       return new L.DivIcon({  \n         html: '&lt;div style=\\\"background-color:rgba(77,77,77,0.05)\\\"&gt;&lt;span&gt;' + cluster.getChildCount() + '&lt;/div&gt;&lt;span&gt;',  \n         className: 'marker-cluster'  \n       });  \n       }\")\n}\n\n## crystallize above code into a function\naddUFOMarker = function(x) {\n  addMarkers(\n    x,\n    ## automatically add sf object\n    icon = ufo_icon,  \n    clusterOptions = markerClusterOptions(  \n      iconCreateFunction = clus_func_js()\n    ),  \n    ## sorry! depends on having below columns\n    label = ~ label,\n    popup = ~comments_,\n    group = \"ufo\"\n  )\n}\n\n## Make UFO Icon ---------------------------------------------------------------\n## access polygon data\ncountry_polygons = spData::world |&gt; \n  filter(iso_a2 %in% c(\"GB\",\"US\",\"CA\"))\n\n## make grid on top of the layer\nmesh=sf::st_make_grid(\n     country_polygons\n    ,cellsize = c(0.5,0.5)\n    ,square = T\n  )\n## intersect points \nids = st_intersects(mesh,ufo_points)\nwhich_non_zeros = which(lengths(ids) != 0)\npixels =st_as_sf(x=mesh[which_non_zeros])\npixels$n = lengths(ids)[which_non_zeros]\n\n## Make the Map ----------------------------------------------------------------\n\n## leaflet color palete\npal &lt;- colorNumeric(\n  palette = \"PuBu\",\n  domain = sqrt(pixels$n))\n\n## use base layer of map\n## set view to USA\nus_lon &lt;- -95.71 #W\nus_lat &lt;- 37.09 #N\nbase_map = ufo_points |&gt; \n  mutate(\n     label = paste(city,state,country,shape)\n    ,comments_ = paste0(datetime,\": \", comments)\n  ) |&gt; \n  leaflet()  |&gt; \n  setView(lng = us_lon, lat = us_lat, zoom =4) |&gt; \n  addProviderTiles(providers$CartoDB.Positron)\n\n## the grand map himself\nbase_map |&gt; \n  addPolygons(\n    data=pixels,stroke=F,fillColor = ~pal(sqrt(pixels$n))\n    , fillOpacity = 0.8\n    , group = \"heatmap\") |&gt; \n  addUFOMarker() |&gt; ## leaflet color palete\n  addLayersControl(\n    overlayGroups=c(\"ufo\",\"heatmap\")\n  )\n\n\n\n\n\n\nIf you explore this map, you will find there are barely any record outside North America The majority of sighting concentrate on US. Actually if you identify the rest of the data. Majority comes from a country that is not US…\n\nufo |&gt; \n  left_join(select(ufo_points,id),\"id\",keep=T,suffix=c(\"\",\"_p\")) |&gt;\n  filter(is.na(id_p)) |&gt; \n  slice_sample(n=10)\n\n\n  \n\n\n\n\nufo |&gt; \n  left_join(select(ufo_points,id),\"id\",keep=T,suffix=c(\"\",\"_p\")) |&gt;\n  filter(is.na(id_p)) |&gt; \n  group_by(country) |&gt; \n  # mutate(country=coalesce(country,paste(\"-\",state))) |&gt; \n  count() |&gt; \n  arrange(desc(n))\n\n\n  \n\n\n\nWe are missing 1894 Great Britain sighting.\nIn some cases the “UFOs” concentrate at one point:\n\n\n\n\n\n\n\n\n\n\n\n(a) Concentrated Sightings at NYC\n\n\n\n\n\n\n\n\n\n\n\n(b) Civic Centre in 1964\n\n\n\n\n\n\n\nFigure 1: About 500 sighting squished into one spot, nearest location is NY civic center.\n\n\n\nIt is likely that those data does not have specific coordinate so they are squished together into one marker.\n\n\nSighting Over Time\nNext lets overview if there are any temporary pattern in terms of number of UFO sightings.\n\n\nanimation map\nlibrary(plotly)\nlibrary(RColorBrewer)\nmapbox_token = Sys.getenv(\"MAPBOX_TOKEN\")\n\n## add coordinates to data (very not-elegant)\nxy=sf::st_coordinates(ufo_points)\nufo_points$lon = xy[,1]\nufo_points$lat = xy[,2]\nufo_points$year = year(ufo_points$datetime)\n\n## animation map himself\nufo_points |&gt; \n  plot_ly(\n    type='densitymapbox'\n    ,frame = ~year\n  ) |&gt; \n  add_trace(\n     radius = 5\n    ,lon = ~lon\n    ,lat = ~lat\n    ,colorscale=\"Electric\"## refer to plotly OOP document, this is shared with Python\n    ,showlegend=F\n  ) |&gt; \n  config(mapboxAccessToken = Sys.getenv(\"MAPBOX_TOKEN\")) |&gt; \n  layout(\n    mapbox = list(\n      # style=\"carto-positron\",\n      style=\"dark\",## this only works if you have mapbox token\n      zoom=2.5,\n      center= list(lon=us_lon,lat=us_lat))\n    # , coloraxis = list(colorscale=\"Viridis\")\n    ) |&gt; \n  animation_opts(\n    150,\n    easing = \"sin\", #redraw = FALSE\n  )\n\n\n\n\n\n\n\n\nCode\nufo |&gt; \n  left_join(select(ufo_points,id),\"id\",keep=T,suffix=c(\"\",\"_p\")) |&gt;\n  mutate(\n    month_year=floor_date(datetime,\"months\"),\n    miss_coord=is.na(id_p)) |&gt; \n  group_by(month_year,miss_coord) |&gt; \n  summarise(n=n(),.groups=\"drop\") |&gt; \n  ggplot(aes(x=month_year,y=n,color=miss_coord)) +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"number of occurance\") +\n  xlab(\"month and year\") + \n  geom_vline(xintercept=as.POSIXct(ymd(\"1990-01-01\")),linetype=\"dashed\",color=\"midnightblue\") +\n  annotate(\"text\"\n        ,x=as.POSIXct(ymd(\"1990-01-01\")),y=700\n        ,label=\"Year 1990\",color=\"midnightblue\") +\n  ggtitle(\"UFO Sighting between 1910-2014 around the World\") +\n  scale_color_manual(\n    values=c(\"TRUE\"=\"grey\",\"FALSE\"=\"black\"),name=\"\",labels=c(\"TRUE\"=\"Not on Map\",\"FALSE\"=\"On Map\")) +\n  theme(legend.position=\"top\")\n\n\n\n\n\n\n\n\n\n\nufo_weight = ufo |&gt; \n  select(id,duration_seconds) |&gt; \n  mutate(weight = log(duration_seconds)) ## normalized weight\n\nBelow we are focusing on the US, analyzing 1990 onward:\n\nCode\n## Seasonal Pattern\nufo |&gt; \n  mutate(month_w = isoweek(datetime),year=year(datetime)) |&gt; \n  group_by(month_w,year,country) |&gt; \n  filter(country == \"us\") |&gt; \n  filter(year &gt;= 1990) |&gt; \n  summarise(n = as.double( n() ), .groups=\"drop\") |&gt; \n  ungroup() |&gt; \n  mutate(across(c(month_w,year), as.factor)) |&gt; \n  ggplot(aes(x = month_w,y=year,fill=n)) + \n  geom_bin2d() +\n  facet_wrap(~country,scales=\"free\") +\n  theme_minimal() +\n  scale_fill_viridis_c(option=\"E\",trans=\"sqrt\") +\n  # geom_vline(xintercept=c(6,7),linetype=\"dashed\",alpha=c(0.3,1)) +\n  coord_polar(start = -pi/53) +\n  ggtitle(\"Count of UFO sighting in US by Week 1990 - 2014\") +\n  theme(legend.position = \"none\")\n## temporary pattern\nufo |&gt; \n  filter(country == \"us\") |&gt; \n  mutate(hour = hour(datetime),year=year(datetime)) |&gt; \n  filter(year &gt;= 1990) |&gt;\n  group_by(hour,year) |&gt; \n  summarise(n = n()/(365 * 24),.groups=\"drop\") |&gt; \n  ungroup() |&gt; \n  ## ploting\n  mutate(across(c(hour,year), as.factor)) |&gt; \n  ggplot(aes(x = hour,y=year,fill=n)) + \n  geom_bin2d() +\n  scale_fill_viridis_c(option=\"E\",trans=\"sqrt\") +\n  theme_minimal() +\n  scale_y_discrete(labels= c() ) +\n  coord_polar(start= -pi/24) +\n  ## annotate a layer of \n  geom_vline(aes(xintercept=21),color=\"black\",linetype=\"dashed\") +\n  ggtitle(\"Every hour, average sightings of UFO in US, 1990 - 2014\") +\n  theme(\n    legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nThe hourly pattern is more clear than yearly pattern. The peak interval for UFO spotting time is 20:00 - 22:00 (the usual time when I decide to pull a glass of wine). There are not much pattern in terms of week other than than the fact that it tend to happen in holidays.\nThere are systematic ways of analyzing cycles. A spectrum analysis could be extremely helpful here. Or a “periodontal”. I am also curious if incorporating public holidays in the US will explain more temporary variance. This two maybe solved by fit a machine learning model.\nConsiderably, where the time occurs is also important. The animated map has verified that sighting tend to happen “high population density” cities. Rumor say UFO sighting happens near water because the flying source use water as fuel. Arguably, for archaeology reasons, cities tends to developed around rivers and ports. So it is hard to tell if UFO appears because of water or cities, until we whichever better explains variance in the model.\n\n\nShape and Description of UFO\nClassic text analysis often uses “tf-idf” (term frequency and inverse document frequency). I find this matrix less usefully since many of the “comment” are actually short-text. Term frequency within document are mostly 1 or 2. In fact in this data, the mix term frequency after taken out stop words are 5.\nI took a much simply English analysis but counting only nous and adjectives. This gaves us a flavour of what’s been used to describe UFO.\n\n\ncalculate essential matrix\nlibrary(tidytext)\n# if (!require(\"pacman\")) install.packages(\"pacman\"); library(pacman)\n# p_load_gh('hrbrmstr/pluralize')\n\n## we want to focus on nonu and adjectives\nnonu_adj = parts_of_speech |&gt; \n  filter(pos == 'Nonu' | pos == 'Adjective') |&gt; \n  group_by(word) |&gt; \n  slice_head()\n\n## convert comments into short text\nufo_terms_us_year = ufo |&gt; \n  \n  filter(country == 'us') |&gt; \n  filter(year(datetime) &gt;= 1950) |&gt; \n  \n  ## take out stop words and keep only nonu and adjuctives\n  unnest_tokens(word,comments) |&gt; \n  anti_join(stop_words,\"word\") |&gt;\n  inner_join(nonu_adj,\"word\") |&gt; \n  filter(!word |&gt; str_detect('\\\\d')) |&gt;\n  \n  count(id,word) |&gt; \n  left_join(select(ufo, id, duration_seconds, datetime)) |&gt; \n  mutate(year = year(datetime)) |&gt; \n  \n  ## group wise term-frequency\n  group_by(year) |&gt; \n  mutate(total_n = sum(n)) |&gt; \n  ungroup() |&gt; \n  group_by(year, word) |&gt; \n  mutate(\n    tf_year = sum(n)/total_n ) |&gt;  \n  ungroup() |&gt; \n  \n  ## idf-inverse document frequency acrros\n  mutate(\n    n_doc = unique(id) |&gt; length()\n  ) |&gt; \n  group_by(word) |&gt; \n  mutate(n_doc_term = unique(id) |&gt; length() ) |&gt; \n  ungroup() |&gt; \n  mutate(g_idf = log(n_doc/n_doc_term )) |&gt; \n  \n  ## this new term uses frequency of term within year;\n  ## global inverse document frequency\n  mutate(tf_idf_year_g = tf_year * g_idf) |&gt; \n  ## add in comparation\n  bind_tf_idf(word,id,n)\n\n\nJoining with `by = join_by(id)`\n\n\n\n\nCode\nrequire(patchwork)\n\n\ntop_n_term = ufo_terms_us_year |&gt; \n  group_by(word) |&gt; \n  summarise(n=sum(n), .groups = \"drop\") |&gt; \n  ungroup() |&gt; \n  slice_max(n=35, order_by = n) |&gt; \n  mutate(freq=n/sum(n))\n\nmy_palette = generate_palette(c(top_n_term$word, 'triangular'))\n\ng1 = top_n_term |&gt; \n  mutate(word = factor(word, rev( top_n_term$word) )) |&gt; \n  ggplot(aes(y=word, x=freq, fill = word)) +\n  geom_col() + \n  theme_minimal() + \n  scale_fill_manual(values =  my_palette, guide=NULL) +\n  ylab(\"\") + \n  xlab(\"\") +\n  ggtitle(\"\", \"Global frequency of term\") + \n  theme(\n    panel.grid.major.x=element_blank(),\n    panel.grid.minor.x=element_blank(),\n    panel.grid.minor.y=element_blank(),\n    panel.grid.major.y=element_blank()\n  )\n  \nhighlight_w = c(\"red\", \"white\", \"green\", \"yellow\",  \"moving\")\ng2 = ufo_terms_us_year |&gt; \n  distinct(word,year,tf_year) |&gt; \n  plot_word_line(tf_year, highlight_w = highlight_w ) +\n  theme(\n    panel.grid.major.x=element_blank(),\n    panel.grid.minor.x=element_blank(),\n    panel.grid.minor.y=element_blank()\n  ) +\n  ylab(\"Term Frequency of the year\") + \n  xlab(\"\") +\n  ggtitle(\"\",\"Frequency of term by year\")\n\n\n(g2 | g1 + \n  plot_layout(width = c(5,2))) +\n  plot_annotation(\n    title=\"&lt;b&gt;UFOs are spotted more red than green&lt;/b&gt;\",\n    caption = .label_data_src,\n    subtitle = strwrap(glue::glue(\n      \"Frequency of nouns and adjuctives used to describe UFO event after 1970s. Top 35 terms are shown on the left. Nouns associated to color are highligted\",\n      \"\"\n    )),\n    theme=theme(plot.title=ggtext::element_markdown())\n    )\n\n\n\n\n\n\n\n\n\nIf we look at top nouns and adjectives, it is no surprise we see “flying” or “moving”. I find it interesting that Red and Green are two color of complete opposite spectrum, yet after 2000 more UFO are more likely to be described as red than green.\nI’ve not highlighted shape yet because it turns out “shape” is a column on its own. I included a bit more data from after 1950s so\n\n\nCode\nufo_shapes_p = ufo |&gt; \n  mutate(year = year(datetime)) |&gt;\n  group_by(year, shape) |&gt; \n  \n  summarise(\n    n = n(),\n    .groups = \"drop\"\n  )  |&gt; \n  mutate(total_n = sum(n)) |&gt; \n  ## total number of doc of year\n    group_by(year) |&gt; \n    mutate(year_n = sum(n)) |&gt; \n    ungroup() |&gt; \n  ## total world count\n    group_by(shape) |&gt; \n    mutate(shape_n = sum(n) ) |&gt; \n    ungroup() |&gt; \n  ## calculate propoability\n  mutate(\n    p_shape = shape_n / total_n,\n    p = round(pbinom(n, size=year_n, prob = p_shape, lower.tail=F),4)\n  ) |&gt; \n  \n  mutate(freq = (n + 0.5)/ (year_n + 0.5))\n\nhigh_freq_shape = ufo_shapes_p |&gt; \n  group_by(shape) |&gt; \n  summarise(n = sum(n)) |&gt; \n  filter(!shape |&gt; is.na()) |&gt; \n  mutate(shape = fct_infreq(shape,n)) |&gt; \n  filter(n &gt; quantile(n, 0.60))\n  \nufo_shapes = ufo_shapes_p |&gt; \n  mutate(pp= 1-p) |&gt; \n  filter(year &gt; 1950) |&gt; \n  # filter(pp &gt; 0.95) |&gt; \n  semi_join(high_freq_shape, \"shape\") |&gt; \n  arrange(-desc(freq)) |&gt; \n  mutate(\n    shape=fct_inorder(shape)\n  )\n\n## fit lm to detect trend automatically\nufo_lm = ufo_shapes |&gt; \n  group_by(shape) |&gt; \n  mutate(\n    cov_ = (year - mean(year)) * (freq - mean(freq))\n  ) |&gt; \n  summarise(pearson_r = sum(cov_) / (sd(year) * sd(freq) * n()) )\n  \n\ntrend_palette =  c(\"-\"=\"#D35400\", \"+\"=\"#28B463\", \"0\" = \"#AAB7B8\")\n\ng3 = ufo_shapes |&gt; \n  group_by(shape) |&gt; \n  ## calculate global frequency\n  summarise(n = sum(n), .groups=\"drop\") |&gt; \n  mutate(freq = n/sum(n)) |&gt; \n  left_join(ufo_lm, \"shape\") |&gt; \n  mutate(sign = ifelse(\n    abs(pearson_r) &gt; 0.45,\n    ifelse(pearson_r &lt; 0,\"-\",\"+\" ),\n    \"0\")) |&gt;\n  ## plot specific\n  mutate(.n= 1/n) |&gt; \n  mutate(\n    shape = fct_infreq(shape,.n),\n    .is_edge = freq&gt; quantile(freq,0.9),\n    .hjust = ifelse(.is_edge, 1.2,-0.2),\n    .color=ifelse(.is_edge,\"white\", \"#873600\"),\n    .label = ifelse(sign != \"0\", round(freq,2), \"\")\n  ) |&gt; \n  ggplot(aes(x = freq, y = shape, fill=sign)) +\n  geom_col(width=0.7) + \n  geom_text(\n    aes(label = .label, hjust=.hjust , color=.color)\n    ,size=4) +\n  scale_color_identity() +\n  scale_fill_manual(\n    values = trend_palette, guide = NULL\n  ) + \n  theme_void() +\n  theme(\n    axis.text.y = element_text(size = 7)\n  )\n\n## visualise trend\nufo_shapes_trend_vis_data = ufo_shapes |&gt; \n  left_join(ufo_lm, \"shape\") |&gt; \n  filter(abs(pearson_r) &gt; 0.45) |&gt; \n  mutate(sign = ifelse(pearson_r &lt; 0,\"-\",\"+\" )) \n\nufo_shapes_trend_vis_data_ends = ufo_shapes_trend_vis_data |&gt; \n  group_by(shape) |&gt; \n  filter(year == max(year) | year == min(year))\n\ng4 = ufo_shapes_trend_vis_data |&gt; \n  ggplot(\n    aes(x =year,y=freq)) + \n  ## grey background\n  geom_line(\n    data = ufo_shapes |&gt; mutate(shape2=shape) |&gt; select(-shape),\n    aes(group=shape2), color=\"grey\", size=0.3, alpha=0.5) + \n  ## major line \n  geom_line(aes(color=sign)\n            , size=1) +\n  geom_point(\n    data = ufo_shapes_trend_vis_data_ends, \n    aes(color=sign)\n  ) +\n  geom_text(\n    data = ufo_shapes_trend_vis_data_ends |&gt; slice_max(n=1,order_by=year),\n    vjust=-1,hjust=1,\n    aes(color=sign, label = round(freq,2))\n  ) +\n  ## annotation\n  ## layout\n  ylab(\"Frequency of Shape\") + \n  facet_wrap(~shape) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x=element_blank(),\n    panel.grid.minor.x=element_blank(),\n    panel.grid.minor.y=element_blank()\n  ) + \n  scale_color_manual(values=trend_palette, guide=F)\n  \n\nrequire(ggtext)\n(g4 | g3) + patchwork::plot_layout(widths = c(4,1)) +\n  patchwork::plot_annotation(\n    title=\"&lt;b&gt;UFO has been seen less circular&lt;/b&gt;\", \n    str_replace(str_wrap(glue::glue(\"Showing 5 of 30 most significant trend\",\n    \" mesaured by pearson corelation coefficency.\",\n    \" Downward trend are shown in &lt;b&gt;&lt;span style='color: #D35400'&gt;orange&lt;/span&gt;&lt;/b&gt; and upward trend in &lt;b&gt;&lt;span style='color: #229954'&gt;green&lt;/span&gt;&lt;/b&gt;. &lt;br&gt;&lt;br&gt;\",\n    \"In addtion top 12 most frequent shape are shown. They represent 60% of the shape\")), \"\\n\",\"&lt;br&gt;\"),\n    caption=.label_data_src,\n    theme=theme(\n      plot.subtitle=ggtext::element_markdown(lineheight = 1.1),\n      plot.title=ggtext::element_markdown())\n  )\n\n\n\n\n\n\n\n\n\nThere are about 30 different shaped used to label UFO sighting events. There are going to be a lot of visual load. I have cut off data point before 1950 because values are too small. How I have selected 5 out of 30 shapes is by use “Pearson’s R”(x are year, y are frequency) which are great at spotting consistent line like relationship (but not arch. See Schober et al, 2018). For this reason I’ve select those “Pearson’s R” is higher than 45 %.\nMethodologically, I feel a bit unsettled because prior 200. Low level of data are collected. As how term frequency is measured, the variance in early years will be much higher than those in later years. It is possible that Pearson’s R is high or low simply because of a random deviant from early year which is lack of data. I supposed a better approach would be re-patch data use “uneven” time interval. For example, before 1950 aggregate data point by every 5 years, after 2000 aggregated data point by every quarter. I suppose those subject to further study.\nNever the less, this method successfully delivered “clear” trends for the visual.\nWe see less proportion of UFO event describe “cigar”, “disk” or “oval” and more describe “triangle” or “light”. The most obvious trend is “disk”, At 1950, nearly half record describe “UFO” sighting as disk. Towards the end this is only 3%."
  },
  {
    "objectID": "2024/06-08 UFO Map/01.html#conclusion",
    "href": "2024/06-08 UFO Map/01.html#conclusion",
    "title": "Geo-Spatial Analysis on UFO sighting data",
    "section": "Conclusion",
    "text": "Conclusion\nHas different species of Alien been visiting earth? I don’t think so! I too wondered why it has to be “green” or “red” light. Surely advanced civilization should appreciated different color. I quickly goggled “military aircraft” and immediately realize what the “green” and “red” color are. Here a quote from Wikipedia:\n\n“Some navigation lights are colour-coded red and green to aid traffic control by identifying the craft’s orientation. Their placement is mandated by international conventions or civil authorities such as the International Maritime Organization (IMO).”\n\n\n\n\n\n\n\n\n\n\nMilitary Light\n\n\n\n\n\n\n\nFormation Light\n\n\n\n\n\nI think those people has just been seeing airplanes. Specially that those in the high population density area witness more UFO events. Finally UFOs are often seen after dark around 20:00. That could also explains why so many description mentioned light.\nFor those keen UFO enthusiasm looking for genuine sightings, they will now have to filter through possible dozens of false alert. Or of course, if they are smart, they could gather description of “airplane sighting”, training a model and use to to filter out those false alert. That is a rabbit hole I’ve been trying to avoid."
  },
  {
    "objectID": "2024/09-06 Sandy-Status/sandy-status/vignettes/stochestic_status_flow.html",
    "href": "2024/09-06 Sandy-Status/sandy-status/vignettes/stochestic_status_flow.html",
    "title": "Stochastic Status Flow Experiment - Part 1",
    "section": "",
    "text": "Recreate natural status process flow from a set of random generated event logs."
  },
  {
    "objectID": "2024/09-06 Sandy-Status/sandy-status/vignettes/stochestic_status_flow.html#introduction",
    "href": "2024/09-06 Sandy-Status/sandy-status/vignettes/stochestic_status_flow.html#introduction",
    "title": "Stochastic Status Flow Experiment - Part 1",
    "section": "Introduction",
    "text": "Introduction\nA “status audit log” is a set of data that record status change. A “status” is typically categorical variables. Some data warehousing book will can it a “audit log” or “snap-shot table”.\nThese data captures business related process, such as purchase order going through different stage of purchase - from order to shipment; or a customer moving through different online booking stage - from viewing the web-page advert to actual purchase.\nData analyst may seek to answer questions such as how quick it is for an order to move through or What’s the common customer purchase story.\nThis short experiment use a scholastically generated “status change log” and baysian inference to try answer those questions:\n\nCan we recreate a status flow graph using those data;\nPredict when and how many certain status will move through;\nPredict when a status is not moving"
  },
  {
    "objectID": "2024/09-06 Sandy-Status/sandy-status/vignettes/stochestic_status_flow.html#experiment-complete-chaotic",
    "href": "2024/09-06 Sandy-Status/sandy-status/vignettes/stochestic_status_flow.html#experiment-complete-chaotic",
    "title": "Stochastic Status Flow Experiment - Part 1",
    "section": "Experiment: Complete Chaotic",
    "text": "Experiment: Complete Chaotic\n\nFake Data\nBelow is one of many format of record status change:\n\nmake_fakelog(2)\n\n\n  \n\n\n\n\nThis example data has only two object (id 1, id2)\nStatues:\n\nWe are going to pretent status are represented by these numbers 1, 2, 3, 4..\nColumn from represent status before change\nColumn to represent status after change\n\nColumn log-time is when the change has taken place,\nColumn weight can represent a numerical value of object (in the purchase order context, “weight” can represent price of order).\n\nThis table is randomly generated, every status is random sampled and\nBelow of extract from how a fake log is made: The time is randomly sampled. Each status is random sampled.\nmake_fakelog_1 = function(fake_id, freq, n_status = 3) {\n  \n  fake_to = sample(seq(n_status), size=freq, replace=T)\n  fake_from=lag(fake_to)\n  wts = rnorm(freq, 10, 2) # average value is always 10 or 2 so\n\n  x = sample(seq(freq), size=1)\n  # Generate Time-stamp For One Single Year\n  Year_23 = seq(ymd('2023-01-01'), ymd('2023-12-31'), by='1 day')\n\n  fake_timelog=sort(sample(Year_23, size=freq))\n  data.frame(\n    id = fake_id,\n    from = fake_from,\n    to = fake_to,\n    logtime=fake_timelog,\n    weight = round(wts,2)\n  )\n}\n\n\nStatistical characteristics of random sampled time-gaps\nA randomly sampled value, comparing with previous value is normally distributed in R. (compare against a sample of normal distribution with same mean and standard deviation)\n\nn = 1e4\nx = runif(n)\ndiff_x = (x - lag(x))[-1]\nnorm_x = rnorm(n, mean(diff_x), sd(diff_x))\nplot_against_nrml(norm_x) + \n  ggtitle(\"&lt;b&gt;normal&lt;/b&gt; against &lt;b style='color:blue'&gt;x: runif unsorted&lt;/b&gt;\") \n\n\n\n\n\n\n\n\nHowever if the randomly sampled value are ordered the distribution will be more\n\n## (2) Are uniform sample interval exponential or normal? \n## this type with sorting\nn = 1e3\nx = sort(runif(n))\ndiff_x = (x - lag(x))[-1]\nplot_against_nrml(diff_x) + \n  ggtitle(\"&lt;b&gt;normal&lt;/b&gt; against &lt;b style='color:blue'&gt;x: runif sorted&lt;/b&gt;\") \n\n\n\n\n\n\n\n\nIf we sample from a set of integer instead the distribution will look similar:\n\n## (3) instead of runif just sample from a vector\nn = 1e3\nx = sort(sample(1:(n*10), n,replace = T))\ndiff_x = (x - lag(x))[-1]\nplot_against_nrml(diff_x) +\n  ggtitle(\"&lt;b&gt;normal&lt;/b&gt; against &lt;b style='color:blue'&gt;x: sample from a integer vector&lt;/b&gt;\")\n\n\n\n\n\n\n\n\nSo unsurprisingly the time data follow the same pattern:\n\n## this function helps as created a how very\ntime_delta.log = function(df) {\n  df |&gt; \n    group_by(id) |&gt; \n    mutate(last_time = lag(logtime, order_by = logtime)) |&gt; \n    mutate(time_delta = logtime - last_time) |&gt; \n    filter(!is.na(from)) |&gt; \n    ungroup(id)\n}\ndf = make_fakelog(1e3)\nchange_prequency = df |&gt; \n  time_delta.log()\n\n\nchange_prequency |&gt; \n  slice_sample(n = 3)\n\n\n  \n\n\n\n\n## now use the similar tool to inspect stochestic status log\nchange_prequency |&gt; \n  pull(time_delta) |&gt; \n  as.numeric() |&gt; \n  plot_against_nrml() + \n  ggtitle(\"&lt;b&gt;normal distribution&lt;/b&gt; against &lt;b style='color:blue'&gt;sampled time change &lt;/b&gt;\") +\n  xlab(\"days between status change\")\n\n\n\n\n\n\n\n\nThis looks acceptable and natural.\nIt is possible to try approximate distribution of different path and use distribution to re-sample /predict time_delta\nfirst let’s create a graph Status graph may look like this is simply because the process is stochestic\n\n\nCode\nlibrary(ggraph)\nchange_prequency |&gt; \n  mutate(total_n = n()) |&gt; \n  group_by(from, to) |&gt; \n  summarise(\n    median_td = median(time_delta),\n    sd = sd(time_delta),\n    total_n = first(total_n),\n    n = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  tidygraph::as_tbl_graph() |&gt; \n  mutate(name = paste(\"#\", row_number())) |&gt; \n  ggraph(layout = \"kk\") +\n  geom_edge_bend(\n    aes(label = n)\n  , arrow = arrow(type=\"open\")\n  , alpha = 0.25\n  , color = \"#004D40\"\n  , width = 2\n  ) +\n  geom_node_label(aes(label = name), color = \"white\", fill = \"black\") +\n  ggtitle(\n    \"A Chaotic Status Flow Chart\",\n    stringr::str_replace_all(stringr::str_wrap(width= 50, glue::glue(\n        \"Each &lt;i&gt;node&lt;/i&gt; is a status, each time something pass by it forms an &lt;i&gt;edge&lt;/i&gt;. \"\n      , \"When sampled from a random process, the status form a &lt;i&gt;complete graph&lt;/i&gt;\"))\n      , \"\\n\", \"&lt;br&gt;\"\n  )) +\n  coord_equal() + \n  theme(title = ggtext::element_markdown())\n\n\n\n\n\n\n\n\n\nIn this flow chart status goes to each status evenly, this is because we sampled status randomly. Each status has equal change of been sampled together.\n\n\nTime Interval of Status Jump\nGoing back to the question about when:\n\n## Then possible to create distribution for each edge type\nchange_prequency |&gt; \n  mutate(label = paste(from ,\"-&gt;\",to)) |&gt; \n  mutate(time_delta = as.numeric(time_delta, \"days\")) |&gt; \n  ggplot() + \n  facet_wrap(~label) + \n  geom_histogram(aes(x = time_delta), binwidth = 10) + \n  ggtitle(\"Distribution of Event Time for Each Edge\")\n\n\n\n\n\n\n\n\n\n\nfunction to compute baysian probability\n## The most random state is stochastic and full graph\nsummary.log = function(change_prequency) {\n  change_prequency |&gt;\n    ungroup() |&gt; \n    mutate(total_n = n()) |&gt; \n    group_by(from) |&gt; mutate(from_n = n()) |&gt; ungroup() |&gt; \n    group_by(from, to) |&gt; \n    summarise(\n      median_td = median(time_delta),\n      sd = sd(time_delta),\n      total_n = first(total_n),\n      from_n = first(from_n),\n      n = n(),\n      .groups = \"drop\"\n    )\n}\n## use grid method to study posterior distribution\nfreq_posterior.log = function(\n      change_freq_smry\n    , total_n = total_n\n    , grid_rs = 500\n  ) {\n  grid_rs = 500\n  # .shrink_ = (grid_rs/nrow(change_freq_smry) * 2)\n  p_grid = (1:grid_rs/grid_rs)[1:(grid_rs)]\n  freq_pst_grid = change_freq_smry |&gt; \n    select(from,to,{{total_n}}, n) |&gt; \n    cross_join(tibble(p=p_grid)) |&gt; \n    mutate(\n      likelyhood = dbinom(n, {{total_n}}, p)\n    ) |&gt; \n    group_by(from, to) |&gt; \n    mutate(posterior = likelyhood / sum(likelyhood))\n  return(freq_pst_grid)\n}\n## for be able to say the probability of something lower the 0.005 is this:\nvalidate.posterior = function(posterior, valid_edge,ignorable = 0.05) {\n  ## second you need an acceptance score\n  ## first way to use is acceptance against ignore\n  posterior |&gt; \n    group_by(from,to) |&gt;\n    filter(p &gt; ignorable) |&gt; \n    summarise(ignorability = sum(posterior)) |&gt; \n    left_join(valid_edge, c(\"from\",\"to\")) |&gt; \n    mutate(valid = coalesce(valid, FALSE)) |&gt; \n    mutate(valid = ifelse(valid, 1, 0)) |&gt;\n    mutate(predict = round(ignorability)) |&gt; \n    mutate(result = paste(sep = \"-\"\n        , ifelse(valid == predict, \"True\", \"False\")\n        , ifelse(predict == 1, \"Pos\", \"Neg\")\n        )\n  )\n}\n\nattach_score.posterior = function(posterior, valid_edge,ignorable = 0.05) {\n  score = posterior |&gt; \n    validate.posterior(valid_edge,ignorable = 0.05)\n  posterior |&gt; \n    left_join(score, c(\"from\", \"to\"))\n}\nplot.freq_posterior = function(freq_pst_grid, shadow = T,scales=\"free\",...) {\n  if(shadow) {\n    geom_shadow_ = (list(geom_line(\n      data=freq_pst_grid |&gt; mutate(label1 = paste(from,to))\n      , aes(x = p, y= posterior, group = label1),color=\"grey\",alpha=0.4,linewidth=0.2\n    )))\n  } else {\n    geom_shadow_ = list(NULL)\n  }\n  freq_pst_grid |&gt; \n    mutate(label = paste(from , \"-&gt;\", to)) |&gt; \n    filter(posterior != 0) |&gt; \n    filter(!is.na(posterior)) |&gt; \n    ggplot(aes(x = p, y= posterior,...)) + \n    geom_shadow_ +\n    geom_line() +\n    facet_wrap(~label,scales=scales)\n}\n\n\n\n\nPosterior probability\nThe way we sample posterior probability is simply by compute binomial probability across a grid or probability. This is know as\n“beta-binomial”.\n\nchange_freq_smry=change_prequency |&gt;\n  summary.log()\n\n## use sample method \nfreq_pst_grid = change_freq_smry |&gt; \n  freq_posterior.log()\n\nfreq_pst_grid |&gt; \n  plot.freq_posterior() + \n  ggtitle(\"Posterior of complete random graph\") + \n  scale_x_continuous(limits = c(0.025, 0.10))\n\n\n\n\n\n\n\n\nIt is difficult to tell information from this graph, this is expected."
  },
  {
    "objectID": "2024/09-06 Sandy-Status/sandy-status/vignettes/stochestic_status_flow.html#experiment-pre-determined-status",
    "href": "2024/09-06 Sandy-Status/sandy-status/vignettes/stochestic_status_flow.html#experiment-pre-determined-status",
    "title": "Stochastic Status Flow Experiment - Part 1",
    "section": "Experiment: Pre-Determined Status",
    "text": "Experiment: Pre-Determined Status\nPre-determined means which status can flow to which status follow a set of schema\n\nPrefectly Pre-Determined Status\nNow instead of sample randomly we are going to mandate status which can only flow to these statuses:\n1 ------&gt; 2 -----&gt; 3 \n           \\\n            4\n              \\\n               5\nTo sample a audit log, we are going to sample for all possible shortest path between leaf and nodes:\n\nlibrary(igraph)\n\n### we can possiblely try one with schema just to see what difference it makes\nn_particle = 1000\n\nstate_schema = igraph::graph_from_literal(1 -+ 2 -+ 3:4,4-+5)\n\n## either sample terminals or sample intermediate terminals\nleaves = V(state_schema)[degree(state_schema, mode=\"out\" ) == 0]\npaths = (state_schema |&gt; shortest_paths(1, leaves))$vpath\nedge_to = sample(paths,n_particle,replace=T) |&gt; imap_dfr(~tibble(id = .y, to = names(.x)))\nedge_list = edge_to |&gt;  group_by(id) |&gt; \n  mutate(.order = row_number(), from = lag(to, order_by =.order)) |&gt; \n  select(id,from,to)\n\n## try assign random years to log\nYear_23 = seq(ymd('2023-01-01'), ymd('2023-12-31'), by='1 day')\nn_per_obj = pull(count(edge_list, id),n)\nlogtime = n_per_obj |&gt; map_dfr(~tibble(logtime = sort(sample(Year_23, .x))))\nweight = n_per_obj |&gt; map_dfr(~tibble(weight = rnorm(.x, mean = 10, sd = 2) ))\n\n\nlog = bind_cols(edge_list, logtime, weight)\nfreq_posterior = log |&gt; \n  time_delta.log() |&gt; \n  summary.log() |&gt; \n  freq_posterior.log()\n\n\nlog |&gt; \n  time_delta.log() |&gt; \n  summary.log() |&gt; \n  tidygraph::as_tbl_graph() |&gt; \n  mutate(name = paste(\"#\",name)) |&gt; \n  ggraph(layout = \"kk\") +\n  geom_edge_bend(\n    aes(label = n)\n  , arrow = arrow(type=\"open\")\n  , alpha = 0.25\n  , color = \"#004D40\"\n  , width = 2\n  ) +\n  geom_node_label(aes(label = name), color = \"white\", fill = \"black\") +\n  ggtitle(\"Sample from a Custom Defined Status Flow Chart\") +\n  coord_equal()\n\n\n\n\n\n\n\n\n\nfreq_posterior |&gt; \n  plot.freq_posterior(shadow = T) +\n  ggtitle(\"Natural Progressing Status Log: Posterial of Edge\"\n          , stringr::str_replace_all(stringr::str_wrap(\n            width = 80\n            , string = glue::glue(\n                \"\"\n              , \"If &lt;b&gt;every object has passed through&lt;/b&gt; all the status till the end\"\n              , \", the joined edges has higher probability to occur than branches.\"\n              , \"This is because of the topology of graph: \"\n              , \"the closer the edge path is towards the destiny of graph lower probability\"\n            )), \"\\n\", \"&lt;br&gt;\")\n  ) + \n  theme(title = ggtext::element_markdown())\n\n\n\n\n\n\n\n\n2 -&gt; 3 and 2 -&gt; 4 has the same probability occurrence this is because they are not the same as the very so you will need depend prior probability to depend on previous status\n\nlog |&gt; \n  time_delta.log() |&gt; \n  summary.log() |&gt; \n  freq_posterior.log(from_n) |&gt; \n  plot.freq_posterior(T) +\n  ggtitle(\"Natural Progressing Status Log: Posterior of Possible Status\"\n          , stringr::str_replace_all(stringr::str_wrap(\n            width = 80\n            , string = glue::glue(\n              \"\"\n              , \"If &lt;b&gt;every object has passed through&lt;/b&gt; all the status till the end,\"\n              , \"where there are branches, the status the will split; but \"\n              , \" anytime there are only one path forward the certaintiny increase\"\n            )), \"\\n\", \"&lt;br&gt;\")\n  ) + \n  theme(title = ggtext::element_markdown())\n\n\n\n\n\n\n\n\nIs this good enough for us to tell which path is true path? Possibly yes. If a particular status path path rarely occur, they would goes near the 0 probability. Anywhere if there are certainty towards our model we expect probability density to be much thinner and more local;\nDoes this model able to tell which path is real in a sensible way? Yes we are comparing this process of each object moving through each status as a random process of flipping a coin If the heads up, then move from 2 to 3, if tills up, move from 2 -&gt; 4. Eventually we get a posterior tell us, 50% of the time it move 2-&gt;3 and 50% of the time 2-&gt;4.\nIt’s easy to observe status 3, and 4 has is much shorter in comparative to 1, 5. This is because 3 and 4 share the base of any 2 status. Perhaps it sensible to take them into consideration. In graph terminology this is the degree of node (in this case degree of node 2 degree(V(g)[2])).\nSo what if the status does not move at all? Where status could stagnate or could other wise not able to progress forward.\n\n\nIntroduce Noise to Status Flow\nIntroduce noise by replace 5% of legit status to random sample. Using the same log data from above.\n\n\nintroduce noise\n## how can we wrack a data? \n## by insert random status **in between** the line\n## wrack 2% of data by \npossible_status = names(V(state_schema))\n\n## index each row\nlog_ided = log |&gt; \n  ungroup() |&gt; \n  mutate(row_id = row_number()) |&gt; \n  relocate(row_id)\n\n## random slice 0.05 of the row \nrows_to_change=log_ided |&gt; \n  slice_sample(prop = 0.05) |&gt; \n  select(row_id, to)\n\n## replace the to value to random sampled value\nnew_to_s = sample(possible_status, nrow(rows_to_change),replace=T)\nnew_rows = rows_to_change |&gt; \n  mutate(to = new_to_s, .disrupted = T)\n\n## correct the from to something else\nnoisy_log = log_ided |&gt; \n  mutate(.disrupted = F) |&gt; \n  rows_update(new_rows) |&gt; \n  group_by(id) |&gt; \n  mutate(from = lag(to, order_by = logtime)) |&gt; \n  ungroup() |&gt; \n  select(-row_id)\n#&gt; Matching, by = \"row_id\"\n\n\n\n\nexample of a disrupted status\nect_noisy_log = noisy_log |&gt; \n  filter(.disrupted) |&gt; \n  slice_sample(n=1)\nnoisy_log |&gt; semi_join(ect_noisy_log,\"id\")\n\n\n\n  \n\n\n\nIn this example object 495 change from 1 to 2 at 2023-07-16. This is purposefully made to violate the schema design set out before.\n\nKK Layout for Stablisation\nNow the graph may connect to a full graph. But since the percentage is small, it is possible to use Kamada-Kawai layout algorithm to layout based on inverse-frequency of edge occurrence.\nAlthough the graph itself is a near complete graph, when visualizing the graph stablise to one that looks like original graph.\n\n\nScoring from Baysian Model\nThe socring here is very simple. Use 5% as threshold. If there is a “good chance” that the intrinsic probability of edge is less than 5%, this edge is ignorable. Otherwise keep it. The “good chance” means sum posterior below “5 %” p. Here is just a simple rounding up.\n\n\nResult use sum of outflow as base\n\n\nCode\nstopifnot(exists(\"state_schema\"))\n\nvalid_edge = state_schema |&gt; \n  as_edgelist() |&gt; \n  data.frame()\nnames(valid_edge) &lt;- c(\"from\",\"to\")\nvalid_edge[\"valid\"] &lt;- TRUE\n\nnoisy_log.smry = noisy_log |&gt; \n  time_delta.log() |&gt; \n  summary.log()\n## posteror probability\nnoisy_log.posterior = noisy_log.smry |&gt; \n  freq_posterior.log(total_n = from_n) |&gt; \n  attach_score.posterior(valid_edge)\n#&gt; `summarise()` has grouped output by 'from'. You can override using the\n#&gt; `.groups` argument.\n\nPosterior_Score_Scale = function() {\n  return(list(\n    scale_color_manual(\n    values=c(\"True-Pos\"=\"#004D40\",\"True-Neg\"=\"#004D40\",\n             \"False-Pos\"=\"#D81B60\",\"False-Neg\"=\"#D81B60\")\n  ),\n  scale_linetype_manual(\n    values = c(\"True-Pos\"=\"solid\",\"True-Neg\"=\"dashed\",\n             \"False-Pos\"=\"solid\",\"False-Neg\"=\"dashed\")\n  ))\n  )\n}\n\n## graph of path\ndata_as_graph = noisy_log.smry |&gt; \n   tidygraph::as_tbl_graph()\n\ng1 = noisy_log.posterior |&gt; \n  filter(from != to) |&gt; \n  plot.freq_posterior(\n    color = result,\n    linetype=result,\n    scales=\"fixed\") + \n  Posterior_Score_Scale() +\n  theme_minimal() +\n  theme(\n      axis.text.x = element_text(angle=90)\n    , panel.grid.major.y = element_blank()\n    , panel.grid.minor.y = element_blank()\n    , panel.grid.major.x = element_blank()\n    , panel.grid.minor.x = element_blank()\n    # , legend.position = \"top\"\n  ) +\n  ggtitle(\"posterior\") \n\nset.seed(101)\n\ng2 = data_as_graph |&gt; \n  mutate(name = paste(\"#\",name)) |&gt; \n  ggraph(layout =\"kk\", weights = 1/n) +\n  geom_edge_fan(\n    aes(label = n, width = n)\n  , arrow = arrow(type=\"open\")\n  , alpha = 0.25\n  , position = \"jitter\"\n  , color = \"#004D40\"\n  , n = 100\n  ) +\n  geom_node_label(aes(label = name), color = \"white\", fill = \"black\") +\n  # ggtitle(\"Sample from a Custom Defined Status Flow Chart\") +\n  # coord_equal() +\n  scale_edge_width(range = c(0.1,2.5)) +\n  theme(legend.position = \"none\") + \n  ggtitle(\"edge count\")\n\n(g2 | g1) + \n  plot_layout(ncol = 2, guides =\"collect\") +\n  plot_annotation(title = \"After introducing 5% of noise\") + \n  theme(\n    legend.position = \"top\",legend.direction = 'vertical'\n    )\n\n\n\n\n\n\n\n\n\nMy favorite quote from the book “rethink statistics”: &gt; Once your model produces a posterior distribution, the models work is done. But your work has just begun\nThe model can helping us of cutting off noisy edges but we need something similar to a confidence interval.\n1 -&gt; 3, 1 -&gt; 4 would be highly unlikely. The tricky part is 5 -&gt; 2 or 3 -&gt; 5. These two path are clearly invalid, but the Bayesian has output it to be lower certaintly path. Seems the model is bad where there is a terminal node. This is because from 5, it can still go to 1 or 3. So perhaps, instead of simply sum total out flow source, we need count how many flow enter the node.\n\n\nResult use sum of inflow as base:\n\n\nCode\nsummary.log = function(change_prequency) {\n  income_n.df = change_prequency |&gt; \n    ungroup() |&gt; \n    group_by(to) |&gt; \n    summarise(income_n = n()) |&gt; \n    rename(from = to)\n  change_prequency |&gt;\n      ungroup() |&gt; \n      mutate(total_n = n()) |&gt; \n      group_by(from) |&gt; mutate(from_n = n()) |&gt; ungroup() |&gt; \n      group_by(to) |&gt; mutate(to_n = n()) |&gt; ungroup() |&gt; \n      group_by(from, to) |&gt; \n      summarise(\n        median_td = median(time_delta),\n        sd = sd(time_delta),\n        total_n = first(total_n),\n        from_n = first(from_n),\n        to_n = first(to_n),\n        n = n(),\n        .groups = \"drop\"\n      ) |&gt; \n    left_join(income_n.df, by = \"from\")\n}\n\nnoisy_log.smry = noisy_log |&gt; \n  time_delta.log() |&gt; \n  summary.log()\nnoisy_log.posterior = noisy_log.smry |&gt; \n  freq_posterior.log(total_n = income_n) |&gt; \n  attach_score.posterior(valid_edge)\n#&gt; `summarise()` has grouped output by 'from'. You can override using the\n#&gt; `.groups` argument.\n\ng3 = noisy_log.posterior |&gt; \n  filter(from != to) |&gt; \n  plot.freq_posterior(\n    color = result,\n    linetype=result,\n    scales=\"fixed\") + \n  Posterior_Score_Scale() + \n  theme_minimal() +\n  theme(\n      axis.text.x = element_text(angle=90)\n    , panel.grid.major.y = element_blank()\n    , panel.grid.minor.y = element_blank()\n    , panel.grid.major.x = element_blank()\n    , panel.grid.minor.x = element_blank()\n  ) + \n  ggtitle(\"posterior\")\n\nset.seed(101)\ng4 = noisy_log.smry |&gt; \n  tidygraph::as_tbl_graph() |&gt; \n  mutate(name = paste(\"#\",name)) |&gt; \n  ggraph(layout =\"kk\", weights = 1/n) +\n  geom_edge_fan(\n    aes(label = n, width = n)\n  , arrow = arrow(type=\"open\")\n  , alpha = 0.25\n  , position = \"jitter\"\n  , color = \"#004D40\"\n  , n = 100\n  ) +\n  geom_node_label(aes(label = name), color = \"white\", fill = \"black\") +\n  # ggtitle(\"Sample from a Custom Defined Status Flow Chart\") +\n  # coord_equal() +\n  ggtitle(\"edge cout\") +\n  scale_edge_width(range = c(0.1,2.5)) +\n  theme(legend.position = \"none\")\n\n(g4 | g3) + \n  plot_layout(ncol = 2) +\n  plot_annotation(title = \"5% Noise and use incoming flow as base\")\n#&gt; Warning: Removed 16000 rows containing missing values or values outside the scale range\n#&gt; (`geom_line()`).\n\n\n\n\n\n\n\n\n\nAlthough the result to terminal node has improved, the result for source node has worse off (1-&gt;4, 1-&gt;5). This is because the amout of flow back to 1 is already scares (only 15 total edge).\nOverall, by using sum of incoming edge, the number of false-positive edges reduced but this is rather artificial. The status flow schema we defined here has only one source node.\n“1-&gt;2” has disappeared from posterior because the number of edge pointing to “#1” is lower than the amout going out. So “#1” going out to “#2” is simply impossible. Knowing this we can probability fix this simply add difference out out-flow from 1.\n[tbc]"
  },
  {
    "objectID": "2024/09-06 Sandy-Status/sandy-status/vignettes/stochestic_status_flow.html#referencereading-list",
    "href": "2024/09-06 Sandy-Status/sandy-status/vignettes/stochestic_status_flow.html#referencereading-list",
    "title": "Stochastic Status Flow Experiment - Part 1",
    "section": "Reference/Reading List",
    "text": "Reference/Reading List\n\nMcElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.: Thank this book for everything I understand about Baysian inference.\nThomas Lin Pedersen (2023)’s Package {{ggraph}} is amazing, but I add this link for use {{tweenr}} in the future.\n{{bupar}} Package for business process mining of course every time you have a new idea, chances are someone else has already invented (and commercialized) them.\nResearch by Dr.Mieke Jans: Dr Mieke Jans is a contributor of {{bupar}}. Her papers maybe of interests."
  },
  {
    "objectID": "2024/01-01 Deep Learning Py/01-12.html",
    "href": "2024/01-01 Deep Learning Py/01-12.html",
    "title": "Visualise Matrix Transformation as Paper Folding",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n%config InlineBackend.figure_format='retina'\n# to change default colormap\nplt.rcParams[\"image.cmap\"] = \"Set3\"\n# to change default color cycle\nmyC= plt.cm.tab20b.colors\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=myC[:1]+myC[2:])\nmatplotlib.rcParams['figure.figsize'] = (20, 10)\nplt.style.use('seaborn-v0_8-whitegrid')\n\nm1 = np.array([[1,2],\n            [3,4],\n            [1,1],\n            [5,4]])#two dimenstional tensor\n\nweight = np.array([0.5,0.5])#what tensor will try to guess\nnp.dot(m1,weight)#if this were a scala problem this the value will be used for relu activation\n\n\narray([1.5, 3.5, 1. , 4.5])\nWhat maybe powerful is matrix will changes shapes. Would it matter what input shape I choose?\nCode\nprint('weight2 is')\nweight2 = np.array([[0.5,0.1,0.3],\n                    [0.5,0.2,0.3]])#you can expand infinitly horisentally\n\nprint(weight2)\nprint('Using dot product we expand the width of the matrix to 3')\nprint(np.dot(m1,weight2))\n\nprint('Use another product to trave the dimension back ')\nweight3 = np.array([[0.1,0.2],\n                    [0.9,0.1],\n                    [0.2,0.5]])\nm3 = np.dot(np.dot(m1,weight2),weight3)\nprint(m3)\nprint(\"notice how the first matrix expand matrix dimention to 3 and the second to 2;\")\n\n\nweight2 is\n[[0.5 0.1 0.3]\n [0.5 0.2 0.3]]\nUsing dot product we expand the width of the matrix to 3\n[[1.5 0.5 0.9]\n [3.5 1.1 2.1]\n [1.  0.3 0.6]\n [4.5 1.3 2.7]]\nUse another product to trave the dimension back \n[[0.78 0.8 ]\n [1.76 1.86]\n [0.49 0.53]\n [2.16 2.38]]\nnotice how the first matrix expand matrix dimention to 3 and the second to 2;\nCode\nweight2 = np.array(\n    [[0.5,0.5,0.5],\n    [0.5,1,0.41]])\nweight3 = np.array(\n    [[1, 0.2],\n    [0.9,0.1],\n    [0.9,0.7]])\n# notice how the th\nweight = np.dot(weight2, weight3)\nprint(weight)\n\ndef my_transformation(m1, w):\n    m3=m1.dot(w)\n    return m3\n\ndef plot_transformation(m1, m3, ax=None):\n    if ax is None:\n        ax=plt.gca()\n    for i in range(m1.shape[0]):\n        x=[m1[i,0],m3[i,0]]\n        y=[m1[i,1],m3[i,1]]\n        ax.plot(x,y, 'k-',alpha=0.8,zorder=5)\n    \n    ax.scatter(m1[:,0],m1[:,1], zorder=6,alpha=1)\n    ax.scatter(m3[:,0],m3[:,1],zorder=7, alpha=0.7)\n\n\n[[1.4   0.5  ]\n [1.769 0.487]]\nCode\n# This perhaps is really usefull to test to see what you neuro network is doing:\nrotator = np.array([\n    [0,1],\n    [1,0]\n])\n\nfigure,ax=plt.subplots(figsize=(5,5))\n\nm1=np.random.random((50,2))\nm2=my_transformation(m1, rotator)\nplot_transformation(m1,m2,ax=ax)"
  },
  {
    "objectID": "2024/01-01 Deep Learning Py/01-12.html#create-a-contour-visualiualisation-of-tendency",
    "href": "2024/01-01 Deep Learning Py/01-12.html#create-a-contour-visualiualisation-of-tendency",
    "title": "Visualise Matrix Transformation as Paper Folding",
    "section": "Create a Contour visualiualisation of tendency",
    "text": "Create a Contour visualiualisation of tendency\nNext I want to visualise the tendency the matrix do for sampled at each points… I found two visual idiom: meshgird and quiver.\n\n\nCode\n\nv1=np.arange(-2, 2, .5)\nv2=np.arange(-2, 2, .5)\nx,y = np.meshgrid(v1, v2)\n\n# to help understand what np.meshgird does. \n# It helps creates cross from two vector\nassert x.shape==y.shape\nassert len(v1)==x.shape[1]\nassert x.size == len(v1) * len(v2)\n\nplt.figure(figsize=(3,3))\nplt.plot(x,y,marker='.',linestyle='none',color='k')\n\nx,y = np.meshgrid(v1, v2)\nz=np.sqrt(x**2 + y**2)\nplt.contourf(x,y,z)\nplt.colorbar()\nplt.axis('scaled')\n\n# z = x*np.exp(-x**2 - y**2)\n# v, u = np.gradient(z, .2, .2)\n# fig, ax = plt.subplots()\n# q = ax.quiver(x,y,u,v)\n# plt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncord=np.vstack((x.reshape(1,x.size),y.reshape(1,y.size))).T\nplt.figure(figsize=(3,3))\nplt.plot(cord[:,0],cord[:,1],marker='.',linestyle=\"none\")"
  },
  {
    "objectID": "2024/01-01 Deep Learning Py/index.html",
    "href": "2024/01-01 Deep Learning Py/index.html",
    "title": "Deep Learning with Python Summary Page",
    "section": "",
    "text": "Ledgend:\n\nFirst Lab Explore linear matrix transformation effect for a two dimension array with two feature (just x and y)\nSecond Follow the Deep Learning Book Chapter 3 use example of IBM sentient analysis."
  },
  {
    "objectID": "2024/01-01 Setup Delta Lake/01.html",
    "href": "2024/01-01 Setup Delta Lake/01.html",
    "title": "Setup Spark and and delta Lake",
    "section": "",
    "text": "image"
  },
  {
    "objectID": "2024/01-01 Setup Delta Lake/01.html#basic-tutorial",
    "href": "2024/01-01 Setup Delta Lake/01.html#basic-tutorial",
    "title": "Setup Spark and and delta Lake",
    "section": "",
    "text": "image"
  },
  {
    "objectID": "2024/01-01 Setup Delta Lake/01.html#code",
    "href": "2024/01-01 Setup Delta Lake/01.html#code",
    "title": "Setup Spark and and delta Lake",
    "section": "Code",
    "text": "Code\n\n# set up a project\nimport pyspark\nfrom delta import *\n\nbuilder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n\nspark = configure_spark_with_delta_pip(builder).getOrCreate()\n\n\n# create a table (run once)\ndata = spark.range(0, 5)\ndata.write.format(\"delta\").save(\"/tmp/delta-table\")\n# run again creates following error: \n#AnalysisException: [DELTA_PATH_EXISTS] Cannot write to already existent path file\n\n\n# Read file from path\ndf = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\ndf.show()\n\n+---+\n| id|\n+---+\n|  2|\n|  3|\n|  4|\n|  0|\n|  1|\n+---+\n\n\n\n\n# upate table data\ndata = spark.range(5, 10)\n# brute force update\ndata.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n\n\n# conidtional update\nfrom delta.tables import *\nfrom pyspark.sql.functions import *\n\ndeltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")\n\n# Update every even value by adding 100 to it\ndeltaTable.update(\n  condition = expr(\"id % 2 == 0\"),\n  set = { \"id\": expr(\"id + 100\") })\n\n# Delete every even value\ndeltaTable.delete(condition = expr(\"id % 2 == 0\"))\n\n# Upsert (merge) new data\nnewData = spark.range(0, 20)\n\ndeltaTable.alias(\"oldData\") \\\n  .merge(\n    newData.alias(\"newData\"),\n    \"oldData.id = newData.id\") \\\n  .whenMatchedUpdate(set = { \"id\": col(\"newData.id\") }) \\\n  .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") }) \\\n  .execute()\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n| 15|\n| 16|\n| 17|\n| 18|\n| 19|\n+---+\n\n\n\n\n# time travel\ndf = spark.read.format(\"delta\") \\\n  .option(\"versionAsOf\", 0) \\\n  .load(\"/tmp/delta-table\")\n\ndf.show()\n\n+---+\n| id|\n+---+\n|  2|\n|  3|\n|  4|\n|  0|\n|  1|\n+---+\n\n\n\n\n# perform a live streaming\nstreamingDf = spark.readStream.format(\"rate\").load()\n\nstream = streamingDf \\\n  .selectExpr(\"value as id\") \\\n  .writeStream.format(\"delta\") \\\n  .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\\n  .start(\"/tmp/delta-table\")\n\n23/12/28 19:08:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n23/12/28 19:08:27 ERROR NonFateSharingFuture: Failed to get result from future\nscala.runtime.NonLocalReturnControl\n23/12/28 19:08:37 ERROR NonFateSharingFuture: Failed to get result from future\nscala.runtime.NonLocalReturnControl\n23/12/28 19:08:47 ERROR NonFateSharingFuture: Failed to get result from future\nscala.runtime.NonLocalReturnControl\n23/12/28 19:08:57 ERROR NonFateSharingFuture: Failed to get result from future\nscala.runtime.NonLocalReturnControl\n23/12/28 19:09:07 ERROR NonFateSharingFuture: Failed to get result from future\nscala.runtime.NonLocalReturnControl\n\n\n\nstream.stop()"
  },
  {
    "objectID": "2024/01-01 Setup Delta Lake/01.html#build-in-data",
    "href": "2024/01-01 Setup Delta Lake/01.html#build-in-data",
    "title": "Setup Spark and and delta Lake",
    "section": "Build-in data?",
    "text": "Build-in data?\n\nspark.read.json(\"logs.json\")"
  },
  {
    "objectID": "2024/01-28 Geopandas and Altair Vega/vega-altair.html",
    "href": "2024/01-28 Geopandas and Altair Vega/vega-altair.html",
    "title": "Try out vega altairs geo spatial visualisation",
    "section": "",
    "text": "import altair as alt\nfrom vega_datasets import data\n\nboroughs = alt.topo_feature(data.londonBoroughs.url, 'boroughs')\ntubelines = alt.topo_feature(data.londonTubeLines.url, 'line')\ncentroids = data.londonCentroids.url\nbackground = alt.Chart(boroughs, width=700, height=500).mark_geoshape(\n    stroke='white',\n    strokeWidth=2\n).encode(\n    color=alt.value('#eee'),\n)\nlabels = alt.Chart(centroids).mark_text().encode(\n    longitude='cx:Q',\n    latitude='cy:Q',\n    text='bLabel:N',\n    size=alt.value(8),\n    opacity=alt.value(0.6)\n).transform_calculate(\n    \"bLabel\", \"indexof (datum.name,' ') &gt; 0  ? substring(datum.name,0,indexof(datum.name, ' ')) : datum.name\"\n)\n\nline_scale = alt.Scale(domain=[\"Bakerloo\", \"Central\", \"Circle\", \"District\", \"DLR\",\n                               \"Hammersmith & City\", \"Jubilee\", \"Metropolitan\", \"Northern\",\n                               \"Piccadilly\", \"Victoria\", \"Waterloo & City\"],\n                       range=[\"rgb(137,78,36)\", \"rgb(220,36,30)\", \"rgb(255,206,0)\",\n                              \"rgb(1,114,41)\", \"rgb(0,175,173)\", \"rgb(215,153,175)\",\n                              \"rgb(106,114,120)\", \"rgb(114,17,84)\", \"rgb(0,0,0)\",\n                              \"rgb(0,24,168)\", \"rgb(0,160,226)\", \"rgb(106,187,170)\"])\n\nlines = alt.Chart(tubelines).mark_geoshape(\n    filled=False,\n    strokeWidth=2\n).encode(\n    alt.Color('id:N')\n        .title(None)\n        .legend(orient='bottom-right', offset=0)\n        .scale(line_scale)\n)\n\nbackground + labels + lines\nimport altair as alt\nfrom vega_datasets import data\nimport geopandas as gpd\n\nurl = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\ngdf_ne = gpd.read_file(url)  # zipped shapefile\ngdf_ne = gdf_ne[[\"NAME\", \"CONTINENT\", \"POP_EST\", 'geometry']]\nalt.Chart(gdf_ne).mark_geoshape(\n    fill='lightgrey',stroke='white',strokeWidth=0.5\n)"
  },
  {
    "objectID": "2024/01-28 Geopandas and Altair Vega/vega-altair.html#geo-pandas",
    "href": "2024/01-28 Geopandas and Altair Vega/vega-altair.html#geo-pandas",
    "title": "Try out vega altairs geo spatial visualisation",
    "section": "Geo Pandas",
    "text": "Geo Pandas\n\nimport geopandas\nfrom geodatasets import get_path\n\npath_to_data = get_path(\"nybb\")\ngdf = geopandas.read_file(path_to_data)\n\n\nimport matplotlib.pyplot as plt\n\n%config InlineBackend.figure_format='retina'\n\nax=plt.gca()\ngdf.plot(ax=ax)\ngdf.centroid.plot(ax=ax,color='white')\n\n\n\n\n\n\n\n\n\ngdf.explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "2024/02-09 Kaggle Competition/kaggle-2024-credit-risks/fl-02-10.html",
    "href": "2024/02-09 Kaggle Competition/kaggle-2024-credit-risks/fl-02-10.html",
    "title": "Explore Data in train_static",
    "section": "",
    "text": "import pandas as pd\nimport os\nfrom directory_tree import display_tree\nfrom sklearn.metrics import roc_auc_score\nfrom pathlib import Path\nimport duckdb\nimport ibis\nfrom ibis import _\nfrom ibis import selectors as s\nimport re\nfrom importlib import reload \nfrom IPython.display import display, HTML\n# reload(tools)\n\nfrom tools import FeatureDefinition, pretty_display\nimport altair as alt\nfrom functools import reduce\n\n\nDATA_DIR = 'home-credit-credit-risk-model-stability/parquet_files/train'\np = Path(DATA_DIR)\n\nibis.options.interactive=True\n\n# load data into directory\ndata_files=list(p.glob('*.parquet'))#load_all data\nds = {re.sub('.parquet','',f.name):ibis.read_parquet(f) for f in data_files}\n\nWhy train_static? Please this dataset don’t seems to have cardinality problems.\n\nfrom importlib import reload \nfrom IPython.display import display, HTML\n# reload(tools)\n\nfrom tools import FeatureDefinition, pretty_display\n# lookup column definition here if you don't know what \nintersting_cols = [\n    'bankacctype_710L',\n    'credtype_322L','disbursementtype_67L',\n    'inittransactioncode_186L','lastst_736L','paytype1st_925L','paytype_783L',\n    'twobodfilling_608L','typesuite_864L'\n]\npretty_display(\n    FeatureDefinition().lookup_col(intersting_cols)\n)\n\nVariable.str.contains(\"bankacctype_710L\") or Variable.str.contains(\"credtype_322L\") or Variable.str.contains(\"disbursementtype_67L\") or Variable.str.contains(\"inittransactioncode_186L\") or Variable.str.contains(\"lastst_736L\") or Variable.str.contains(\"paytype1st_925L\") or Variable.str.contains(\"paytype_783L\") or Variable.str.contains(\"twobodfilling_608L\") or Variable.str.contains(\"typesuite_864L\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nFile\n\n\n\n\n0\nbankacctype_710L\nType of applicant's bank account.\n[train_static_0_0, train_static_0_1]\n\n\n1\ncredtype_322L\nType of credit.\n[train_static_0_0, train_static_0_1]\n\n\n2\ndisbursementtype_67L\nType of disbursement.\n[train_static_0_0, train_static_0_1]\n\n\n3\ninittransactioncode_186L\nTransaction type of the initial credit\ntransaction.\n[train_static_0_0, train_static_0_1]\n\n\n4\nlastst_736L\nStatus of the client's previous credit\napplication.\n[train_static_0_0, train_static_0_1]\n\n\n5\npaytype1st_925L\nType of first payment of the client.\n[train_static_0_0, train_static_0_1]\n\n\n6\npaytype_783L\nType of payment.\n[train_static_0_0, train_static_0_1]\n\n\n7\ntwobodfilling_608L\nType of application process.\n[train_static_0_0, train_static_0_1]\n\n\n8\ntypesuite_864L\nPersons accompanying the client during the loan\napplication process.\n[train_static_0_0, train_static_0_1]\n\n\n\n\n\n\n\nTime Sensitivity?\nMaybe not to include time-features in as training dataset.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tools\n\nd=(\n    ds.get('train_base')\n    .mutate(date_decision=_.date_decision.to_timestamp('%Y-%m-%d').date())\n    .mutate(dm=_.WEEK_NUM)\n    .group_by([_.dm, _.target])\n    .aggregate(n=_.count())\n    .to_pandas()\n)\n# d\n\nimport altair as alt\nc0=alt.Chart(\n    ds.get('train_base')\n        .mutate(date_decision=_\n                .date_decision\n                .to_timestamp('%Y-%m-%d').date()\n                .truncate('M')\n                )\n        .group_by([_.date_decision])\n        .aggregate(n=_.count())\n        .to_pandas()\n    ,\n    title=['Loan Approval Volumn',\n           'There is a sharp drope during covid'\n        ],height=150, width=670\n    ).mark_line(color='midnightblue').encode(\n        x=alt.X('date_decision',title='Decision Date'),\n        y=alt.Y('n',title='Loan Approved')\n)\nc1=alt.Chart(d,title='Volumn of Approval and Deafult').mark_area().encode(\n    x=alt.X('dm',title='Week Number'),\n    y=alt.Y('n',title='Number of Case'),\n    color='target:N'\n)\nc2=alt.Chart(d,title='Propotion of Default Overtime').mark_area().encode(\n    x=alt.X('dm',title='Week Number'),\n    y=alt.Y('n',title='Propotion of Case').stack('normalize'),\n    color='target:N'\n)\nc0 & (c1  | c2)\n\n\n\n\n\n\n\n\nreload(tools)\nfrom tools import FeatureDefinition\npretty_display(\n    FeatureDefinition().lookup_dsc('change')\n    .query('File==\"train_static_0_0\"')\n)\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nFile\nDescription\n\n\n\n\n415\nequalitydataagreement_891L\ntrain_static_0_0\nFlag indicating sudden changes in client's social-\ndemographic data (e.g. education, family status,\nhousing type).\n\n\n417\nequalityempfrom_62L\ntrain_static_0_0\nFlag indicating a sudden change in the client's\nlength of employment.\n\n\n\n\n\n\n\n\nExplore Categorical Variables\n\nfd=FeatureDefinition()\nintersting_cols = [\n    'equalitydataagreement_891L',\n    'equalityempfrom_62L',\n    'bankacctype_710L',\n    'credtype_322L','disbursementtype_67L',\n    'inittransactioncode_186L','lastst_736L','paytype1st_925L','paytype_783L',\n    'twobodfilling_608L','typesuite_864L',\n    'lastrejectreasonclient_4145040M'\n]\n# pretty_display(\n#     fd.lookup_col(intersting_cols)\n# )\nd2=(\n    ibis.union(\n        ds.get('train_static_0_0'),\n        ds.get('train_static_0_1')\n    )\n    .select(s.contains(intersting_cols + ['case_id']))\n    .join(ds.get('train_base'), ['case_id'])\n)\nC = []\nfor c in intersting_cols:\n    # display(d2.select(c,'target').head(1))\n    description = fd.lookup_col(c).Description.str.wrap(30).loc[0]\n    # print(description)\n    d=(d2\n        .group_by([c,'target'])\n        .aggregate(n=_.count())\n        .to_pandas())\n    ct1=alt.Chart(\n        d\n    ).mark_bar().encode(\n        y=alt.Y(c + ':N', title=description.split('\\n')),\n        x=alt.X('n:Q',title=''),\n        color='target:N'\n    )\n    ct2=alt.Chart(\n        d,width=100\n    ).mark_bar().encode(\n        y=alt.Y(c + ':N', title='', axis=None),\n        x=alt.X('n:Q',title='').stack('normalize'),\n        color='target:N',\n    )\n    ct=ct1 | ct2\n    C += [ct]\nreduce(alt.vconcat, C).configure_axisY(\n    titleAngle=0,\n    titleAlign=\"left\",\n    titleY=10,\n    titleX=-200,\n    titleColor='#404040',\n    titleFontWeight='lighter'\n).properties(\n    title=[\n        \"Categorical Variable Appears in Static Dataset\",\n        \"Nothing distinct by propotion except 'Status of the client's previous application'\"\n        ]\n)\n\n\n\n\n\n\n\n\npretty_display(fd\n               .lookup_tbl('static_0_0')\n               .query('Variable.str.endswith(\"M\")')\n               .reset_index(drop=True))\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nFile\nDescription\n\n\n\n\n0\nlastapprcommoditycat_1041M\ntrain_static_0_0\nCommodity category of the last loan applications\nmade by the applicant.\n\n\n1\nlastapprcommoditytypec_5251766M\ntrain_static_0_0\nCommodity type of the last application.\n\n\n2\nlastcancelreason_561M\ntrain_static_0_0\nCancellation reason of the last application.\n\n\n3\nlastrejectcommoditycat_161M\ntrain_static_0_0\nCategory of commodity in the applicant's last\nrejected application.\n\n\n4\nlastrejectcommodtypec_5251769M\ntrain_static_0_0\nCommodity type of the last rejected application.\n\n\n5\nlastrejectreason_759M\ntrain_static_0_0\nReason for rejection on the most recent rejected\napplication.\n\n\n6\nlastrejectreasonclient_4145040M\ntrain_static_0_0\nReason for the client's last loan rejection.\n\n\n7\npreviouscontdistrict_112M\ntrain_static_0_0\nContact district of the client's previous approved\napplication.\n\n\n\n\n\n\n\nd=(\n    ds.get('train_static_0_0')\n    .join(ds.get('train_base'),'case_id')\n    .select(s.endswith('M'), _.target, _.case_id)\n    .drop('WEEK_NUM')\n    # .pivot_longer(\n    #     s.endswith('M')\n    # )\n    # .group_by([_.name,_.value])\n    # .aggregate(n=_.count())\n)\n\n\n(d\n.select(c, 'target')\n.group_by([s.contains(c),s.contains('target')])\n.aggregate(n=_.count())\n.to_pandas())\n\n\n\n\n\n\n\n\n\npreviouscontdistrict_112M\ntarget\nn\n\n\n\n\n0\nP6_35_77\n0\n2506\n\n\n1\nP41_138_103\n0\n1766\n\n\n2\nP54_133_26\n0\n8831\n\n\n3\nP197_47_166\n0\n34040\n\n\n4\nP111_135_181\n0\n11957\n\n\n...\n...\n...\n...\n\n\n393\nP159_160_144\n1\n27\n\n\n394\nP217_60_135\n1\n16\n\n\n395\nP78_30_175\n1\n26\n\n\n396\nP7_110_89\n1\n18\n\n\n397\nP31_42_128\n1\n7\n\n\n\n\n398 rows × 3 columns\n\n\n\n\n\nfd=FeatureDefinition()\nmcols = [i for i in d.columns if i not in ['target', \"case_id\"]]\nC=[]\nfor c in mcols:\n    data=(d\n        .select(c, 'target')\n        .group_by([s.contains(c),s.contains('target')])\n        .aggregate(n=_.count())\n        .to_pandas())\n    description=fd.lookup_col(c).Description.str.wrap(30).loc[0]\n    cl1=(\n        alt\n        .Chart(data)\n        .mark_bar()\n        .encode(\n            x='n:Q',\n            y=alt.Y(c + ':N',title=description.split('\\n')),\n            color='target:N'\n        )\n    )\n    cl2=(\n        alt\n        .Chart(data,width=100)\n        .mark_bar(\n        )\n        .encode(\n            x=alt.X('n:Q',title='').stack('normalize'),\n            y=alt.Y(c + ':N',title='',axis=None),\n            color='target:N'\n        )\n    )\n    C+=[cl1 | cl2]\nCH=reduce(alt.vconcat,C)\nCH.configure_axisY(\n    titleAngle=0,\n    titleAlign=\"left\",\n    titleY=10,\n    titleX=-250,\n    titleColor='#404040',\n    titleFontWeight='lighter'\n).properties(\n    title='All Category Variable'\n)\n\n\n\n\n\n\n\n\n# alt.data_transformers.enable(\"vegafusion\")\n# alt.Chart(d.to_pandas()).transform_fold(\n#     [i for i in d.columns if i not in ['target', \"case_id\"]]\n# ).mark_bar().encode(\n#     x='count():Q',\n#     y='value:N',\n#     column='key:N'\n# )\nd\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┓\n┃ lastapprcommoditycat_1041M ┃ lastapprcommoditytypec_5251766M ┃ lastcancelreason_561M ┃ lastrejectcommoditycat_161M ┃ lastrejectcommodtypec_5251769M ┃ lastrejectreason_759M ┃ lastrejectreasonclient_4145040M ┃ previouscontdistrict_112M ┃ target ┃ case_id ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━┩\n│ string                     │ string                          │ string                │ string                      │ string                         │ string                │ string                          │ string                    │ int64  │ int64   │\n├────────────────────────────┼─────────────────────────────────┼───────────────────────┼─────────────────────────────┼────────────────────────────────┼───────────────────────┼─────────────────────────────────┼───────────────────────────┼────────┼─────────┤\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │       0 │\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │       2 │\n│ a55475b1                   │ a55475b1                        │ P94_109_143           │ a55475b1                    │ a55475b1                       │ P94_109_143           │ a55475b1                        │ a55475b1                  │      0 │       3 │\n│ a55475b1                   │ a55475b1                        │ P94_109_143           │ a55475b1                    │ a55475b1                       │ P94_109_143           │ a55475b1                        │ a55475b1                  │      0 │       6 │\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │       7 │\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │       8 │\n│ a55475b1                   │ a55475b1                        │ P73_130_169           │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │      10 │\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │      11 │\n│ a55475b1                   │ a55475b1                        │ a55475b1              │ a55475b1                    │ a55475b1                       │ a55475b1              │ a55475b1                        │ a55475b1                  │      0 │      12 │\n│ a55475b1                   │ a55475b1                        │ P94_109_143           │ a55475b1                    │ a55475b1                       │ P94_109_143           │ a55475b1                        │ a55475b1                  │      0 │      13 │\n│ …                          │ …                               │ …                     │ …                           │ …                              │ …                     │ …                               │ …                         │      … │       … │\n└────────────────────────────┴─────────────────────────────────┴───────────────────────┴─────────────────────────────┴────────────────────────────────┴───────────────────────┴─────────────────────────────────┴───────────────────────────┴────────┴─────────┘"
  },
  {
    "objectID": "2024/03-15-Time-Series/r-book.html",
    "href": "2024/03-15-Time-Series/r-book.html",
    "title": "Use R in a jupyter notebook?",
    "section": "",
    "text": "This notebook is writtten in jupyter!\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.0     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv lubridate 1.9.2     v tibble    3.1.8\nv purrr     1.0.1     v tidyr     1.3.0\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ndf = readr::read_csv('data/store-sales-time-series-forecasting/train.csv')\n\nRows: 3000888 Columns: 6\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (1): family\ndbl  (4): id, store_nbr, sales, onpromotion\ndate (1): date\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndf |&gt; head()\n\n\n\nA tibble: 6 x 6\n\n\nid\ndate\nstore_nbr\nfamily\nsales\nonpromotion\n\n\n&lt;dbl&gt;\n&lt;date&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0\n2013-01-01\n1\nAUTOMOTIVE\n0\n0\n\n\n1\n2013-01-01\n1\nBABY CARE\n0\n0\n\n\n2\n2013-01-01\n1\nBEAUTY\n0\n0\n\n\n3\n2013-01-01\n1\nBEVERAGES\n0\n0\n\n\n4\n2013-01-01\n1\nBOOKS\n0\n0\n\n\n5\n2013-01-01\n1\nBREAD/BAKERY\n0\n0"
  },
  {
    "objectID": "2024/05-19 Factor Analysis/lda.html",
    "href": "2024/05-19 Factor Analysis/lda.html",
    "title": "Linear Discriminant Analysis",
    "section": "",
    "text": "Other name: Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher’s linear discriminant.\nFisher is a bigolgist who studies species of birds. (this story is hypothtical) Think in Fisher’s perspective I want to classify bird without measuring ratio…\n\n\nSeems sci-learn’s LDA don’t like this datasets\n\n\ngenerate two linear correlated group\n%config InlineBackend.figure_format='retina'\n# %matplotlib widget\n%matplotlib inline\n# change to widget for interactive\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\n\nsample_size = 100\nx1 = np.random.normal(scale = 1, size = sample_size)\ny1 = x1 * 0.2 + 3 + np.random.normal(scale=0.1, size=sample_size)\nz1 = 0.5 * x1 + y1 + np.random.normal(scale=0.1, size=sample_size)\n\nx2 = np.random.normal(scale = 1, size = sample_size)\ny2 = x2 * 0.6 + 2 + np.random.normal(scale=0.1, size=sample_size)\nz2 = 0.7 * x2 + 0.1 * y2 + np.random.normal(scale=0.1, size=sample_size)\n\nx3 = np.random.normal(scale = 1, size = sample_size)\ny3 = x3 + 1 + np.random.normal(scale=0.1, size=sample_size)\nz3 = 0.5 * x3 + 0.5 * y2 + np.random.normal(scale=0.1, size=sample_size)\n\nx = np.hstack((x1,x2\n               ,x3\n               ))\ny = np.hstack((y1,y2\n               ,y3\n               ))\nz = np.hstack((z1,z2\n               ,z3\n               ))\ng = np.hstack( (np.repeat(1,sample_size)\n              , np.repeat(0,sample_size)\n              , np.repeat(2,sample_size)\n              ))\n\nfigure = plt.figure()\n\nax=figure.add_subplot(111, projection=\"3d\")\nax.scatter(x,y,z, c=g)\nax.set_title(\"Project this Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(n_components=2)\n\nm = np.vstack((x,y,z)).T\nX_2d = lda.fit(m,g).transform(m)\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(X_2d[:,0],X_2d[:,1],c=g)\nax.set_title(\"LDA projection\")\n\nText(0.5, 1.0, 'LDA projection')\n\n\n\n\n\n\n\n\n\nThe effect of LDA or PCA is similar to seeing a 3D object with one eye: projection of higher dimension into one dimension.\n\n\n\n\nLinear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels…\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\n## fit model with principle component\npca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)\n\n## fit model with LDA\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_r2 = lda.fit(X, y).transform(X)\n\n## plog graph\nfig, ax = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\nax1=ax[0]\nax2=ax[1]\n\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\nlw = 2\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    ax1.scatter(\n        X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\n    )\nax1.legend(loc=\"best\", shadow=False, scatterpoints=1)\nax1.set_title(\"PCA of IRIS dataset\")\n\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    ax2.scatter(\n        X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name\n    )\nax2.legend(loc=\"best\", shadow=False, scatterpoints=1)\nax2.set_title(\"LDA of IRIS dataset\")\n\nfig.show()\n\n/var/folders/r5/1cdq52mn21zdnqzl0fvp44zw0000gn/T/ipykernel_98332/2715178903.py:43: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n\n\n\n\nThis following section it maybe useful for popup a few images of iris:\n\nFrom an artist’s perspective the differences between these iris’s are dimensionality of petal and sepal. The latend varible here is probably can view as (how square are these two petal)\n\n\n\nThis blog post thanks to Andy Jones\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate data\nmu1 = [0, 0]\nmu2 = [1, 2]\ncov = [[1, 0], [0, 1]]\nn1 = 500\nn2 = 50\nn = n1 + n2\nx1 = np.random.multivariate_normal(mean=mu1, cov=cov, size=n1)\nx2 = np.random.multivariate_normal(mean=mu2, cov=cov, size=n2)\nx = np.vstack([x1, x2])\ny = np.concatenate([np.repeat(1, n1), np.repeat(2, n2)])\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(x[:, 0], x[:, 1], c=y)\n\n\n\n\n\n\n\n\n\npi_hat_1 = n1 / n ## baysian prior also frequency of target\npi_hat_2 = n2 / n ## baysian prior also frequency of target\n\n## avg,expected_value,or position of normal\nmu_hat_1 = 1 / n1 * np.sum(x1, axis=0) #expected value for group1\nmu_hat_2 = 1 / n2 * np.sum(x2, axis=0) #expected value for group2\n\n# variance, or width of normal function\ncov_hat_1 = 1 / (n1 - 1) * np.matmul((x1 - mu_hat_1).T, (x1 - mu_hat_1)) # with-in group co-variance\ncov_hat_2 = 1 / (n2 - 1) * np.matmul((x2 - mu_hat_2).T, (x2 - mu_hat_2))\ncov_hat = (cov_hat_1 + cov_hat_2) / 2\n\n\n## defined log likelyhood function\ndef LL(x # any point pair\n       , mu # average value\n       , sigma # covariance\n       , prior):\n    dist = x - mu\n    cov_inv = np.linalg.inv(sigma) ## inversion of cov matrix\n    cov_det = np.linalg.det(sigma) ## determinant of cov matrix\n    return -1/2 * np.log(2 * np.pi * cov_det) - 1/2 * dist.T @ cov_inv @ dist + np.log(prior)\n\n## use this to test just so I simplify syntax of the function\n# assert LL_(point_grid[0],mu_hat_1, cov_hat, pi_hat_1) == LL(point_grid[0],mu_hat_1, cov_hat, pi_hat_1)\n## interestingly I also saw v.T * M * v.T in egen value + egen vector... so it reflect a common pattern of something? \n## `cov_inv @ dist` is actually \n\n\n\nIn short LL is “Stacked Normal Distribution”\nGiven probability density function of a draw \\(x_i\\) from a normal distribute function:\n\\[\nf(x_i; \\mu,\\sigma^2) = (2\\pi\\sigma^2)^\\frac{1}{2}\\exp(-\\frac{1}{2}\\frac{(x_i - \\mu)^2}{\\sigma^2})\n\\]\nThis function gives joint probability density function for give sample \\(\\xi=[x_1 ... x_n]\\). This is the Likelihood Function\n\\[\n\\begin{align}\nL(\\xi,\\mu,\\sigma^2) &= \\prod_{i}^{n}{f(x_i; \\mu,\\sigma^2)} \\\\\n&= (2\\pi\\sigma^2)^{-n/2}\\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}{(x_i-\\mu)^2} )\n\\end{align}\n\\]\nBecause the area under density function is one, the joint probability function of varible would be many of these PDF stacked together. The mental image for this is think of any vector (no matter the sequence). The probability of \\(\\xi={x_1 ... x_n}\\) occur follow above function and it doesn’t matter which order.\nThe Log-likelihood Function is just this function logged:\n\\[\n\\theta = (\\mu;\\sigma^2) \\\\\n\\begin{align}\nl(\\theta; \\xi) &= ln[(2\\pi\\sigma^2)^{-n/2}\\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}{(x_i-\\mu)^2} )] \\\\\n&= ln[(2\\pi\\sigma^2) + \\ln[exp(-\\frac{1}{2\\sigma^2} \\sum_i^n{(x_i -\\mu)^2} ))] \\\\\n&= -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n{(x_i - \\mu)^2}\n\\end{align}\n\\] The beauty of Log-Likelihood function is it canceled a lot of \\(\\exp\\) element of normal distribution denity function and instead given you an algibratic expression, which you can then carry on to express with Matrix.\nAn Optimisation Problem\nLog likelihoold function is used when \\(\\^{\\theta}\\) is unknown but a set of sample vector \\(\\xi\\) is. This became a maximisation problem that obtain a \\(\\^{\\theta}\\) that will result that achieve this result. In the language of Math this is written as:\n\\[\n\\^{\\theta} = \\arg \\max_{\\theta}l(\\theta; \\xi)\n\\]\n(source: thanks to statlect)\n\n\n\nLDA approaches the problem by assuming that the condition \\(p(x|y=0)\\) and \\(p(x|y=1)\\) are both normal distribution, with mean and covariance parameter \\((\\mu, \\sigma_0)\\) \\((\\mu. \\sigma_0)\\), under this assumption, the Bayes-optimal solution is to predict points as being from the second class if the log of likelihood ratio is bigger than some threshold \\(T\\)\n\n\nunfload to explain dist.T @ cov_inv @ dist\n## I find it confusing when down to one dimension vector\n## because NP consider it no difference vertical or horizontal\nnp.array([1,2]) @ np.array([[1,2],[3,4]]) ## so this is in fact consider first as horizontal\n# 1 * 1 + 2 * 3 , 1 * 2 + 2 * 4\n## this code below will result in different value\n# np.array([[1,2],[3,4]]) @ np.array([1,2])\n\nnp.array([1,2]) @ np.array([[1,2],[3,4]]) @ np.array([1,2]) \n## 7 * 1  + 10 * 2\n\n\n27\n\n\n\n\n\n\npoint_grid = np.mgrid[-10:10.1:0.5, -10:10.1:0.5].reshape(2, -1).T\nll_vals_1 = [LL(x, mu_hat_1, cov_hat, pi_hat_1) for x in point_grid]\nll_vals_2 = [LL(x, mu_hat_2, cov_hat, pi_hat_2) for x in point_grid]\n\n\npoint_grid\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(point_grid[:, 0], point_grid[:, 1], c=ll_vals_1,marker=\"s\")\nax.plot(mu_hat_1[0], mu_hat_1[1], 'k-^', markersize=14)\n# ax.colorbar()\nax.set_title(\"Log likely hood function\")\n\nText(0.5, 1.0, 'Log likely hood function')\n\n\n\n\n\n\n\n\n\n\ncov_hat\n\narray([[1.19308602, 0.01970993],\n       [0.01970993, 1.0835684 ]])\n\n\n\n\ndef abline(slope, intercept):\n    \"\"\"Plot a line from slope and intercept\"\"\"\n    axes = plt.gca()\n    x_vals = np.array(axes.get_xlim())\n    y_vals = intercept + slope * x_vals\n    plt.plot(x_vals, y_vals, '--')\n    \ncov_inv = np.linalg.inv(cov_hat)\n\n# slope\nslope_vec = cov_inv @ (mu_hat_1 - mu_hat_2)\nslope = -slope_vec[0] / slope_vec[1]\n\n# intercept\nintercept_partial = (\n      np.log(pi_hat_2) - np.log(pi_hat_1) \n    + 0.5 * (mu_hat_1.T @ cov_inv @ mu_hat_1) \n    - 0.5 * (mu_hat_2.T @ cov_inv @ mu_hat_2))\nintercept = intercept_partial / slope_vec[1]\n\n# plotting\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(x[:, 0], x[:, 1], c=y)\nabline(slope, intercept)\nax.plot(mu_hat_1[0], mu_hat_1[1], 'rp', markersize=14)\nax.plot(mu_hat_2[0], mu_hat_2[1], 'rp', markersize=14)\n\n\n\n\n\n\n\n\n\nnp.linalg.inv(cov_hat_1 + cov_hat_2) @ \n\narray([[ 0.41920724, -0.00762531],\n       [-0.00762531,  0.46157704]])\n\n\n\n\n\n\n\n\n\n# tip mash gird does this\nnp.mgrid[1:2.5:0.5, 1:2.5:0.5]# \n\narray([[[1. , 1. , 1. ],\n        [1.5, 1.5, 1.5],\n        [2. , 2. , 2. ]],\n\n       [[1. , 1.5, 2. ],\n        [1. , 1.5, 2. ],\n        [1. , 1.5, 2. ]]])\n\n\n\nnp.mgrid[1:2.5:0.5, 1:2.5:0.5].reshape(2,-1).T\n\narray([[1. , 1. ],\n       [1. , 1.5],\n       [1. , 2. ],\n       [1.5, 1. ],\n       [1.5, 1.5],\n       [1.5, 2. ],\n       [2. , 1. ],\n       [2. , 1.5],\n       [2. , 2. ]])"
  },
  {
    "objectID": "2024/05-19 Factor Analysis/lda.html#background-of-the-analysis---sir-ronald-fisher-1939",
    "href": "2024/05-19 Factor Analysis/lda.html#background-of-the-analysis---sir-ronald-fisher-1939",
    "title": "Linear Discriminant Analysis",
    "section": "",
    "text": "Other name: Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher’s linear discriminant.\nFisher is a bigolgist who studies species of birds. (this story is hypothtical) Think in Fisher’s perspective I want to classify bird without measuring ratio…\n\n\nSeems sci-learn’s LDA don’t like this datasets\n\n\ngenerate two linear correlated group\n%config InlineBackend.figure_format='retina'\n# %matplotlib widget\n%matplotlib inline\n# change to widget for interactive\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nfrom matplotlib import animation\n\n\nsample_size = 100\nx1 = np.random.normal(scale = 1, size = sample_size)\ny1 = x1 * 0.2 + 3 + np.random.normal(scale=0.1, size=sample_size)\nz1 = 0.5 * x1 + y1 + np.random.normal(scale=0.1, size=sample_size)\n\nx2 = np.random.normal(scale = 1, size = sample_size)\ny2 = x2 * 0.6 + 2 + np.random.normal(scale=0.1, size=sample_size)\nz2 = 0.7 * x2 + 0.1 * y2 + np.random.normal(scale=0.1, size=sample_size)\n\nx3 = np.random.normal(scale = 1, size = sample_size)\ny3 = x3 + 1 + np.random.normal(scale=0.1, size=sample_size)\nz3 = 0.5 * x3 + 0.5 * y2 + np.random.normal(scale=0.1, size=sample_size)\n\nx = np.hstack((x1,x2\n               ,x3\n               ))\ny = np.hstack((y1,y2\n               ,y3\n               ))\nz = np.hstack((z1,z2\n               ,z3\n               ))\ng = np.hstack( (np.repeat(1,sample_size)\n              , np.repeat(0,sample_size)\n              , np.repeat(2,sample_size)\n              ))\n\nfigure = plt.figure()\n\nax=figure.add_subplot(111, projection=\"3d\")\nax.scatter(x,y,z, c=g)\nax.set_title(\"Project this Data\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(n_components=2)\n\nm = np.vstack((x,y,z)).T\nX_2d = lda.fit(m,g).transform(m)\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(X_2d[:,0],X_2d[:,1],c=g)\nax.set_title(\"LDA projection\")\n\nText(0.5, 1.0, 'LDA projection')\n\n\n\n\n\n\n\n\n\nThe effect of LDA or PCA is similar to seeing a 3D object with one eye: projection of higher dimension into one dimension.\n\n\n\n\nLinear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels…\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\n## fit model with principle component\npca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)\n\n## fit model with LDA\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_r2 = lda.fit(X, y).transform(X)\n\n## plog graph\nfig, ax = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\nax1=ax[0]\nax2=ax[1]\n\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\nlw = 2\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    ax1.scatter(\n        X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\n    )\nax1.legend(loc=\"best\", shadow=False, scatterpoints=1)\nax1.set_title(\"PCA of IRIS dataset\")\n\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    ax2.scatter(\n        X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name\n    )\nax2.legend(loc=\"best\", shadow=False, scatterpoints=1)\nax2.set_title(\"LDA of IRIS dataset\")\n\nfig.show()\n\n/var/folders/r5/1cdq52mn21zdnqzl0fvp44zw0000gn/T/ipykernel_98332/2715178903.py:43: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n\n\n\n\nThis following section it maybe useful for popup a few images of iris:\n\nFrom an artist’s perspective the differences between these iris’s are dimensionality of petal and sepal. The latend varible here is probably can view as (how square are these two petal)\n\n\n\nThis blog post thanks to Andy Jones\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate data\nmu1 = [0, 0]\nmu2 = [1, 2]\ncov = [[1, 0], [0, 1]]\nn1 = 500\nn2 = 50\nn = n1 + n2\nx1 = np.random.multivariate_normal(mean=mu1, cov=cov, size=n1)\nx2 = np.random.multivariate_normal(mean=mu2, cov=cov, size=n2)\nx = np.vstack([x1, x2])\ny = np.concatenate([np.repeat(1, n1), np.repeat(2, n2)])\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(x[:, 0], x[:, 1], c=y)\n\n\n\n\n\n\n\n\n\npi_hat_1 = n1 / n ## baysian prior also frequency of target\npi_hat_2 = n2 / n ## baysian prior also frequency of target\n\n## avg,expected_value,or position of normal\nmu_hat_1 = 1 / n1 * np.sum(x1, axis=0) #expected value for group1\nmu_hat_2 = 1 / n2 * np.sum(x2, axis=0) #expected value for group2\n\n# variance, or width of normal function\ncov_hat_1 = 1 / (n1 - 1) * np.matmul((x1 - mu_hat_1).T, (x1 - mu_hat_1)) # with-in group co-variance\ncov_hat_2 = 1 / (n2 - 1) * np.matmul((x2 - mu_hat_2).T, (x2 - mu_hat_2))\ncov_hat = (cov_hat_1 + cov_hat_2) / 2\n\n\n## defined log likelyhood function\ndef LL(x # any point pair\n       , mu # average value\n       , sigma # covariance\n       , prior):\n    dist = x - mu\n    cov_inv = np.linalg.inv(sigma) ## inversion of cov matrix\n    cov_det = np.linalg.det(sigma) ## determinant of cov matrix\n    return -1/2 * np.log(2 * np.pi * cov_det) - 1/2 * dist.T @ cov_inv @ dist + np.log(prior)\n\n## use this to test just so I simplify syntax of the function\n# assert LL_(point_grid[0],mu_hat_1, cov_hat, pi_hat_1) == LL(point_grid[0],mu_hat_1, cov_hat, pi_hat_1)\n## interestingly I also saw v.T * M * v.T in egen value + egen vector... so it reflect a common pattern of something? \n## `cov_inv @ dist` is actually \n\n\n\nIn short LL is “Stacked Normal Distribution”\nGiven probability density function of a draw \\(x_i\\) from a normal distribute function:\n\\[\nf(x_i; \\mu,\\sigma^2) = (2\\pi\\sigma^2)^\\frac{1}{2}\\exp(-\\frac{1}{2}\\frac{(x_i - \\mu)^2}{\\sigma^2})\n\\]\nThis function gives joint probability density function for give sample \\(\\xi=[x_1 ... x_n]\\). This is the Likelihood Function\n\\[\n\\begin{align}\nL(\\xi,\\mu,\\sigma^2) &= \\prod_{i}^{n}{f(x_i; \\mu,\\sigma^2)} \\\\\n&= (2\\pi\\sigma^2)^{-n/2}\\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}{(x_i-\\mu)^2} )\n\\end{align}\n\\]\nBecause the area under density function is one, the joint probability function of varible would be many of these PDF stacked together. The mental image for this is think of any vector (no matter the sequence). The probability of \\(\\xi={x_1 ... x_n}\\) occur follow above function and it doesn’t matter which order.\nThe Log-likelihood Function is just this function logged:\n\\[\n\\theta = (\\mu;\\sigma^2) \\\\\n\\begin{align}\nl(\\theta; \\xi) &= ln[(2\\pi\\sigma^2)^{-n/2}\\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}{(x_i-\\mu)^2} )] \\\\\n&= ln[(2\\pi\\sigma^2) + \\ln[exp(-\\frac{1}{2\\sigma^2} \\sum_i^n{(x_i -\\mu)^2} ))] \\\\\n&= -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n{(x_i - \\mu)^2}\n\\end{align}\n\\] The beauty of Log-Likelihood function is it canceled a lot of \\(\\exp\\) element of normal distribution denity function and instead given you an algibratic expression, which you can then carry on to express with Matrix.\nAn Optimisation Problem\nLog likelihoold function is used when \\(\\^{\\theta}\\) is unknown but a set of sample vector \\(\\xi\\) is. This became a maximisation problem that obtain a \\(\\^{\\theta}\\) that will result that achieve this result. In the language of Math this is written as:\n\\[\n\\^{\\theta} = \\arg \\max_{\\theta}l(\\theta; \\xi)\n\\]\n(source: thanks to statlect)\n\n\n\nLDA approaches the problem by assuming that the condition \\(p(x|y=0)\\) and \\(p(x|y=1)\\) are both normal distribution, with mean and covariance parameter \\((\\mu, \\sigma_0)\\) \\((\\mu. \\sigma_0)\\), under this assumption, the Bayes-optimal solution is to predict points as being from the second class if the log of likelihood ratio is bigger than some threshold \\(T\\)\n\n\nunfload to explain dist.T @ cov_inv @ dist\n## I find it confusing when down to one dimension vector\n## because NP consider it no difference vertical or horizontal\nnp.array([1,2]) @ np.array([[1,2],[3,4]]) ## so this is in fact consider first as horizontal\n# 1 * 1 + 2 * 3 , 1 * 2 + 2 * 4\n## this code below will result in different value\n# np.array([[1,2],[3,4]]) @ np.array([1,2])\n\nnp.array([1,2]) @ np.array([[1,2],[3,4]]) @ np.array([1,2]) \n## 7 * 1  + 10 * 2\n\n\n27\n\n\n\n\n\n\npoint_grid = np.mgrid[-10:10.1:0.5, -10:10.1:0.5].reshape(2, -1).T\nll_vals_1 = [LL(x, mu_hat_1, cov_hat, pi_hat_1) for x in point_grid]\nll_vals_2 = [LL(x, mu_hat_2, cov_hat, pi_hat_2) for x in point_grid]\n\n\npoint_grid\n\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(point_grid[:, 0], point_grid[:, 1], c=ll_vals_1,marker=\"s\")\nax.plot(mu_hat_1[0], mu_hat_1[1], 'k-^', markersize=14)\n# ax.colorbar()\nax.set_title(\"Log likely hood function\")\n\nText(0.5, 1.0, 'Log likely hood function')\n\n\n\n\n\n\n\n\n\n\ncov_hat\n\narray([[1.19308602, 0.01970993],\n       [0.01970993, 1.0835684 ]])\n\n\n\n\ndef abline(slope, intercept):\n    \"\"\"Plot a line from slope and intercept\"\"\"\n    axes = plt.gca()\n    x_vals = np.array(axes.get_xlim())\n    y_vals = intercept + slope * x_vals\n    plt.plot(x_vals, y_vals, '--')\n    \ncov_inv = np.linalg.inv(cov_hat)\n\n# slope\nslope_vec = cov_inv @ (mu_hat_1 - mu_hat_2)\nslope = -slope_vec[0] / slope_vec[1]\n\n# intercept\nintercept_partial = (\n      np.log(pi_hat_2) - np.log(pi_hat_1) \n    + 0.5 * (mu_hat_1.T @ cov_inv @ mu_hat_1) \n    - 0.5 * (mu_hat_2.T @ cov_inv @ mu_hat_2))\nintercept = intercept_partial / slope_vec[1]\n\n# plotting\nfig = plt.figure()\nax = fig.add_subplot()\n\nax.scatter(x[:, 0], x[:, 1], c=y)\nabline(slope, intercept)\nax.plot(mu_hat_1[0], mu_hat_1[1], 'rp', markersize=14)\nax.plot(mu_hat_2[0], mu_hat_2[1], 'rp', markersize=14)\n\n\n\n\n\n\n\n\n\nnp.linalg.inv(cov_hat_1 + cov_hat_2) @ \n\narray([[ 0.41920724, -0.00762531],\n       [-0.00762531,  0.46157704]])\n\n\n\n\n\n\n\n\n\n# tip mash gird does this\nnp.mgrid[1:2.5:0.5, 1:2.5:0.5]# \n\narray([[[1. , 1. , 1. ],\n        [1.5, 1.5, 1.5],\n        [2. , 2. , 2. ]],\n\n       [[1. , 1.5, 2. ],\n        [1. , 1.5, 2. ],\n        [1. , 1.5, 2. ]]])\n\n\n\nnp.mgrid[1:2.5:0.5, 1:2.5:0.5].reshape(2,-1).T\n\narray([[1. , 1. ],\n       [1. , 1.5],\n       [1. , 2. ],\n       [1.5, 1. ],\n       [1.5, 1.5],\n       [1.5, 2. ],\n       [2. , 1. ],\n       [2. , 1.5],\n       [2. , 2. ]])"
  },
  {
    "objectID": "2024/10-22 Mars Js/blog/01.html",
    "href": "2024/10-22 Mars Js/blog/01.html",
    "title": "Building on Mars Part I - Interactive 3D City + THREE.js",
    "section": "",
    "text": "Create 3D city map and put it on Mars In this 3D map, you can hover over an 3D object, and “magic” will happen: which is letting you"
  },
  {
    "objectID": "2024/10-22 Mars Js/blog/01.html#overview",
    "href": "2024/10-22 Mars Js/blog/01.html#overview",
    "title": "Building on Mars Part I - Interactive 3D City + THREE.js",
    "section": "Overview",
    "text": "Overview\nStep 1: Generate a 3D map Step 2: Load these asset using two pre-written example function from library THREE NOTE: You need identify those individual element; Step 3: Set up raycaster and normalized mouse casting;\n\nStep 0: Having a Node Enrionment\nFirst you need already have “node.js” and “vite” set up in your directory and having an index.html and a script in your directory (Install Three.js).\n├── index.html\n├── script.js\nIn script.js\n// script.js\n// a1-3D-City-Map.html\nimport * as THREE from 'three';\n// STL loader\nimport { STLLoader } from 'three/examples/jsm/loaders/STLLoader'\n// For camera view\nimport { OrbitControls } from 'three/examples/jsm/controls/OrbitControls'\n\n// if loading a project or Gltf use this\nimport { GLTFLoader } from 'three/examples/jsm/loaders/GLTFLoader.js';\n// ...\nAdd this script to html anywhere as a module\n&lt;body&gt;\n  &lt;script type=\"module\" src=\"a1-3D-City-Map.js\"&gt;&lt;/script&gt;\n&lt;/body&gt; \nTo start serving this three-D scene, just go\n# in your project terminal go\nnpx vite\n\n\nStep 1: Generate Fake 3D City Map\nUtilize open source software to generate a FAKE map by probabletrain: Probabletrain’s 3D Map Generator This will give you a directory looks like this:\npublic/model 2\n├── README.txt\n├── blocks.stl\n├── buildings.stl\n├── coastline.stl\n├── domain.stl\n├── river.stl\n├── roads.stl\n└── sea.stl\nThe whole building part. What I do is turn the bundle file into parts using blender for advanced editing.\npublic/model 2/building-parts\n├── building.bin\n├── building.gltf\n├── individuals.bin\n├── individuals.glb\n├── individuals.gltf\n├── part.bin\n├── part.gltf\n└── parts.stl\n\n\nStep 2: Loading: Two Different Loading\nWhen exporting from Blender there are two 3D object format, one is .stl, the other is .gltf. They seem to be written for a few examples for THREE.js, but so useful that we may as well just use them here.\n\nSingle STL is a single gometry file\n\nWithin each stl is a single “geoemtry” object (vector specifying the shape of geometry)\n\nGeometry is just a skeleton, now we want to add skins, known as materials [[/Building and Road Materials]]\nThe magic fomula is Matrial + Geometry = Mesh!\n\nlet road = null;\nloader.load(\n  '/model 2/roads.stl',// 1** the first arg is stl file location\n  function (geometry) {\n      // **2.1 create a mesh using stil geometry\n      const mesh = new THREE.Mesh(geometry, road_material);\n      road = mesh; // **2.2 expose this object so we can reference later\n      road.obj_type = \"road\" // set a custom attribute for reference\n      scene.add(mesh); // 2.3 add this mesh to scene\n  },  // 2** the second tell THREE to add scene\n  (xhr) =&gt; {\n      console.log((xhr.loaded / xhr.total) * 100 + '% loaded')\n  },  // 3** the thrid is side effect or midware?\n  (error) =&gt; {\n      console.log(error)\n  }   // 4** error handle\n)\nGLTF is like a collection of gomemetry mesh material, animation exported out of blender\nGLTF has attribute “scene”\n\nlet buildings = null\ngltf_loader.load(\n    \"/model 2/building-parts/building.gltf\",\n    function(gltf) {\n        // because out of blender the rotating is wrong\n        gltf.scene.rotation.x = Math.PI / 2;\n        // this is label not copy\n        buildings = gltf.scene.children\n        // console.log(buildings)\n        buildings.map(mesh=&gt; mesh.material=building_material.clone()) // clone is actually important, if you don't clone materials, the materials will reference the same thing, so when you change material color the whole thing changes\n        buildings.map(mesh=&gt;mesh.obj_type = \"building\")\n        buildings.map((mesh,index)=&gt;mesh.obj_id = index)\n        // this modify the whole thing in the mesh\n        scene.add(gltf.scene)\n    }\n)\n\n\n\nStep 3: Set up orbit control and ray cast!\n\nSetup Raycaster\nIn a 2D space, your cursor’s x & y can be used to indicate which element on a 2D plan is selected, in a 3D space however, you need projection. For your 2D computer screen to “know” which 3D object is selected, you need imformation about viewing angle - this is your camera. * mouse: you need track cursor x and y. If you have event listener, this is “clientX” & “clientY” (W3School teach you basic javascript: W3Schools online HTML editor). This is expressed relative to client screen size. For three to use they need to be normalized * raycaster : think of raycaster as a red layser beam pointing at any 3D blocks. Finally, to find what object are calculated were “penetrated” through the “raycaster” you use this method from “raycaster” object called “setFromcamera”.\nraycaster.setFromCamera(mouse, camera);\nIn context of THREE scene:\n// Function to detect the object under the mouse and change its color\nwindow.addEventListener('mousemove', onMouseMove, false);\nconst raycaster = new THREE.Raycaster();\nconst mouse = new THREE.Vector2();\nfunction onMouseMove(event) {\n  // Calculate mouse position in normalized device coordinates (-1 to +1) for both components\n  mouse.x = (event.clientX / window.innerWidth) * 2 - 1;\n  mouse.y = -(event.clientY / window.innerHeight) * 2 + 1;\n};\n\n\nControl Color Change on Element Selected\nraycaster have a property for storing any object the “ray” penetrates. This vector store a tremendours amout of json property. The one we want is the one called “object”. We want whatever individual building block is selected, the material color needs changing.\n// change color of selected geometry\nfunction hoverInteraction() {\n    // Update the raycaster with the current camera and mouse positions\n    raycaster.setFromCamera(mouse, camera);\n  \n    // Calculate objects intersecting the raycaster\n    const intersects = raycaster.intersectObjects(buildings,false);\n    // If there's an intersected object, change its color\n    // if(building) {\n    if(intersects.length != 0) {\n        const intersectedObject = intersects[0].object;\n        if(!!intersectedObject.obj_type) {\n            if(intersectedObject.obj_type == \"building\") {\n       // dirty way to change only selected building color only\n                intersectedObject.material.color.set(\"#3498db\")\n        // if we want to be fancy, we can also change material here, something more glossy\n            }\n        } \n    } else {\n        buildings.map(mesh=&gt; mesh.material.color.set(\"#f7f9f9\"))\n    }\n  }\nEventually the renderer function (onMonseMove will go on to animate and render) #### Camera Control & Animate Animate by convension may not make sense for us if we are just rotating a static 3D object. But in fact the concept “Animate” is important in implementing 3D interactivity to web development. Its is intuitive for web user to think they are interacting with a virtual object, when in fact what they are interacting with is a “two dimension plane” and “videos”, or “steaming frames” the webserver sent every second for your eyes only.\nSo the task of rotating viewing angle of the object is infact, tracking how my click movement has moved and rotate my camera based on it.\nFortunately someone have already wrote this task for us.\nconst controls = new OrbitControls(camera, renderer.domElement) //renderer is webGL render\ncontrols.enableDamping = true\nFinally, we setup streaming for the object to animate:\nfunction render() {\n    renderer.render(scene, camera)\n}\nfunction animate() {\n    requestAnimationFrame(animate);\n    hoverInteraction(); // just defined in previous step\n    controls.update();\n    render()\n}\nanimate()"
  }
]